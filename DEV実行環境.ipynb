{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"DEV実行環境.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1LAYl1_kH8saAkFz7wwaUW53_4hqiAXbS","authorship_tag":"ABX9TyN0oJMxCsRM8ePZf7E9xL19"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"widgets":{"application/vnd.jupyter.widget-state+json":{"fdc6e8fc857c497fb5b725949bcdcd46":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_98a24c5ac37d4387b96247a64c1372f3","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_cdb19e97b507427aae9a4881742d9d6d","IPY_MODEL_6941b7237e04438f9395b2490c90a10a","IPY_MODEL_652a36bec25f49dd8d1f5cbc57402837"]}},"98a24c5ac37d4387b96247a64c1372f3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"cdb19e97b507427aae9a4881742d9d6d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_da3ec7022ca249329825f33a7522e457","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Testing: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_2d8afa9fdf5d430d9fef595471061c6a"}},"6941b7237e04438f9395b2490c90a10a":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_3aab20b2a78942ff97543defbb3f6293","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6506f053e5054cb18a529ed49b392158"}},"652a36bec25f49dd8d1f5cbc57402837":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_c43b6a693e0b4af8b4eff1d77ed4c231","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 125/125 [02:13&lt;00:00,  1.01it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_162014b729d54de2adb141753a041e61"}},"da3ec7022ca249329825f33a7522e457":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"2d8afa9fdf5d430d9fef595471061c6a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3aab20b2a78942ff97543defbb3f6293":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"6506f053e5054cb18a529ed49b392158":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c43b6a693e0b4af8b4eff1d77ed4c231":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"162014b729d54de2adb141753a041e61":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"702e6bb503ca4a4885e3920c05a7cabf":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_d1099ccaea9641b4b465b550fddcf4db","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_99773eb59e0e45cd8b211e10f1f59820","IPY_MODEL_5319edc0707e40eda070b6e285ee6906","IPY_MODEL_813fb8ea9a01480e9f5ca3d13d5dd44d"]}},"d1099ccaea9641b4b465b550fddcf4db":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"99773eb59e0e45cd8b211e10f1f59820":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_ee1e2fccdeaf4568b5eadd134e8b896a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Validation sanity check:   0%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_0794694188694a82b247070ceb595d94"}},"5319edc0707e40eda070b6e285ee6906":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7e8560edaf2240078f7b5bb0dd65af1e","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"danger","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_181463f2563748eaa89d60c42f8951b0"}},"813fb8ea9a01480e9f5ca3d13d5dd44d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_363f5049239b42268afc08478e8c1b63","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/2 [00:48&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_125b60ae92bf4ce096524a6ae64e99c8"}},"ee1e2fccdeaf4568b5eadd134e8b896a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"0794694188694a82b247070ceb595d94":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7e8560edaf2240078f7b5bb0dd65af1e":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"181463f2563748eaa89d60c42f8951b0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"363f5049239b42268afc08478e8c1b63":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"125b60ae92bf4ce096524a6ae64e99c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"e38915d2b83044c2a66cdf4a1edf3d14":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0a4fa8640fd54e459e4ec5c8d7298d1e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_91f253f3a1204785a2f7c9033b024640","IPY_MODEL_aeffe848a6044e6fa625c15ff0d48686","IPY_MODEL_4b0f98402fde4fbd830d931893bbe529"]}},"0a4fa8640fd54e459e4ec5c8d7298d1e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":"row wrap","width":"100%","min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":"inline-flex","left":null}},"91f253f3a1204785a2f7c9033b024640":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_dae97cfc37fd419893e5fccd21f747c7","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Epoch 0:   0%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_6d746117a5044cc0b506b06f0aa0af79"}},"aeffe848a6044e6fa625c15ff0d48686":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9f8bf4310d4247779fc103cfe36972d6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"","max":338,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":0,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_cc1543901dcf416eb4d202d15fb4b778"}},"4b0f98402fde4fbd830d931893bbe529":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_a62ce01af61d46abb113e5f5ca275cad","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 0/338 [00:00&lt;?, ?it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_64e1c33027804be6938a151499cb2f36"}},"dae97cfc37fd419893e5fccd21f747c7":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"6d746117a5044cc0b506b06f0aa0af79":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9f8bf4310d4247779fc103cfe36972d6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"cc1543901dcf416eb4d202d15fb4b778":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":"2","_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a62ce01af61d46abb113e5f5ca275cad":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"64e1c33027804be6938a151499cb2f36":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"markdown","metadata":{"id":"lGJcm7o2jKVj"},"source":["# 20210905"]},{"cell_type":"code","metadata":{"id":"1b8QAqk479BT"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r-TePcdEjNZu"},"source":["## Install dependency\n","~~~\n","Based on the pyproject.toml in the current path, dependency will be installed in order.\n","~~~"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yg67eyyOlzBy","executionInfo":{"status":"ok","timestamp":1630821801701,"user_tz":-540,"elapsed":399,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"8f8cf551-25f4-4290-8720-791e070dfdac"},"source":["cd /content/drive/MyDrive/ColabNotebooks/Research/KG2QGwithT5/adjacencyattentionwithoutselfloop"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ColabNotebooks/Research/KG2QGwithT5/adjacencyattentionwithoutselfloop\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fm3qGOhtpd-H","executionInfo":{"status":"ok","timestamp":1630815099887,"user_tz":-540,"elapsed":935,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"cabd0b7a-ce43-4e29-c640-05d9d2f95f47"},"source":["!poetry lock"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/bin/bash: poetry: command not found\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Q1-Szq2-jNZu","executionInfo":{"status":"ok","timestamp":1630815110479,"user_tz":-540,"elapsed":8063,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"8cc4f2ca-bcab-4291-e148-c9ef16f7891e"},"source":["!pip install -q --pre poetry==1.1.8"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 173 kB 9.4 MB/s \n","\u001b[K     |████████████████████████████████| 91 kB 13.8 MB/s \n","\u001b[K     |████████████████████████████████| 40 kB 7.2 MB/s \n","\u001b[K     |████████████████████████████████| 54 kB 3.5 MB/s \n","\u001b[K     |████████████████████████████████| 420 kB 39.9 MB/s \n","\u001b[K     |████████████████████████████████| 5.3 MB 52.5 MB/s \n","\u001b[K     |████████████████████████████████| 54 kB 3.5 MB/s \n","\u001b[K     |████████████████████████████████| 3.0 MB 68.5 MB/s \n","\u001b[K     |████████████████████████████████| 338 kB 78.3 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"c_btxatZjNZu","executionInfo":{"status":"ok","timestamp":1630815261458,"user_tz":-540,"elapsed":150984,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"2f370d41-7a1e-4871-b481-1818510a0977"},"source":["!pip install --requirement <(poetry export --format requirements.txt)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ignoring colorama: markers 'python_version >= \"3.6\" and python_full_version < \"3.0.0\" and platform_system == \"Windows\" or python_full_version >= \"3.5.0\" and python_version >= \"3.6\" and platform_system == \"Windows\"' don't match your environment\n","Ignoring pywin32: markers 'sys_platform == \"win32\" and python_version >= \"3.6\"' don't match your environment\n","Ignoring waitress: markers 'platform_system == \"Windows\" and python_version >= \"3.6\" and python_full_version >= \"3.6.0\"' don't match your environment\n","Collecting absl-py==0.13.0\n","  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 7.2 MB/s \n","\u001b[?25hCollecting aiohttp==3.7.4.post0\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 55.8 MB/s \n","\u001b[?25hCollecting alembic==1.4.1\n","  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 62.8 MB/s \n","\u001b[?25hCollecting antlr4-python3-runtime==4.8\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 65.2 MB/s \n","\u001b[?25hCollecting async-timeout==3.0.1\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: attrs==21.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 49)) (21.2.0)\n","Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 52)) (4.2.2)\n","Requirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 55)) (2021.5.30)\n","Collecting chardet==4.0.0\n","  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n","\u001b[K     |████████████████████████████████| 178 kB 61.5 MB/s \n","\u001b[?25hRequirement already satisfied: charset-normalizer==2.0.4 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 61)) (2.0.4)\n","Collecting click==8.0.1\n","  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 8.9 MB/s \n","\u001b[?25hCollecting cloudpickle==1.6.0\n","  Downloading cloudpickle-1.6.0-py3-none-any.whl (23 kB)\n","Collecting configparser==5.0.2\n","  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n","Collecting databricks-cli==0.15.0\n","  Downloading databricks-cli-0.15.0.tar.gz (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 5.9 MB/s \n","\u001b[?25hCollecting docker-pycreds==0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting docker==5.0.2\n","  Downloading docker-5.0.2-py2.py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 62.5 MB/s \n","\u001b[?25hRequirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 85)) (0.3)\n","Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 88)) (3.0.12)\n","Collecting flask==2.0.1\n","  Downloading Flask-2.0.1-py3-none-any.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 4.9 MB/s \n","\u001b[?25hCollecting fsspec==2021.8.1\n","  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n","\u001b[K     |████████████████████████████████| 119 kB 78.6 MB/s \n","\u001b[?25hCollecting future==0.18.2\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 67.2 MB/s \n","\u001b[?25hCollecting gitdb==4.0.7\n","  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 2.3 MB/s \n","\u001b[?25hCollecting gitpython==3.1.20\n","  Downloading GitPython-3.1.20-py3-none-any.whl (178 kB)\n","\u001b[K     |████████████████████████████████| 178 kB 77.2 MB/s \n","\u001b[?25hCollecting google-auth-oauthlib==0.4.6\n","  Downloading google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n","Collecting google-auth==1.35.0\n","  Downloading google_auth-1.35.0-py2.py3-none-any.whl (152 kB)\n","\u001b[K     |████████████████████████████████| 152 kB 70.6 MB/s \n","\u001b[?25hRequirement already satisfied: greenlet==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 111)) (1.1.1)\n","Requirement already satisfied: grpcio==1.39.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 162)) (1.39.0)\n","Collecting gunicorn==20.1.0\n","  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 10.9 MB/s \n","\u001b[?25hCollecting hydra-core==1.1.1\n","  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 80.6 MB/s \n","\u001b[?25hCollecting hydra==2.5\n","  Downloading Hydra-2.5.tar.gz (82 kB)\n","\u001b[K     |████████████████████████████████| 82 kB 734 kB/s \n","\u001b[?25hCollecting idna==3.2\n","  Downloading idna-3.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 9.6 MB/s \n","\u001b[?25hCollecting importlib-metadata==4.8.1\n","  Downloading importlib_metadata-4.8.1-py3-none-any.whl (17 kB)\n","Requirement already satisfied: importlib-resources==5.2.2 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 228)) (5.2.2)\n","Collecting itsdangerous==2.0.1\n","  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\n","Collecting jinja2==3.0.1\n","  Downloading Jinja2-3.0.1-py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 75.7 MB/s \n","\u001b[?25hRequirement already satisfied: joblib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 237)) (1.0.1)\n","Collecting mako==1.1.5\n","  Downloading Mako-1.1.5-py2.py3-none-any.whl (75 kB)\n","\u001b[K     |████████████████████████████████| 75 kB 5.7 MB/s \n","\u001b[?25hRequirement already satisfied: markdown==3.3.4 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 243)) (3.3.4)\n","Requirement already satisfied: markupsafe==2.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 246)) (2.0.1)\n","Collecting mlflow==1.20.2\n","  Downloading mlflow-1.20.2-py3-none-any.whl (14.6 MB)\n","\u001b[K     |████████████████████████████████| 14.6 MB 38.4 MB/s \n","\u001b[?25hCollecting multidict==5.1.0\n","  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n","\u001b[K     |████████████████████████████████| 142 kB 65.3 MB/s \n","\u001b[?25hCollecting nltk==3.6\n","  Downloading nltk-3.6-py3-none-any.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 68.6 MB/s \n","\u001b[?25hCollecting numpy==1.21.1\n","  Downloading numpy-1.21.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n","\u001b[K     |████████████████████████████████| 15.7 MB 71 kB/s \n","\u001b[?25hRequirement already satisfied: oauthlib==3.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 374)) (3.1.1)\n","Collecting omegaconf==2.1.1\n","  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 4.2 MB/s \n","\u001b[?25hCollecting packaging==21.0\n","  Downloading packaging-21.0-py3-none-any.whl (40 kB)\n","\u001b[K     |████████████████████████████████| 40 kB 7.1 MB/s \n","\u001b[?25hRequirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 383)) (1.1.5)\n","Collecting pillow==8.3.2\n","  Downloading Pillow-8.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 34.1 MB/s \n","\u001b[?25hRequirement already satisfied: prometheus-client==0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 462)) (0.11.0)\n","Collecting prometheus-flask-exporter==0.18.2\n","  Downloading prometheus_flask_exporter-0.18.2.tar.gz (22 kB)\n","Requirement already satisfied: promise==2.3 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 467)) (2.3)\n","Requirement already satisfied: protobuf==3.17.3 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 469)) (3.17.3)\n","Collecting psutil==5.8.0\n","  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n","\u001b[K     |████████████████████████████████| 296 kB 59.5 MB/s \n","\u001b[?25hRequirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 526)) (0.2.8)\n","Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 540)) (0.4.8)\n","Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 554)) (2.4.7)\n","Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 557)) (2.8.2)\n","Collecting python-editor==1.0.4\n","  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n","Collecting pytorch-lightning==1.2.1\n","  Downloading pytorch_lightning-1.2.1-py3-none-any.whl (814 kB)\n","\u001b[K     |████████████████████████████████| 814 kB 64.1 MB/s \n","\u001b[?25hCollecting pytz==2021.1\n","  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n","\u001b[K     |████████████████████████████████| 510 kB 80.0 MB/s \n","\u001b[?25hCollecting pyyaml==5.3.1\n","  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n","\u001b[K     |████████████████████████████████| 269 kB 74.9 MB/s \n","\u001b[?25hCollecting querystring-parser==1.2.4\n","  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n","Collecting regex==2021.8.28\n","  Downloading regex-2021.8.28-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (745 kB)\n","\u001b[K     |████████████████████████████████| 745 kB 54.1 MB/s \n","\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 644)) (1.3.0)\n","Collecting requests==2.26.0\n","  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 1.1 MB/s \n","\u001b[?25hRequirement already satisfied: rsa==4.7.2 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 651)) (4.7.2)\n","Collecting sacremoses==0.0.45\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 81.7 MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.96\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 55.6 MB/s \n","\u001b[?25hCollecting sentry-sdk==1.3.1\n","  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 75.6 MB/s \n","\u001b[?25hCollecting shortuuid==1.0.1\n","  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n","Collecting six==1.16.0\n","  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting smmap==4.0.0\n","  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n","Collecting sqlalchemy==1.4.23\n","  Downloading SQLAlchemy-1.4.23-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 61.3 MB/s \n","\u001b[?25hRequirement already satisfied: sqlparse==0.4.1 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 743)) (0.4.1)\n","Collecting subprocess32==3.5.4\n","  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 9.4 MB/s \n","\u001b[?25hRequirement already satisfied: tabulate==0.8.9 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 750)) (0.8.9)\n","Requirement already satisfied: tensorboard-data-server==0.6.1 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 753)) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit==1.8.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 757)) (1.8.0)\n","Requirement already satisfied: tensorboard==2.6.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 759)) (2.6.0)\n","Collecting torch==1.7.1\n","  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n","\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n","\u001b[?25hCollecting torchtext==0.8.0\n","  Downloading torchtext-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (6.9 MB)\n","\u001b[K     |████████████████████████████████| 6.9 MB 26.7 MB/s \n","\u001b[?25hCollecting torchvision==0.8.2\n","  Downloading torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 5.7 MB/s \n","\u001b[?25hCollecting tqdm==4.62.2\n","  Downloading tqdm-4.62.2-py2.py3-none-any.whl (76 kB)\n","\u001b[K     |████████████████████████████████| 76 kB 6.0 MB/s \n","\u001b[?25hCollecting typing-extensions==3.10.0.2\n","  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n","Collecting urllib3==1.26.6\n","  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 77.0 MB/s \n","\u001b[?25hCollecting wandb==0.10.7\n","  Downloading wandb-0.10.7-py2.py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 64.4 MB/s \n","\u001b[?25hCollecting watchdog==2.1.5\n","  Downloading watchdog-2.1.5-py3-none-manylinux2014_x86_64.whl (75 kB)\n","\u001b[K     |████████████████████████████████| 75 kB 5.5 MB/s \n","\u001b[?25hCollecting websocket-client==1.2.1\n","  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 2.0 MB/s \n","\u001b[?25hCollecting werkzeug==2.0.1\n","  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n","\u001b[K     |████████████████████████████████| 288 kB 81.8 MB/s \n","\u001b[?25hCollecting yarl==1.6.3\n","  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n","\u001b[K     |████████████████████████████████| 294 kB 69.0 MB/s \n","\u001b[?25hRequirement already satisfied: zipp==3.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 874)) (3.5.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.35.0->-r /dev/fd/63 (line 108)) (57.4.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.6.0->-r /dev/fd/63 (line 759)) (0.37.0)\n","\u001b[33mWARNING: The candidate selected for download or install is a yanked version: 'gitpython' candidate (version 3.1.20 at https://files.pythonhosted.org/packages/55/60/f884f01eef2a7255875862ec1b12d57d74113ec6e8d9e16c4d254cd6aa3c/GitPython-3.1.20-py3-none-any.whl#sha256=b1e1c269deab1b08ce65403cf14e10d2ef1f6c89e33ea7c5e5bb0222ea593b8a (from https://pypi.org/simple/gitpython/) (requires-python:>=3.6))\n","Reason for being yanked: Issues with typing of ordered dict\u001b[0m\n","Building wheels for collected packages: alembic, antlr4-python3-runtime, databricks-cli, future, hydra, prometheus-flask-exporter, pyyaml, subprocess32\n","  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158172 sha256=55ffdd38a34d178dd1cdc624a737dc27b3da8b0b46aa627b33d6ae41732790cd\n","  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=f89aef4358692fe16896e62c658c28982c7414ba9282a3937b70940de5a26a51\n","  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n","  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for databricks-cli: filename=databricks_cli-0.15.0-py3-none-any.whl size=105260 sha256=e8fb76424e19b274660ad135de51a5a3df784bcbe4122d1d6733f614a947c22b\n","  Stored in directory: /root/.cache/pip/wheels/e7/ba/75/284f9a90ff7a010bb23b9798f2e9a19dd9fe619379c917bff4\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=6e3a7bb38ead2e7c0dc87312781db8d064f61fa3a6df3a926e304b1c16de1405\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","  Building wheel for hydra (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hydra: filename=Hydra-2.5-cp37-cp37m-linux_x86_64.whl size=220766 sha256=494fe1826642c4b29f969d5ee894ac1b65b7b5f90e7442639391aba2d4344c96\n","  Stored in directory: /root/.cache/pip/wheels/46/28/7d/3b38a41d900da90c4e17576f442bac9344eb1f5a4e78ee9f83\n","  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.2-py3-none-any.whl size=17415 sha256=cbcc17cb577a4e403947c448e1e62be52f0d87bdd665923bf394136364b73e77\n","  Stored in directory: /root/.cache/pip/wheels/6a/1e/1c/c765920cb92b2f0343d2dd8b481a407cee2823f9b4bbd2e52a\n","  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44635 sha256=8d2a4045ac4153d113f17b2c71e0c7209cdabd3a6afafc56c639a5fb74e06e35\n","  Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=73fe6238a742f867c7b8ca88bf0aedc306151fd15f958f5692c1ebc28b6f227e\n","  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n","Successfully built alembic antlr4-python3-runtime databricks-cli future hydra prometheus-flask-exporter pyyaml subprocess32\n","Installing collected packages: urllib3, typing-extensions, idna, six, requests, multidict, importlib-metadata, yarl, werkzeug, smmap, jinja2, itsdangerous, google-auth, click, chardet, async-timeout, websocket-client, sqlalchemy, pyyaml, pytz, python-editor, numpy, mako, google-auth-oauthlib, gitdb, fsspec, flask, antlr4-python3-runtime, aiohttp, absl-py, watchdog, tqdm, torch, subprocess32, shortuuid, sentry-sdk, regex, querystring-parser, psutil, prometheus-flask-exporter, pillow, packaging, omegaconf, gunicorn, gitpython, future, docker-pycreds, docker, databricks-cli, configparser, cloudpickle, alembic, wandb, torchvision, torchtext, sentencepiece, sacremoses, pytorch-lightning, nltk, mlflow, hydra-core, hydra\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 3.7.4.3\n","    Uninstalling typing-extensions-3.7.4.3:\n","      Successfully uninstalled typing-extensions-3.7.4.3\n","  Attempting uninstall: idna\n","    Found existing installation: idna 2.10\n","    Uninstalling idna-2.10:\n","      Successfully uninstalled idna-2.10\n","  Attempting uninstall: six\n","    Found existing installation: six 1.15.0\n","    Uninstalling six-1.15.0:\n","      Successfully uninstalled six-1.15.0\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 1.7.0\n","    Uninstalling importlib-metadata-1.7.0:\n","      Successfully uninstalled importlib-metadata-1.7.0\n","  Attempting uninstall: werkzeug\n","    Found existing installation: Werkzeug 1.0.1\n","    Uninstalling Werkzeug-1.0.1:\n","      Successfully uninstalled Werkzeug-1.0.1\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 2.11.3\n","    Uninstalling Jinja2-2.11.3:\n","      Successfully uninstalled Jinja2-2.11.3\n","  Attempting uninstall: itsdangerous\n","    Found existing installation: itsdangerous 1.1.0\n","    Uninstalling itsdangerous-1.1.0:\n","      Successfully uninstalled itsdangerous-1.1.0\n","  Attempting uninstall: google-auth\n","    Found existing installation: google-auth 1.34.0\n","    Uninstalling google-auth-1.34.0:\n","      Successfully uninstalled google-auth-1.34.0\n","  Attempting uninstall: click\n","    Found existing installation: click 7.1.2\n","    Uninstalling click-7.1.2:\n","      Successfully uninstalled click-7.1.2\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 3.0.4\n","    Uninstalling chardet-3.0.4:\n","      Successfully uninstalled chardet-3.0.4\n","  Attempting uninstall: sqlalchemy\n","    Found existing installation: SQLAlchemy 1.4.22\n","    Uninstalling SQLAlchemy-1.4.22:\n","      Successfully uninstalled SQLAlchemy-1.4.22\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: pytz\n","    Found existing installation: pytz 2018.9\n","    Uninstalling pytz-2018.9:\n","      Successfully uninstalled pytz-2018.9\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Attempting uninstall: google-auth-oauthlib\n","    Found existing installation: google-auth-oauthlib 0.4.5\n","    Uninstalling google-auth-oauthlib-0.4.5:\n","      Successfully uninstalled google-auth-oauthlib-0.4.5\n","  Attempting uninstall: flask\n","    Found existing installation: Flask 1.1.4\n","    Uninstalling Flask-1.1.4:\n","      Successfully uninstalled Flask-1.1.4\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 0.12.0\n","    Uninstalling absl-py-0.12.0:\n","      Successfully uninstalled absl-py-0.12.0\n","  Attempting uninstall: tqdm\n","    Found existing installation: tqdm 4.62.0\n","    Uninstalling tqdm-4.62.0:\n","      Successfully uninstalled tqdm-4.62.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2019.12.20\n","    Uninstalling regex-2019.12.20:\n","      Successfully uninstalled regex-2019.12.20\n","  Attempting uninstall: psutil\n","    Found existing installation: psutil 5.4.8\n","    Uninstalling psutil-5.4.8:\n","      Successfully uninstalled psutil-5.4.8\n","  Attempting uninstall: pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 20.9\n","    Uninstalling packaging-20.9:\n","      Successfully uninstalled packaging-20.9\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 1.3.0\n","    Uninstalling cloudpickle-1.3.0:\n","      Successfully uninstalled cloudpickle-1.3.0\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.10.0+cu102\n","    Uninstalling torchvision-0.10.0+cu102:\n","      Successfully uninstalled torchvision-0.10.0+cu102\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.10.0\n","    Uninstalling torchtext-0.10.0:\n","      Successfully uninstalled torchtext-0.10.0\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.1 which is incompatible.\n","tensorflow 2.6.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n","tensorflow 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\n","tensorflow-metadata 1.2.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.13.0 which is incompatible.\n","poetry 1.1.8 requires importlib-metadata<2.0.0,>=1.6.0; python_version < \"3.8\", but you have importlib-metadata 4.8.1 which is incompatible.\n","poetry 1.1.8 requires packaging<21.0,>=20.4, but you have packaging 21.0 which is incompatible.\n","poetry-core 1.0.4 requires importlib-metadata<2.0.0,>=1.7.0; python_version >= \"2.7\" and python_version < \"2.8\" or python_version >= \"3.5\" and python_version < \"3.8\", but you have importlib-metadata 4.8.1 which is incompatible.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n","google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed absl-py-0.13.0 aiohttp-3.7.4.post0 alembic-1.4.1 antlr4-python3-runtime-4.8 async-timeout-3.0.1 chardet-4.0.0 click-8.0.1 cloudpickle-1.6.0 configparser-5.0.2 databricks-cli-0.15.0 docker-5.0.2 docker-pycreds-0.4.0 flask-2.0.1 fsspec-2021.8.1 future-0.18.2 gitdb-4.0.7 gitpython-3.1.20 google-auth-1.35.0 google-auth-oauthlib-0.4.6 gunicorn-20.1.0 hydra-2.5 hydra-core-1.1.1 idna-3.2 importlib-metadata-4.8.1 itsdangerous-2.0.1 jinja2-3.0.1 mako-1.1.5 mlflow-1.20.2 multidict-5.1.0 nltk-3.6 numpy-1.21.1 omegaconf-2.1.1 packaging-21.0 pillow-8.3.2 prometheus-flask-exporter-0.18.2 psutil-5.8.0 python-editor-1.0.4 pytorch-lightning-1.2.1 pytz-2021.1 pyyaml-5.3.1 querystring-parser-1.2.4 regex-2021.8.28 requests-2.26.0 sacremoses-0.0.45 sentencepiece-0.1.96 sentry-sdk-1.3.1 shortuuid-1.0.1 six-1.16.0 smmap-4.0.0 sqlalchemy-1.4.23 subprocess32-3.5.4 torch-1.7.1 torchtext-0.8.0 torchvision-0.8.2 tqdm-4.62.2 typing-extensions-3.10.0.2 urllib3-1.26.6 wandb-0.10.7 watchdog-2.1.5 websocket-client-1.2.1 werkzeug-2.0.1 yarl-1.6.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","google","numpy","psutil","pydevd_plugins","pytz","six"]}}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"_CeH4xcGjNZu"},"source":["## Import Library"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5ALdAj9tjNZv","executionInfo":{"status":"ok","timestamp":1630821811428,"user_tz":-540,"elapsed":2608,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"0bb3101f-7f25-4ded-d41b-69bcffdd6246"},"source":["import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","import re\n","from itertools import chain\n","from string import punctuation\n"," \n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pytorch_lightning as pl\n","\n","import io\n","import string\n","from collections import Counter, defaultdict, OrderedDict\n","from nltk.tokenize import wordpunct_tokenize, word_tokenize\n"," \n","import gc\n","\n","from AdjacencyAttentionWithoutSelfloopTransformers import (\n","  AdamW,\n","  T5ForConditionalGeneration,\n","  T5Tokenizer,\n","  get_linear_schedule_with_warmup\n",")\n","\n","import os.path\n","\n","import wandb\n","wandb.login()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmacho000\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"fklVeEs5jNZv"},"source":["## Preprocess"]},{"cell_type":"code","metadata":{"id":"RtWfaN9ujNZv"},"source":["def preprocess_wq(config):\n","    if not os.path.isfile(\"processed/mhqg-wq/SOURCE_ID_memmap.npy\"):\n","        SOURCE_ID_PATH = \"processed/mhqg-wq/SOURCE_ID_memmap.npy\"\n","        SOURCE_MASK_PATH = \"processed/mhqg-wq/SOURCE_MASK_memmap.npy\"\n","        SOURCE_CROSS_MASK_PATH = \"processed/mhqg-wq/SOURCE_CROSS_MASK_memmap.npy\"\n","        TARGET_ID_PATH = \"processed/mhqg-wq/TARGET_ID_memmap.npy\"\n","        TARGET_MASK_PATH = \"processed/mhqg-wq/TARGET_MASK_memmap.npy\"\n","        # input_length = 18624\n","        input_length = 18989\n","        SOURCE_ID_memmap = np.memmap(\n","        filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,512) \n","        )\n","\n","        SOURCE_MASK_memmap = np.memmap(\n","        filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512,512)\n","        )\n","\n","        SOURCE_CROSS_MASK_memmap = np.memmap(\n","        filename=SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512)\n","        )\n","\n","        TARGET_ID_memmap = np.memmap(\n","        filename=TARGET_ID_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,100)\n","        )\n","\n","        TARGET_MASK_memmap = np.memmap(\n","        filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,100)\n","        )\n","\n","        del SOURCE_ID_memmap\n","        del SOURCE_MASK_memmap\n","        del SOURCE_CROSS_MASK_memmap\n","        del TARGET_ID_memmap\n","        del TARGET_MASK_memmap\n","        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        with open(\"data/mhqg-wq/train.json\", 'r') as f:\n","            list_index = 0\n","            for index, line in enumerate(f):\n","                line = line.strip()\n","                jo = json.loads(line, object_pairs_hook=OrderedDict)\n","                assert len(jo['inGraph']['g_adj']) > 0\n","\n","                answers = jo['answers']\n","                normalized_answers = \"\"\n","                for x in answers:\n","                    normalized_answers += (x + \" \")\n","                target = jo['outSeq']\n","                n = len(jo['inGraph']['g_node_names'])\n","                adj_matrix = {}\n","                adj_matrix2 = {}\n","                graph = {'node_name_id2word':{}}\n","                for idx, nid in enumerate(jo['inGraph']['g_node_names']):\n","                    # replace answer token to \"<answer> answer <answer>\"\n","                    if jo['inGraph']['g_node_names'][nid] in answers:\n","                        graph['node_name_id2word'][nid] = \"<answer> \"+ jo['inGraph']['g_node_names'][nid] + \" <answer>\"\n","                    else:\n","                        graph['node_name_id2word'][nid] = jo['inGraph']['g_node_names'][nid]\n","\n","                    # create adj_matrix with 0 value\n","                    for idy, nid2 in enumerate(jo['inGraph']['g_node_names']):\n","                        if jo['inGraph']['g_node_names'][nid2] in answers:\n","                            adj_matrix2[\"<answer> \"+ jo['inGraph']['g_node_names'][nid2] + \" <answer>\"] = 0\n","                        else:\n","                            adj_matrix2[jo['inGraph']['g_node_names'][nid2]] = 0\n","                    if jo['inGraph']['g_node_names'][nid] in answers:\n","                        adj_matrix[\"<answer> \"+ jo['inGraph']['g_node_names'][nid] + \" <answer>\"] = adj_matrix2.copy()\n","                    else:\n","                        adj_matrix[jo['inGraph']['g_node_names'][nid]] = adj_matrix2.copy()\n","                for nid, val in jo['inGraph']['g_adj'].items():\n","                    for nid2, edge in val.items():\n","                        edge = edge.split('/')[-1]\n","                        adj_matrix[graph[\"node_name_id2word\"][nid]][graph[\"node_name_id2word\"][nid2]] = 1\n","                input_knowledge_graph = \"\"\n","                for i,ids in enumerate(list(adj_matrix.keys())):\n","                    input_knowledge_graph += (ids + \" \")\n","\n","                input, target = input_knowledge_graph, target\n","\n","                tokenized_row_input = \"\"\n","                tokenized_row_input = tokenizer.tokenize(input)\n","                if len(tokenized_row_input) > 512:\n","                    continue\n","\n","                tokenized_inputs = tokenizer.batch_encode_plus_self_attention(\n","                    [input_knowledge_graph], adjacancy_matrix=adj_matrix, max_length=512, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                tokenized_targets = tokenizer.batch_encode_plus(\n","                    [target], max_length=100, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","                SOURCE_ID_memmap = np.memmap(\n","                    filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,512) \n","                )\n","\n","                SOURCE_MASK_memmap = np.memmap(\n","                    filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512,512)\n","                )\n","\n","                SOURCE_CROSS_ATTENTION_MASK_memmap = np.memmap(\n","                    filename=SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512)\n","                )\n","\n","                TARGET_ID_memmap = np.memmap(\n","                    filename=TARGET_ID_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,100)\n","                )\n","\n","                TARGET_MASK_memmap = np.memmap(\n","                    filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,100)\n","                )\n","\n","                SOURCE_ID_memmap[list_index] = tokenized_inputs[\"input_ids\"].squeeze().numpy()\n","                SOURCE_MASK_memmap[list_index] = tokenized_inputs[\"attention_mask\"].squeeze().numpy()\n","                SOURCE_CROSS_ATTENTION_MASK_memmap[list_index] = tokenized_inputs[\"cross_attention_mask\"].squeeze().numpy()\n","\n","                TARGET_ID_memmap[list_index] = tokenized_targets[\"input_ids\"].squeeze().numpy()\n","                TARGET_MASK_memmap[list_index] = tokenized_targets[\"attention_mask\"].squeeze().numpy()\n","\n","                del SOURCE_ID_memmap\n","                del SOURCE_MASK_memmap\n","                del SOURCE_CROSS_ATTENTION_MASK_memmap\n","                del TARGET_ID_memmap\n","                del TARGET_MASK_memmap\n","\n","                if index%500==0:\n","                    print(f\"{index} was finished\", index)\n","\n","                list_index += 1\n","\n","    if not os.path.isfile(\"processed/mhqg-wq/VAL_SOURCE_ID_memmap.npy\"):\n","        SOURCE_ID_PATH = \"processed/mhqg-wq/VAL_SOURCE_ID_memmap.npy\"\n","        SOURCE_MASK_PATH = \"processed/mhqg-wq/VAL_SOURCE_MASK_memmap.npy\"\n","        SOURCE_CROSS_MASK_PATH = \"processed/mhqg-wq/VAL_SOURCE_CROSS_MASK_memmap.npy\"\n","        TARGET_ID_PATH = \"processed/mhqg-wq/VAL_TARGET_ID_memmap.npy\"\n","        TARGET_MASK_PATH = \"processed/mhqg-wq/VAL_TARGET_MASK_memmap.npy\"\n","        # input_length = 18624\n","        input_length = 2000\n","        SOURCE_ID_memmap = np.memmap(\n","        filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,512) \n","        )\n","\n","        SOURCE_MASK_memmap = np.memmap(\n","        filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512,512)\n","        )\n","\n","        SOURCE_CROSS_MASK_memmap = np.memmap(\n","        filename=SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512)\n","        )\n","\n","        TARGET_ID_memmap = np.memmap(\n","        filename=TARGET_ID_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,100)\n","        )\n","\n","        TARGET_MASK_memmap = np.memmap(\n","        filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,100)\n","        )\n","\n","        del SOURCE_ID_memmap\n","        del SOURCE_MASK_memmap\n","        del SOURCE_CROSS_MASK_memmap\n","        del TARGET_ID_memmap\n","        del TARGET_MASK_memmap\n","        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        with open(\"data/mhqg-wq/dev.json\", 'r') as f:\n","            list_index = 0\n","            for index, line in enumerate(f):\n","                line = line.strip()\n","                jo = json.loads(line, object_pairs_hook=OrderedDict)\n","                assert len(jo['inGraph']['g_adj']) > 0\n","\n","                answers = jo['answers']\n","                normalized_answers = \"\"\n","                for x in answers:\n","                    normalized_answers += (x + \" \")\n","                target = jo['outSeq']\n","                n = len(jo['inGraph']['g_node_names'])\n","                adj_matrix = {}\n","                adj_matrix2 = {}\n","                graph = {'node_name_id2word':{}}\n","                for idx, nid in enumerate(jo['inGraph']['g_node_names']):\n","                    # replace answer token to \"<answer> answer <answer>\"\n","                    if jo['inGraph']['g_node_names'][nid] in answers:\n","                        graph['node_name_id2word'][nid] = \"<answer> \"+ jo['inGraph']['g_node_names'][nid] + \" <answer>\"\n","                    else:\n","                        graph['node_name_id2word'][nid] = jo['inGraph']['g_node_names'][nid]\n","\n","                    # create adj_matrix with 0 value\n","                    for idy, nid2 in enumerate(jo['inGraph']['g_node_names']):\n","                        if jo['inGraph']['g_node_names'][nid2] in answers:\n","                            adj_matrix2[\"<answer> \"+ jo['inGraph']['g_node_names'][nid2] + \" <answer>\"] = 0\n","                        else:\n","                            adj_matrix2[jo['inGraph']['g_node_names'][nid2]] = 0\n","                    if jo['inGraph']['g_node_names'][nid] in answers:\n","                        adj_matrix[\"<answer> \"+ jo['inGraph']['g_node_names'][nid] + \" <answer>\"] = adj_matrix2.copy()\n","                    else:\n","                        adj_matrix[jo['inGraph']['g_node_names'][nid]] = adj_matrix2.copy()\n","                for nid, val in jo['inGraph']['g_adj'].items():\n","                    for nid2, edge in val.items():\n","                        edge = edge.split('/')[-1]\n","                        adj_matrix[graph[\"node_name_id2word\"][nid]][graph[\"node_name_id2word\"][nid2]] = 1\n","                input_knowledge_graph = \"\"\n","                for i,ids in enumerate(list(adj_matrix.keys())):\n","                    input_knowledge_graph += (ids + \" \")\n","\n","                input, target = input_knowledge_graph, target\n","\n","                tokenized_row_input = \"\"\n","                tokenized_row_input = tokenizer.tokenize(input)\n","                if len(tokenized_row_input) > 512:\n","                    continue\n","\n","                tokenized_inputs = tokenizer.batch_encode_plus_self_attention(\n","                    [input_knowledge_graph], adjacancy_matrix=adj_matrix, max_length=512, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                tokenized_targets = tokenizer.batch_encode_plus(\n","                    [target], max_length=100, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","                SOURCE_ID_memmap = np.memmap(\n","                    filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,512) \n","                )\n","\n","                SOURCE_MASK_memmap = np.memmap(\n","                    filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512,512)\n","                )\n","\n","                SOURCE_CROSS_ATTENTION_MASK_memmap = np.memmap(\n","                    filename=SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512)\n","                )\n","\n","                TARGET_ID_memmap = np.memmap(\n","                    filename=TARGET_ID_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,100)\n","                )\n","\n","                TARGET_MASK_memmap = np.memmap(\n","                    filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,100)\n","                )\n","\n","                SOURCE_ID_memmap[list_index] = tokenized_inputs[\"input_ids\"].squeeze().numpy()\n","                SOURCE_MASK_memmap[list_index] = tokenized_inputs[\"attention_mask\"].squeeze().numpy()\n","                SOURCE_CROSS_ATTENTION_MASK_memmap[list_index] = tokenized_inputs[\"cross_attention_mask\"].squeeze().numpy()\n","\n","                TARGET_ID_memmap[list_index] = tokenized_targets[\"input_ids\"].squeeze().numpy()\n","                TARGET_MASK_memmap[list_index] = tokenized_targets[\"attention_mask\"].squeeze().numpy()\n","\n","                del SOURCE_ID_memmap\n","                del SOURCE_MASK_memmap\n","                del SOURCE_CROSS_ATTENTION_MASK_memmap\n","                del TARGET_ID_memmap\n","                del TARGET_MASK_memmap\n","\n","                if index%500==0:\n","                    print(f\"{index} was finished\", index)\n","\n","                list_index += 1\n","\n","    if not os.path.isfile(\"processed/mhqg-wq/TEST_SOURCE_ID_memmap.npy\"):\n","        SOURCE_ID_PATH = \"processed/mhqg-wq/TEST_SOURCE_ID_memmap.npy\"\n","        SOURCE_MASK_PATH = \"processed/mhqg-wq/TEST_SOURCE_MASK_memmap.npy\"\n","        SOURCE_CROSS_MASK_PATH = \"processed/mhqg-wq/TEST_SOURCE_CROSS_MASK_memmap.npy\"\n","        TARGET_ID_PATH = \"processed/mhqg-wq/TEST_TARGET_ID_memmap.npy\"\n","        TARGET_MASK_PATH = \"processed/mhqg-wq/TEST_TARGET_MASK_memmap.npy\"\n","        # input_length = 18624\n","        input_length = 2000\n","        SOURCE_ID_memmap = np.memmap(\n","        filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,512) \n","        )\n","\n","        SOURCE_MASK_memmap = np.memmap(\n","        filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512,512)\n","        )\n","\n","        SOURCE_CROSS_MASK_memmap = np.memmap(\n","        filename=SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512)\n","        )\n","\n","        TARGET_ID_memmap = np.memmap(\n","        filename=TARGET_ID_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,100)\n","        )\n","\n","        TARGET_MASK_memmap = np.memmap(\n","        filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,100)\n","        )\n","\n","        del SOURCE_ID_memmap\n","        del SOURCE_MASK_memmap\n","        del SOURCE_CROSS_MASK_memmap\n","        del TARGET_ID_memmap\n","        del TARGET_MASK_memmap\n","        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        with open(\"data/mhqg-wq/test.json\", 'r') as f:\n","            list_index = 0\n","            for index, line in enumerate(f):\n","                line = line.strip()\n","                jo = json.loads(line, object_pairs_hook=OrderedDict)\n","                assert len(jo['inGraph']['g_adj']) > 0\n","\n","                answers = jo['answers']\n","                normalized_answers = \"\"\n","                for x in answers:\n","                    normalized_answers += (x + \" \")\n","                target = jo['outSeq']\n","                n = len(jo['inGraph']['g_node_names'])\n","                adj_matrix = {}\n","                adj_matrix2 = {}\n","                graph = {'node_name_id2word':{}}\n","                for idx, nid in enumerate(jo['inGraph']['g_node_names']):\n","                    # replace answer token to \"<answer> answer <answer>\"\n","                    if jo['inGraph']['g_node_names'][nid] in answers:\n","                        graph['node_name_id2word'][nid] = \"<answer> \"+ jo['inGraph']['g_node_names'][nid] + \" <answer>\"\n","                    else:\n","                        graph['node_name_id2word'][nid] = jo['inGraph']['g_node_names'][nid]\n","\n","                    # create adj_matrix with 0 value\n","                    for idy, nid2 in enumerate(jo['inGraph']['g_node_names']):\n","                        if jo['inGraph']['g_node_names'][nid2] in answers:\n","                            adj_matrix2[\"<answer> \"+ jo['inGraph']['g_node_names'][nid2] + \" <answer>\"] = 0\n","                        else:\n","                            adj_matrix2[jo['inGraph']['g_node_names'][nid2]] = 0\n","                    if jo['inGraph']['g_node_names'][nid] in answers:\n","                        adj_matrix[\"<answer> \"+ jo['inGraph']['g_node_names'][nid] + \" <answer>\"] = adj_matrix2.copy()\n","                    else:\n","                        adj_matrix[jo['inGraph']['g_node_names'][nid]] = adj_matrix2.copy()\n","                for nid, val in jo['inGraph']['g_adj'].items():\n","                    for nid2, edge in val.items():\n","                        edge = edge.split('/')[-1]\n","                        adj_matrix[graph[\"node_name_id2word\"][nid]][graph[\"node_name_id2word\"][nid2]] = 1\n","                input_knowledge_graph = \"\"\n","                for i,ids in enumerate(list(adj_matrix.keys())):\n","                    input_knowledge_graph += (ids + \" \")\n","\n","                input, target = input_knowledge_graph, target\n","\n","                tokenized_row_input = \"\"\n","                tokenized_row_input = tokenizer.tokenize(input)\n","                if len(tokenized_row_input) > 512:\n","                    continue\n","\n","                tokenized_inputs = tokenizer.batch_encode_plus_self_attention(\n","                    [input_knowledge_graph], adjacancy_matrix=adj_matrix, max_length=512, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                tokenized_targets = tokenizer.batch_encode_plus(\n","                    [target], max_length=100, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","                SOURCE_ID_memmap = np.memmap(\n","                    filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,512) \n","                )\n","\n","                SOURCE_MASK_memmap = np.memmap(\n","                    filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512,512)\n","                )\n","\n","                SOURCE_CROSS_ATTENTION_MASK_memmap = np.memmap(\n","                    filename=SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512)\n","                )\n","\n","                TARGET_ID_memmap = np.memmap(\n","                    filename=TARGET_ID_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,100)\n","                )\n","\n","                TARGET_MASK_memmap = np.memmap(\n","                    filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,100)\n","                )\n","\n","                SOURCE_ID_memmap[list_index] = tokenized_inputs[\"input_ids\"].squeeze().numpy()\n","                SOURCE_MASK_memmap[list_index] = tokenized_inputs[\"attention_mask\"].squeeze().numpy()\n","                SOURCE_CROSS_ATTENTION_MASK_memmap[list_index] = tokenized_inputs[\"cross_attention_mask\"].squeeze().numpy()\n","\n","                TARGET_ID_memmap[list_index] = tokenized_targets[\"input_ids\"].squeeze().numpy()\n","                TARGET_MASK_memmap[list_index] = tokenized_targets[\"attention_mask\"].squeeze().numpy()\n","\n","                del SOURCE_ID_memmap\n","                del SOURCE_MASK_memmap\n","                del SOURCE_CROSS_ATTENTION_MASK_memmap\n","                del TARGET_ID_memmap\n","                del TARGET_MASK_memmap\n","\n","                if index%500==0:\n","                    print(f\"{index} was finished\", index)\n","\n","                list_index += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BHZ0Bx1bjNZw"},"source":["def preprocess_pq(config):\n","    # preprocess input data\n","    if not os.path.isfile(\"processed/mhqg-pq/SOURCE_ID_memmap.npy\"):\n","        SOURCE_ID_PATH = \"processed/mhqg-pq/SOURCE_ID_memmap.npy\"\n","        SOURCE_MASK_PATH = \"processed/mhqg-pq/SOURCE_MASK_memmap.npy\"\n","        SOURCE_CROSS_MASK_PATH = \"processed/mhqg-pq/SOURCE_CROSS_MASK_memmap.npy\"\n","        TARGET_ID_PATH = \"processed/mhqg-pq/TARGET_ID_memmap.npy\"\n","        TARGET_MASK_PATH = \"processed/mhqg-pq/TARGET_MASK_memmap.npy\"\n","        input_length = 9793\n","        SOURCE_ID_memmap = np.memmap(\n","        filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,512) \n","        )\n","\n","        SOURCE_MASK_memmap = np.memmap(\n","        filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512,512)\n","        )\n","\n","        SOURCE_CROSS_MASK_memmap = np.memmap(\n","        filename=SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512)\n","        )\n","\n","        TARGET_ID_memmap = np.memmap(\n","        filename=TARGET_ID_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,100)\n","        )\n","\n","        TARGET_MASK_memmap = np.memmap(\n","        filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,100)\n","        )\n","\n","        del SOURCE_ID_memmap\n","        del SOURCE_MASK_memmap\n","        del SOURCE_CROSS_MASK_memmap\n","        del TARGET_ID_memmap\n","        del TARGET_MASK_memmap\n","        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        with open(\"data/mhqg-pq/train.json\", 'r') as f:\n","            list_index = 0\n","            for index, line in enumerate(f):\n","                line = line.strip()\n","                jo = json.loads(line, object_pairs_hook=OrderedDict)\n","                assert len(jo['inGraph']['g_adj']) > 0\n","\n","                answers = jo['answers']\n","                normalized_answers = \"\"\n","                for x in answers:\n","                    normalized_answers += (x + \" \")\n","                target = jo['outSeq']\n","\n","                # used for model input\n","                adj_matrix = {}\n","                # used for create adjacency matrix\n","                adj_matrix2 = {}\n","                # map from id to node name\n","                graph = {'node_name_id2word':{}}\n","\n","                # make graph mapping from id to node name\n","                for idx, nid in enumerate(jo['inGraph']['g_node_names']):\n","                    # replace answer token to \"<answer> answer <answer>\"\n","                    if jo['inGraph']['g_node_names'][nid] in answers:\n","                        graph['node_name_id2word'][nid] = \"<answer> \"+ jo['inGraph']['g_node_names'][nid] + \" <answer>\"\n","                    else:\n","                        graph['node_name_id2word'][nid] = jo['inGraph']['g_node_names'][nid]\n","\n","                    # create adj_matrix with 0 value\n","                    for idy, nid2 in enumerate(jo['inGraph']['g_node_names']):\n","                        if jo['inGraph']['g_node_names'][nid2] in answers:\n","                            adj_matrix2[\"<answer> \"+ jo['inGraph']['g_node_names'][nid2] + \" <answer>\"] = 0\n","                        else:\n","                            adj_matrix2[jo['inGraph']['g_node_names'][nid2]] = 0\n","\n","                    if jo['inGraph']['g_node_names'][nid] in answers:\n","                        adj_matrix[\"<answer> \"+ jo['inGraph']['g_node_names'][nid] + \" <answer>\"] = adj_matrix2.copy()\n","                    else:\n","                        adj_matrix[jo['inGraph']['g_node_names'][nid]] = adj_matrix2.copy()\n","\n","\n","                for nid, val in jo['inGraph']['g_adj'].items():\n","                    for nid2, edge in val.items():\n","                        adj_matrix[graph[\"node_name_id2word\"][nid]][graph[\"node_name_id2word\"][nid2]] = 1\n","                input_knowledge_graph = \"\"\n","                for i,ids in enumerate(list(adj_matrix.keys())):\n","                    input_knowledge_graph += (ids + \" \")\n","\n","                input, target = input_knowledge_graph, target\n","\n","                tokenized_row_input = \"\"\n","                tokenized_row_input = tokenizer.tokenize(input)\n","                if len(tokenized_row_input) > 512:\n","                    continue\n","\n","                tokenized_inputs = tokenizer.batch_encode_plus_self_attention(\n","                    [input_knowledge_graph], adjacancy_matrix=adj_matrix, max_length=512, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                tokenized_targets = tokenizer.batch_encode_plus(\n","                    [target], max_length=100, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","                SOURCE_ID_memmap = np.memmap(\n","                    filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,512) \n","                )\n","\n","                SOURCE_MASK_memmap = np.memmap(\n","                    filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512,512)\n","                )\n","\n","                SOURCE_CROSS_ATTENTION_MASK_memmap = np.memmap(\n","                    filename=SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512)\n","                )\n","\n","                TARGET_ID_memmap = np.memmap(\n","                    filename=TARGET_ID_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,100)\n","                )\n","\n","                TARGET_MASK_memmap = np.memmap(\n","                    filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,100)\n","                )\n","\n","                SOURCE_ID_memmap[list_index] = tokenized_inputs[\"input_ids\"].squeeze().numpy()\n","                SOURCE_MASK_memmap[list_index] = tokenized_inputs[\"attention_mask\"].squeeze().numpy()\n","                SOURCE_CROSS_ATTENTION_MASK_memmap[list_index] = tokenized_inputs[\"cross_attention_mask\"].squeeze().numpy()\n","\n","                TARGET_ID_memmap[list_index] = tokenized_targets[\"input_ids\"].squeeze().numpy()\n","                TARGET_MASK_memmap[list_index] = tokenized_targets[\"attention_mask\"].squeeze().numpy()\n","\n","                del SOURCE_ID_memmap\n","                del SOURCE_MASK_memmap\n","                del SOURCE_CROSS_ATTENTION_MASK_memmap\n","                del TARGET_ID_memmap\n","                del TARGET_MASK_memmap\n","\n","                if index%500==0:\n","                    print(f\"{index} was finished\", index)\n","\n","                list_index += 1\n","\n","    # preprocess validation.json \n","    if not os.path.isfile(\"processed/mhqg-pq/VAL_SOURCE_ID_memmap.npy\"):\n","        SOURCE_ID_PATH = \"processed/mhqg-pq/VAL_SOURCE_ID_memmap.npy\"\n","        SOURCE_MASK_PATH = \"processed/mhqg-pq/VAL_SOURCE_MASK_memmap.npy\"\n","        SOURCE_CROSS_MASK_PATH = \"processed/mhqg-pq/VAL_SOURCE_CROSS_MASK_memmap.npy\"\n","        TARGET_ID_PATH = \"processed/mhqg-pq/VAL_TARGET_ID_memmap.npy\"\n","        TARGET_MASK_PATH = \"processed/mhqg-pq/VAL_TARGET_MASK_memmap.npy\"\n","        input_length = 1000\n","        SOURCE_ID_memmap = np.memmap(\n","        filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,512) \n","        )\n","\n","        SOURCE_MASK_memmap = np.memmap(\n","        filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512,512)\n","        )\n","\n","        SOURCE_CROSS_MASK_memmap = np.memmap(\n","        filename=SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512)\n","        )\n","\n","        TARGET_ID_memmap = np.memmap(\n","        filename=TARGET_ID_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,100)\n","        )\n","\n","        TARGET_MASK_memmap = np.memmap(\n","        filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,100)\n","        )\n","\n","        del SOURCE_ID_memmap\n","        del SOURCE_MASK_memmap\n","        del SOURCE_CROSS_MASK_memmap\n","        del TARGET_ID_memmap\n","        del TARGET_MASK_memmap\n","        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        with open(\"data/mhqg-pq/dev.json\", 'r') as f:\n","            list_index = 0\n","            for index, line in enumerate(f):\n","                line = line.strip()\n","                jo = json.loads(line, object_pairs_hook=OrderedDict)\n","                assert len(jo['inGraph']['g_adj']) > 0\n","\n","                answers = jo['answers']\n","                normalized_answers = \"\"\n","                for x in answers:\n","                    normalized_answers += (x + \" \")\n","                target = jo['outSeq']\n","\n","                # used for model input\n","                adj_matrix = {}\n","                # used for create adjacency matrix\n","                adj_matrix2 = {}\n","                # map from id to node name\n","                graph = {'node_name_id2word':{}}\n","\n","                # make graph mapping from id to node name\n","                for idx, nid in enumerate(jo['inGraph']['g_node_names']):\n","                    # replace answer token to \"<answer> answer <answer>\"\n","                    if jo['inGraph']['g_node_names'][nid] in answers:\n","                        graph['node_name_id2word'][nid] = \"<answer> \"+ jo['inGraph']['g_node_names'][nid] + \" <answer>\"\n","                    else:\n","                        graph['node_name_id2word'][nid] = jo['inGraph']['g_node_names'][nid]\n","\n","                    # create adj_matrix with 0 value\n","                    for idy, nid2 in enumerate(jo['inGraph']['g_node_names']):\n","                        if jo['inGraph']['g_node_names'][nid2] in answers:\n","                            adj_matrix2[\"<answer> \"+ jo['inGraph']['g_node_names'][nid2] + \" <answer>\"] = 0\n","                        else:\n","                            adj_matrix2[jo['inGraph']['g_node_names'][nid2]] = 0\n","\n","                    if jo['inGraph']['g_node_names'][nid] in answers:\n","                        adj_matrix[\"<answer> \"+ jo['inGraph']['g_node_names'][nid] + \" <answer>\"] = adj_matrix2.copy()\n","                    else:\n","                        adj_matrix[jo['inGraph']['g_node_names'][nid]] = adj_matrix2.copy()\n","\n","\n","                for nid, val in jo['inGraph']['g_adj'].items():\n","                    for nid2, edge in val.items():\n","                        adj_matrix[graph[\"node_name_id2word\"][nid]][graph[\"node_name_id2word\"][nid2]] = 1\n","                input_knowledge_graph = \"\"\n","                for i,ids in enumerate(list(adj_matrix.keys())):\n","                    input_knowledge_graph += (ids + \" \")\n","\n","                input, target = input_knowledge_graph, target\n","\n","                tokenized_row_input = \"\"\n","                tokenized_row_input = tokenizer.tokenize(input)\n","                if len(tokenized_row_input) > 512:\n","                    continue\n","\n","                tokenized_inputs = tokenizer.batch_encode_plus_self_attention(\n","                    [input_knowledge_graph], adjacancy_matrix=adj_matrix, max_length=512, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                tokenized_targets = tokenizer.batch_encode_plus(\n","                    [target], max_length=100, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","                SOURCE_ID_memmap = np.memmap(\n","                    filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,512) \n","                )\n","\n","                SOURCE_MASK_memmap = np.memmap(\n","                    filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512,512)\n","                )\n","\n","                SOURCE_CROSS_ATTENTION_MASK_memmap = np.memmap(\n","                    filename=SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512)\n","                )\n","\n","                TARGET_ID_memmap = np.memmap(\n","                    filename=TARGET_ID_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,100)\n","                )\n","\n","                TARGET_MASK_memmap = np.memmap(\n","                    filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,100)\n","                )\n","\n","                SOURCE_ID_memmap[list_index] = tokenized_inputs[\"input_ids\"].squeeze().numpy()\n","                SOURCE_MASK_memmap[list_index] = tokenized_inputs[\"attention_mask\"].squeeze().numpy()\n","                SOURCE_CROSS_ATTENTION_MASK_memmap[list_index] = tokenized_inputs[\"cross_attention_mask\"].squeeze().numpy()\n","\n","                TARGET_ID_memmap[list_index] = tokenized_targets[\"input_ids\"].squeeze().numpy()\n","                TARGET_MASK_memmap[list_index] = tokenized_targets[\"attention_mask\"].squeeze().numpy()\n","\n","                del SOURCE_ID_memmap\n","                del SOURCE_MASK_memmap\n","                del SOURCE_CROSS_ATTENTION_MASK_memmap\n","                del TARGET_ID_memmap\n","                del TARGET_MASK_memmap\n","\n","                if index%500==0:\n","                    print(f\"{index} was finished\", index)\n","\n","                list_index += 1\n","\n","    # preprocess test.json\n","    if not os.path.isfile(\"processed/mhqg-pq/TEST_SOURCE_ID_memmap.npy\"):\n","        SOURCE_ID_PATH = \"processed/mhqg-pq/TEST_SOURCE_ID_memmap.npy\"\n","        SOURCE_MASK_PATH = \"processed/mhqg-pq/TEST_SOURCE_MASK_memmap.npy\"\n","        SOURCE_CROSS_MASK_PATH = \"processed/mhqg-pq/TEST_SOURCE_CROSS_MASK_memmap.npy\"\n","        TARGET_ID_PATH = \"processed/mhqg-pq/TEST_TARGET_ID_memmap.npy\"\n","        TARGET_MASK_PATH = \"processed/mhqg-pq/TEST_TARGET_MASK_memmap.npy\"\n","        # input_length = 18624\n","        input_length = 1000\n","        SOURCE_ID_memmap = np.memmap(\n","        filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,512) \n","        )\n","\n","        SOURCE_MASK_memmap = np.memmap(\n","        filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512,512)\n","        )\n","\n","        SOURCE_CROSS_MASK_memmap = np.memmap(\n","        filename=SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512)\n","        )\n","\n","        TARGET_ID_memmap = np.memmap(\n","        filename=TARGET_ID_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,100)\n","        )\n","\n","        TARGET_MASK_memmap = np.memmap(\n","        filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,100)\n","        )\n","\n","        del SOURCE_ID_memmap\n","        del SOURCE_MASK_memmap\n","        del SOURCE_CROSS_MASK_memmap\n","        del TARGET_ID_memmap\n","        del TARGET_MASK_memmap\n","        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        with open(\"data/mhqg-pq/test.json\", 'r') as f:\n","            list_index = 0\n","            for index, line in enumerate(f):\n","                line = line.strip()\n","                jo = json.loads(line, object_pairs_hook=OrderedDict)\n","                assert len(jo['inGraph']['g_adj']) > 0\n","\n","                answers = jo['answers']\n","                normalized_answers = \"\"\n","                for x in answers:\n","                    normalized_answers += (x + \" \")\n","                target = jo['outSeq']\n","\n","                # used for model input\n","                adj_matrix = {}\n","                # used for create adjacency matrix\n","                adj_matrix2 = {}\n","                # map from id to node name\n","                graph = {'node_name_id2word':{}}\n","\n","                # make graph mapping from id to node name\n","                for idx, nid in enumerate(jo['inGraph']['g_node_names']):\n","                    # replace answer token to \"<answer> answer <answer>\"\n","                    if jo['inGraph']['g_node_names'][nid] in answers:\n","                        graph['node_name_id2word'][nid] = \"<answer> \"+ jo['inGraph']['g_node_names'][nid] + \" <answer>\"\n","                    else:\n","                        graph['node_name_id2word'][nid] = jo['inGraph']['g_node_names'][nid]\n","\n","                    # create adj_matrix with 0 value\n","                    for idy, nid2 in enumerate(jo['inGraph']['g_node_names']):\n","                        if jo['inGraph']['g_node_names'][nid2] in answers:\n","                            adj_matrix2[\"<answer> \"+ jo['inGraph']['g_node_names'][nid2] + \" <answer>\"] = 0\n","                        else:\n","                            adj_matrix2[jo['inGraph']['g_node_names'][nid2]] = 0\n","\n","                    if jo['inGraph']['g_node_names'][nid] in answers:\n","                        adj_matrix[\"<answer> \"+ jo['inGraph']['g_node_names'][nid] + \" <answer>\"] = adj_matrix2.copy()\n","                    else:\n","                        adj_matrix[jo['inGraph']['g_node_names'][nid]] = adj_matrix2.copy()\n","\n","\n","                for nid, val in jo['inGraph']['g_adj'].items():\n","                    for nid2, edge in val.items():\n","                        adj_matrix[graph[\"node_name_id2word\"][nid]][graph[\"node_name_id2word\"][nid2]] = 1\n","                input_knowledge_graph = \"\"\n","                for i,ids in enumerate(list(adj_matrix.keys())):\n","                    input_knowledge_graph += (ids + \" \")\n","\n","                input, target = input_knowledge_graph, target\n","\n","                tokenized_row_input = \"\"\n","                tokenized_row_input = tokenizer.tokenize(input)\n","                if len(tokenized_row_input) > 512:\n","                    continue\n","\n","                tokenized_inputs = tokenizer.batch_encode_plus_self_attention(\n","                    [input_knowledge_graph], adjacancy_matrix=adj_matrix, max_length=512, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                tokenized_targets = tokenizer.batch_encode_plus(\n","                    [target], max_length=100, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","                SOURCE_ID_memmap = np.memmap(\n","                    filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,512) \n","                )\n","\n","                SOURCE_MASK_memmap = np.memmap(\n","                    filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512,512)\n","                )\n","\n","                SOURCE_CROSS_ATTENTION_MASK_memmap = np.memmap(\n","                    filename=SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512)\n","                )\n","\n","                TARGET_ID_memmap = np.memmap(\n","                    filename=TARGET_ID_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,100)\n","                )\n","\n","                TARGET_MASK_memmap = np.memmap(\n","                    filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,100)\n","                )\n","\n","                SOURCE_ID_memmap[list_index] = tokenized_inputs[\"input_ids\"].squeeze().numpy()\n","                SOURCE_MASK_memmap[list_index] = tokenized_inputs[\"attention_mask\"].squeeze().numpy()\n","                SOURCE_CROSS_ATTENTION_MASK_memmap[list_index] = tokenized_inputs[\"cross_attention_mask\"].squeeze().numpy()\n","\n","                TARGET_ID_memmap[list_index] = tokenized_targets[\"input_ids\"].squeeze().numpy()\n","                TARGET_MASK_memmap[list_index] = tokenized_targets[\"attention_mask\"].squeeze().numpy()\n","\n","                del SOURCE_ID_memmap\n","                del SOURCE_MASK_memmap\n","                del SOURCE_CROSS_ATTENTION_MASK_memmap\n","                del TARGET_ID_memmap\n","                del TARGET_MASK_memmap\n","\n","                if index%500==0:\n","                    print(f\"{index} was finished\", index)\n","\n","                list_index += 1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XlwCx1n5jNZx"},"source":["class Preprocess():\n","    def __init__(self, config):\n","        if config.experiment.data==\"mhqg-wq\":\n","            preprocess_wq(config)\n","        elif config.experiment.data==\"mhqg-pq\":\n","            preprocess_pq(config)\n","        else:\n","            raise"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5K0qw5n9jNZx"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"LqoyrzjLjNZx"},"source":["from torch.utils.data import Dataset\n","import numpy as np\n","import json\n","import torch\n","from collections import Counter, defaultdict, OrderedDict\n","import gc\n","import os\n","\n","class JsonDatasetWQ(Dataset):\n","    def __init__(self, tokenizer, data_dir, type_path, input_max_len=512, target_max_len=512, mode=None):\n","        assert mode==\"Train\" or mode==\"Val\" or mode==\"Test\"\n","        self.file_path = os.path.join(data_dir, type_path)\n","        \n","        self.input_max_len = input_max_len\n","        self.target_max_len = target_max_len\n","        self.tokenizer = tokenizer\n","        self.inputs = []\n","        self.targets = []\n"," \n","        self.mode = mode\n"," \n","        if mode==\"Train\":\n","          self.SOURCE_ID_PATH = \"processed/mhqg-wq/SOURCE_ID_memmap.npy\"\n","          self.SOURCE_MASK_PATH = \"processed/mhqg-wq/SOURCE_MASK_memmap.npy\"\n","          self.SOURCE_CROSS_MASK_PATH = \"processed/mhqg-wq/SOURCE_CROSS_MASK_memmap.npy\"\n","          self.TARGET_ID_PATH = \"processed/mhqg-wq/TARGET_ID_memmap.npy\"\n","          self.TARGET_MASK_PATH = \"processed/mhqg-wq/TARGET_MASK_memmap.npy\"\n","        #   self.input_length = 18624\n","          self.input_length = 18989\n","          self.list_index = 18989\n","        elif mode==\"Val\":\n","          self.SOURCE_ID_PATH = \"processed/mhqg-wq/VAL_SOURCE_ID_memmap.npy\"\n","          self.SOURCE_MASK_PATH = \"processed/mhqg-wq/VAL_SOURCE_MASK_memmap.npy\"\n","          self.SOURCE_CROSS_MASK_PATH = \"processed/mhqg-wq/VAL_SOURCE_CROSS_MASK_memmap.npy\"\n","          self.TARGET_ID_PATH = \"processed/mhqg-wq/VAL_TARGET_ID_memmap.npy\"\n","          self.TARGET_MASK_PATH = \"processed/mhqg-wq/VAL_TARGET_MASK_memmap.npy\"\n","          self.input_length = 2000\n","          self.list_index = 2000\n","        #   self.input_length = 1985\n","        elif mode==\"Test\":\n","          self.SOURCE_ID_PATH = \"processed/mhqg-wq/TEST_SOURCE_ID_memmap.npy\"\n","          self.SOURCE_MASK_PATH = \"processed/mhqg-wq/TEST_SOURCE_MASK_memmap.npy\"\n","          self.SOURCE_CROSS_MASK_PATH = \"processed/mhqg-wq/TEST_SOURCE_CROSS_MASK_memmap.npy\"\n","          self.TARGET_ID_PATH = \"processed/mhqg-wq/TEST_TARGET_ID_memmap.npy\"\n","          self.TARGET_MASK_PATH = \"processed/mhqg-wq/TEST_TARGET_MASK_memmap.npy\"\n","          self.input_length = 2000\n","          self.list_index = 2000\n","          \n","        #   self.input_length = 1985\n"," \n","        self.SOURCE_ID_memmap = np.memmap(\n","          filename=self.SOURCE_ID_PATH, dtype=np.int64, mode=\"r\",shape=(self.input_length,512) \n","        )\n"," \n","        self.SOURCE_MASK_memmap = np.memmap(\n","          filename=self.SOURCE_MASK_PATH, dtype=np.int64, mode=\"r\", shape=(self.input_length,512,512)\n","        )\n","\n","        self.VAL_SOURCE_CROSS_MASK_memmap = np.memmap(\n","          filename=self.SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"r\", shape=(self.input_length,512)\n","        )\n"," \n","        self.TARGET_ID_memmap = np.memmap(\n","          filename=self.TARGET_ID_PATH, dtype=np.int64, mode=\"r\", shape=(self.input_length,100)\n","        )\n"," \n","        self.TARGET_MASK_memmap = np.memmap(\n","          filename=self.TARGET_MASK_PATH, dtype=np.int64, mode=\"r\",shape=(self.input_length,100)\n","        )\n","\n","  \n","    def __len__(self):\n","        return self.list_index\n","  \n","    def __getitem__(self, index):\n","        return {\"source_ids\": torch.from_numpy(np.array(self.SOURCE_ID_memmap[index])).squeeze(), \"source_mask\": torch.from_numpy(np.array(self.SOURCE_MASK_memmap[index])).squeeze(), \"cross_attention_mask\":torch.from_numpy(np.array(self.VAL_SOURCE_CROSS_MASK_memmap[index])).squeeze(),\n","                \"target_ids\": torch.from_numpy(np.array(self.TARGET_ID_memmap[index])).squeeze(), \"target_mask\": torch.from_numpy(np.array(self.TARGET_MASK_memmap[index])).squeeze()}"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Y80ocv2Ne4M_"},"source":["from torch.utils.data import Dataset\n","import numpy as np\n","import json\n","import torch\n","from collections import Counter, defaultdict, OrderedDict\n","import gc\n","import os\n","\n","class JsonDatasetPQ(Dataset):\n","    def __init__(self, tokenizer, data_dir, type_path, input_max_len=512, target_max_len=512, mode=None):\n","        assert mode==\"Train\" or mode==\"Val\" or mode==\"Test\"\n","        self.file_path = os.path.join(data_dir, type_path)\n","        \n","        self.input_max_len = input_max_len\n","        self.target_max_len = target_max_len\n","        self.tokenizer = tokenizer\n","        self.inputs = []\n","        self.targets = []\n"," \n","        self.mode = mode\n"," \n","        if mode==\"Train\":\n","          self.SOURCE_ID_PATH = \"processed/mhqg-pq/SOURCE_ID_memmap.npy\"\n","          self.SOURCE_MASK_PATH = \"processed/mhqg-pq/SOURCE_MASK_memmap.npy\"\n","          self.SOURCE_CROSS_MASK_PATH = \"processed/mhqg-pq/SOURCE_CROSS_MASK_memmap.npy\"\n","          self.TARGET_ID_PATH = \"processed/mhqg-pq/TARGET_ID_memmap.npy\"\n","          self.TARGET_MASK_PATH = \"processed/mhqg-pq/TARGET_MASK_memmap.npy\"\n","          self.input_length = 9793\n","          self.list_index = 9793\n","        elif mode==\"Val\":\n","          self.SOURCE_ID_PATH = \"processed/mhqg-pq/VAL_SOURCE_ID_memmap.npy\"\n","          self.SOURCE_MASK_PATH = \"processed/mhqg-pq/VAL_SOURCE_MASK_memmap.npy\"\n","          self.SOURCE_CROSS_MASK_PATH = \"processed/mhqg-pq/VAL_SOURCE_CROSS_MASK_memmap.npy\"\n","          self.TARGET_ID_PATH = \"processed/mhqg-pq/VAL_TARGET_ID_memmap.npy\"\n","          self.TARGET_MASK_PATH = \"processed/mhqg-pq/VAL_TARGET_MASK_memmap.npy\"\n","          self.input_length = 1000\n","          self.list_index = 1000\n","        elif mode==\"Test\":\n","          self.SOURCE_ID_PATH = \"processed/mhqg-pq/TEST_SOURCE_ID_memmap.npy\"\n","          self.SOURCE_MASK_PATH = \"processed/mhqg-pq/TEST_SOURCE_MASK_memmap.npy\"\n","          self.SOURCE_CROSS_MASK_PATH = \"processed/mhqg-pq/TEST_SOURCE_CROSS_MASK_memmap.npy\"\n","          self.TARGET_ID_PATH = \"processed/mhqg-pq/TEST_TARGET_ID_memmap.npy\"\n","          self.TARGET_MASK_PATH = \"processed/mhqg-pq/TEST_TARGET_MASK_memmap.npy\"\n","          self.input_length = 1000\n","          self.list_index = 1000\n","          \n"," \n","        self.SOURCE_ID_memmap = np.memmap(\n","          filename=self.SOURCE_ID_PATH, dtype=np.int64, mode=\"r\",shape=(self.input_length,512) \n","        )\n"," \n","        self.SOURCE_MASK_memmap = np.memmap(\n","          filename=self.SOURCE_MASK_PATH, dtype=np.int64, mode=\"r\", shape=(self.input_length,512,512)\n","        )\n","\n","        self.VAL_SOURCE_CROSS_MASK_memmap = np.memmap(\n","          filename=self.SOURCE_CROSS_MASK_PATH, dtype=np.int64, mode=\"r\", shape=(self.input_length,512)\n","        )\n"," \n","        self.TARGET_ID_memmap = np.memmap(\n","          filename=self.TARGET_ID_PATH, dtype=np.int64, mode=\"r\", shape=(self.input_length,100)\n","        )\n"," \n","        self.TARGET_MASK_memmap = np.memmap(\n","          filename=self.TARGET_MASK_PATH, dtype=np.int64, mode=\"r\",shape=(self.input_length,100)\n","        )\n","\n","  \n","    def __len__(self):\n","        return self.list_index\n","  \n","    def __getitem__(self, index):\n","        return {\"source_ids\": torch.from_numpy(np.array(self.SOURCE_ID_memmap[index])).squeeze(), \"source_mask\": torch.from_numpy(np.array(self.SOURCE_MASK_memmap[index])).squeeze(), \"cross_attention_mask\":torch.from_numpy(np.array(self.VAL_SOURCE_CROSS_MASK_memmap[index])).squeeze(),\n","                \"target_ids\": torch.from_numpy(np.array(self.TARGET_ID_memmap[index])).squeeze(), \"target_mask\": torch.from_numpy(np.array(self.TARGET_MASK_memmap[index])).squeeze()}"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uEAB5N6njNZy"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"8ORkXx80jNZy"},"source":["import pytorch_lightning as pl\n","from AdjacencyAttentionWithoutSelfloopTransformers import (\n","  AdamW,\n","  T5ForConditionalGeneration,\n","  T5Tokenizer,\n","  get_linear_schedule_with_warmup\n",")\n","from torch.utils.data import DataLoader\n","\n","from tqdm.auto import tqdm\n","\n","import pandas as pd\n","from core.evaluation.eval import QGEvalCap\n","\n","from omegaconf import DictConfig\n","\n","def saveOutputs(hparams: DictConfig, inputs: list, outputs: list, targets: list) -> None:\n","  data = pd.DataFrame(list(zip(inputs, outputs, targets)), columns =['inputs', 'outputs', 'targets'])\n","  data.to_csv(os.path.join(\"out\",hparams.experiment.model_dir.split(\"/\")[-1]+\".csv\"),index=False, header=True)\n","\n","def run_eval(target_src, decoded_text) -> dict:\n","  assert len(target_src) == len(decoded_text)\n","  eval_targets = {}\n","  eval_predictions = {}\n","  for idx in range(len(target_src)):\n","      eval_targets[idx] = [target_src[idx]]\n","      eval_predictions[idx] = [decoded_text[idx]]\n","\n","  QGEval = QGEvalCap(eval_targets, eval_predictions)\n","  scores = QGEval.evaluate()\n","  return scores\n","\n","class T5FineTuner(pl.LightningModule):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        # 事前学習済みモデルの読み込み\n","        self.model = T5ForConditionalGeneration.from_pretrained(config.experiment.model_name_or_path)\n","\n","        # トークナイザーの読み込み\n","        self.tokenizer = T5Tokenizer.from_pretrained(config.experiment.tokenizer_name_or_path, is_fast=True)\n","\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        self.save_hyperparameters()\n","\n","    def forward(self, input_ids, attention_mask=None, cross_attention_mask=None, decoder_input_ids=None, \n","                decoder_attention_mask=None, labels=None, mode=None):\n","        \"\"\"順伝搬\"\"\"\n","        return self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            cross_attention_mask=cross_attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            labels=labels,\n","            three_dim_attention_mask=True\n","        )\n","\n","    def _step(self, batch, mode=None):\n","        \"\"\"ロス計算\"\"\"\n","        labels = batch[\"target_ids\"].detach().clone()\n","\n","        # All labels set to -100 are ignored (masked), \n","        # the loss is only computed for labels in [0, ..., config.vocab_size]\n","        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","        outputs = self(\n","            input_ids=batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            cross_attention_mask=batch[\"cross_attention_mask\"],\n","            decoder_attention_mask=batch['target_mask'],\n","            labels=labels,\n","            mode=mode\n","        )\n","\n","        return outputs\n","\n","    def training_step(self, batch, batch_idx):\n","        \"\"\"訓練ステップ処理\"\"\"\n","        outputs = self._step(batch)\n","        loss = outputs[0]\n","        logit = outputs[1]\n","        # logging metrics we calculated by hand\n","        self.log('train/loss', loss, on_epoch=True)\n","        return {\"loss\": loss}\n","\n","    def validation_step(self, batch, batch_idx):\n","        \"\"\"バリデーションステップ処理\"\"\"\n","        outputs = self._step(batch)\n","        loss = outputs[0]\n","        logit = outputs[1]\n","        self.log(\"valid/loss_epoch\", loss)  # default on val/test is on_epoch only\n","        # return {\"val_loss\": loss}\n","\n","    def test_epoch_end(self, training_step_outputs):\n","        saveOutputs(self.config, self.inputs, self.outputs, self.targets)\n","        scores = run_eval(self.targets, self.outputs)\n","        wandb.log(scores)\n","\n","    def configure_optimizers(self):\n","        \"\"\"オプティマイザーとスケジューラーを作成する\"\"\"\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() \n","                            if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.config.training.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() \n","                            if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, \n","                          lr=self.config.training.learning_rate, \n","                          eps=self.config.training.adam_epsilon)\n","        self.optimizer = optimizer\n","\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer, num_warmup_steps=self.config.training.warmup_steps, \n","            num_training_steps=self.t_total\n","        )\n","        self.scheduler = scheduler\n","\n","        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}]\n","\n","    def get_dataset(self, tokenizer, type_path, args, mode=None):\n","        \"\"\"データセットを作成する\"\"\"\n","        if args.experiment.data==\"mhqg-wq\":\n","          return JsonDatasetWQ(\n","            tokenizer=tokenizer, \n","            data_dir=args.experiment.data_dir, \n","            type_path=type_path, \n","            input_max_len=args.model.max_input_length,\n","            target_max_len=args.model.max_target_length,\n","            mode=mode)\n","        else:\n","          return JsonDatasetPQ(\n","            tokenizer=tokenizer, \n","            data_dir=args.experiment.data_dir, \n","            type_path=type_path, \n","            input_max_len=args.model.max_input_length,\n","            target_max_len=args.model.max_target_length,\n","            mode=mode)\n","    \n","    def setup(self, stage=None):\n","        \"\"\"初期設定（データセットの読み込み）\"\"\"\n","        if stage == 'fit' or stage is None:\n","            train_dataset = self.get_dataset(tokenizer=self.tokenizer, \n","                                             type_path=\"train.json\", args=self.config, mode=\"Train\")\n","            self.train_dataset = train_dataset\n","\n","            val_dataset = self.get_dataset(tokenizer=self.tokenizer, \n","                                           type_path=\"dev.json\", args=self.config, mode=\"Val\")\n","            self.val_dataset = val_dataset\n","\n","            self.t_total = (\n","                (len(train_dataset) // (self.config.training.train_batch_size * max(1, self.config.training.n_gpu)))\n","                // self.config.training.gradient_accumulation_steps\n","                * float(self.config.training.num_train_epochs)\n","            )\n","        elif stage == 'test':\n","            val_dataset = self.get_dataset(tokenizer=self.tokenizer, \n","                                           type_path=\"test.json\", args=self.config, mode=\"Test\")\n","            self.test_dataset = test_dataset\n","\n","    def train_dataloader(self):\n","        \"\"\"訓練データローダーを作成する\"\"\"\n","        return DataLoader(self.train_dataset, \n","                          batch_size=self.config.training.train_batch_size, \n","                          drop_last=True, shuffle=True, num_workers=4)\n","\n","    def val_dataloader(self):\n","        \"\"\"バリデーションデータローダーを作成する\"\"\"\n","        return DataLoader(self.val_dataset, \n","                          batch_size=self.config.training.eval_batch_size, \n","                          num_workers=4)\n","        \n","    def test_dataloader(self):\n","        \"\"\"バリデーションデータローダーを作成する\"\"\"\n","        return DataLoader(self.val_dataset, \n","                          batch_size=self.config.training.test_batch_size, \n","                          num_workers=4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n-EpS4gN5duK"},"source":["## Evaluation"]},{"cell_type":"code","metadata":{"id":"T_6ZVtWOHV2R"},"source":["from AdjacencyAttentionWithoutSelfloopTransformers import (\n","  AdamW,\n","  T5ForConditionalGeneration,\n","  T5Tokenizer,\n","  get_linear_schedule_with_warmup\n",")\n","from torch.utils.data import DataLoader\n","\n","from tqdm.auto import tqdm\n","\n","import pandas as pd\n","from core.evaluation.eval import QGEvalCap\n","\n","from omegaconf import DictConfig\n","\n","\n","# OmegaConf.register_new_resolver(\"now\", lambda pattern: strftime(pattern, localtime()))\n","\n","def saveOutputs(hparams: DictConfig, inputs: list, outputs: list, targets: list) -> None:\n","  data = pd.DataFrame(list(zip(inputs, outputs, targets)), columns =['inputs', 'outputs', 'targets'])\n","  data.to_csv(os.path.join(\"out\",hparams.experiment.model_dir.split(\"/\")[-1]+\".csv\"),index=False, header=True)\n","\n","def run_eval(target_src, decoded_text) -> dict:\n","  assert len(target_src) == len(decoded_text)\n","  eval_targets = {}\n","  eval_predictions = {}\n","  for idx in range(len(target_src)):\n","      eval_targets[idx] = [target_src[idx]]\n","      eval_predictions[idx] = [decoded_text[idx]]\n","\n","  QGEval = QGEvalCap(eval_targets, eval_predictions)\n","  scores = QGEval.evaluate()\n","  return scores\n","\n","class T5Evaluation(pl.LightningModule):\n","    def __init__(self, config: DictConfig, train_params: DictConfig):\n","        super().__init__()\n","        self.config = config\n","        self.train_params = train_params\n","        # 事前学習済みモデルの読み込み\n","        self.model = T5ForConditionalGeneration.from_pretrained(self.config.experiment.model_dir)\n","\n","        # トークナイザーの読み込み\n","        self.tokenizer = T5Tokenizer.from_pretrained(self.config.experiment.model_dir, is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        self.save_hyperparameters()\n","\n","        self.inputs = []\n","        self.outputs = []\n","        self.targets = []\n","\n","\n","    def forward(self, input_ids, attention_mask=None, cross_attention_mask=None, decoder_input_ids=None, \n","                decoder_attention_mask=None, labels=None):\n","        \"\"\"順伝搬\"\"\"\n","        return self.model.generate(input_ids=input_ids, \n","            attention_mask=attention_mask, \n","            cross_attention_mask=cross_attention_mask,\n","            max_length=self.config.model.max_target_length,\n","            temperature=1.0,          # 生成にランダム性を入れる温度パラメータ\n","            repetition_penalty=1.5,   # 同じ文の繰り返し（モード崩壊）へのペナルティ\n","            three_dim_attention_mask=True\n","            )\n","\n","    def _step(self, batch):\n","        \"\"\"ロス計算\"\"\"\n","        labels = batch[\"target_ids\"]\n","\n","        # All labels set to -100 are ignored (masked), \n","        # the loss is only computed for labels in [0, ..., config.vocab_size]\n","        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","        outputs = self(\n","            input_ids=batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            cross_attention_mask=batch[\"cross_attention_mask\"],\n","            decoder_attention_mask=batch['target_mask'],\n","            labels=labels\n","        )\n","\n","        return outputs\n","\n","    def test_step(self, batch, batch_idx):\n","        \"\"\"テストステップ処理\"\"\"\n","        output = self._step(batch)\n","        labels = batch[\"target_ids\"]\n","        labels[labels[:, :] == -100] = 0\n","        output_text = [self.tokenizer.decode(ids, skip_special_tokens=True, \n","                                clean_up_tokenization_spaces=False) \n","                    for ids in output]\n","        target_text = [self.tokenizer.decode(ids, skip_special_tokens=True, \n","                                clean_up_tokenization_spaces=False) \n","                    for ids in labels]\n","        input_text = [self.tokenizer.decode(ids, skip_special_tokens=False, \n","                                clean_up_tokenization_spaces=False) \n","                    for ids in batch[\"source_ids\"]]\n","\n","        self.inputs.extend(input_text)\n","        self.outputs.extend(output_text)\n","        self.targets.extend(target_text)\n","\n","        return {\"batch_idx\":batch_idx}\n","        \n","    \n","    def test_epoch_end(self, training_step_outputs):\n","        saveOutputs(self.config, self.inputs, self.outputs, self.targets)\n","        scores = run_eval(self.targets, self.outputs)\n","        wandb.log(scores)\n","\n","    def configure_optimizers(self):\n","        \"\"\"オプティマイザーとスケジューラーを作成する\"\"\"\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() \n","                            if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.config.training.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() \n","                            if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, \n","                          lr=self.config.training.learning_rate, \n","                          eps=self.config.training.adam_epsilon)\n","        self.optimizer = optimizer\n","\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer, num_warmup_steps=self.config.training.warmup_steps, \n","            num_training_steps=self.t_total\n","        )\n","        self.scheduler = scheduler\n","\n","        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}]\n","\n","    def get_dataset(self, tokenizer, type_path, mode=None):\n","        \"\"\"データセットを作成する\"\"\"\n","        if self.config.get('experiment').data==\"mhqg-wq\":\n","          return JsonDatasetWQ(\n","            tokenizer=tokenizer, \n","            data_dir=self.config.experiment.data_dir, \n","            type_path=type_path, \n","            input_max_len=self.config.model.max_input_length,\n","            target_max_len=self.config.model.max_target_length,\n","            mode=mode)\n","        else:\n","          return JsonDatasetPQ(\n","            tokenizer=tokenizer, \n","            data_dir=self.config.experiment.data_dir, \n","            type_path=type_path, \n","            input_max_len=self.config.model.max_input_length,\n","            target_max_len=self.config.model.max_target_length,\n","            mode=mode)\n","    \n","    def setup(self, stage=None):\n","        \"\"\"初期設定（データセットの読み込み）\"\"\"\n","        if stage == 'test':\n","            test_dataset = self.get_dataset(tokenizer=self.tokenizer, \n","                                           type_path=\"test.json\", mode=\"Test\")\n","            self.test_dataset = test_dataset\n","        \n","    def test_dataloader(self):\n","        \"\"\"バリデーションデータローダーを作成する\"\"\"\n","        return DataLoader(self.test_dataset, \n","                          batch_size=self.config.training.test_batch_size, \n","                          num_workers=4)\n","\n","class Evaluation():\n","  def __init__(self, config, train_params):\n","    self.config = config\n","    self.train_params = train_params\n","\n","  def run(self):\n","    # Tokenizer\n","    tokenizer = T5Tokenizer.from_pretrained(self.config.experiment.model_dir, is_fast=True)\n","    trained_model = T5ForConditionalGeneration.from_pretrained(self.config.experiment.model_dir)\n","\n","    if self.config.experiment.data==\"mhqg-wq\":\n","        # import test data\n","        test_dataset = JsonDatasetWQ(tokenizer, self.config.experiment.data_dir, \"test.json\", \n","                                input_max_len=self.config.model.max_input_length, \n","                                target_max_len=self.config.model.max_target_length,\n","                                mode=\"Test\")\n","\n","    test_loader = DataLoader(test_dataset, batch_size=8, num_workers=4)\n","\n","    trained_model.eval()\n","\n","    inputs = []\n","    outputs = []\n","    targets = []\n","\n","    for index, batch in enumerate(tqdm(test_loader)):\n","        input_ids = batch['source_ids']\n","        input_mask = batch['source_mask']\n","        input_cross_attention_mask = batch[\"cross_attention_mask\"]\n","        if self.config.training.n_gpu:\n","            input_ids = input_ids.cuda()\n","            input_mask = input_mask.cuda()\n","\n","        output = trained_model.generate(input_ids=input_ids, \n","            attention_mask=input_mask, \n","            cross_attention_mask=input_cross_attention_mask,\n","            max_length=self.config.model.max_target_length,\n","            temperature=1.0,          # 生成にランダム性を入れる温度パラメータ\n","            repetition_penalty=1.5,   # 同じ文の繰り返し（モード崩壊）へのペナルティ\n","            three_dim_attention_mask=True\n","            )\n","\n","        output_text = [tokenizer.decode(ids, skip_special_tokens=True, \n","                                clean_up_tokenization_spaces=False) \n","                    for ids in output]\n","        target_text = [tokenizer.decode(ids, skip_special_tokens=True, \n","                                clean_up_tokenization_spaces=False) \n","                    for ids in batch[\"target_ids\"]]\n","        input_text = [tokenizer.decode(ids, skip_special_tokens=False, \n","                                clean_up_tokenization_spaces=False) \n","                    for ids in input_ids]\n","\n","        inputs.extend(input_text)\n","        outputs.extend(output_text)\n","        targets.extend(target_text)\n","\n","        saveOutputs(self.config, inputs, outputs, targets)\n","        scores = run_eval(targets, outputs)\n","        self.train_params[\"logger\"].log_metrics(scores)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"q4W3UXEe4Dfh"},"source":["## Main"]},{"cell_type":"code","metadata":{"id":"5edHAbL84Gs5"},"source":["import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","from itertools import chain\n","from string import punctuation\n"," \n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","from collections import Counter, defaultdict, OrderedDict\n","from nltk.tokenize import wordpunct_tokenize, word_tokenize\n"," \n","import gc\n","\n","from omegaconf import DictConfig\n","import omegaconf\n","\n","import hydra\n","import pytorch_lightning as pl\n","\n","\n","\n","import textwrap\n","from tqdm.auto import tqdm\n","from sklearn import metrics\n","\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning.callbacks import EarlyStopping\n","\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","def get_train_param(config: DictConfig) -> dict:\n","    wandb_logger = WandbLogger(\n","        name=(\"exp_\" + str(config.experiment.wandb.exp_num)),\n","        project=config.experiment.wandb.project,\n","        log_model=True,\n","    )\n","    checkpoint_path = os.path.join(\n","        config.experiment.model_dir, config.experiment.wandb.checkpoint_path\n","    )\n","    wandb_logger.log_hyperparams(config)\n","    early_stopping = EarlyStopping('valid/loss_epoch')\n","    train_params = dict(\n","        callbacks=[early_stopping],\n","        logger = wandb_logger,\n","        accumulate_grad_batches=config.training.gradient_accumulation_steps,\n","        gpus=config.training.n_gpu,\n","        max_epochs=config.training.num_train_epochs,\n","        precision= 16 if config.training.fp_16 else 32,\n","        amp_level=config.training.opt_level,\n","        gradient_clip_val=config.training.max_grad_norm,\n","    )\n","    return train_params\n","\n","# @hydra.main(config_path=\"config.yaml\")\n","def train(config: DictConfig, train_params: dict) -> None:\n","    \n","    # conduct transfer learning\n","    model = T5FineTuner(config)\n","    wandb.watch(model, log_freq=100)\n","    trainer = pl.Trainer(**train_params)\n","    trainer.fit(model)\n","\n","    model.tokenizer.save_pretrained(config.experiment.model_dir)\n","    model.model.save_pretrained(config.experiment.model_dir)\n","\n","    # trainer.test(model)\n","\n","    # torch.save(\n","    #             {\"model\": model.state_dict(), \"preds\": preds}, OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\"\n","    #         )\n","\n","# @hydra.main(config_path=\"adjacencyattentionwithoutselfloop\", config_name=\"config\")\n","def evaluation(config: DictConfig, train_params: dict) -> None:\n","    print(\"config\",config)\n","    print(\"type_config\",type(config))\n","    set_seed(42)\n","    USE_GPU = torch.cuda.is_available()\n","    config.training.n_gpu=1 if USE_GPU else 0\n","    eval_model = T5Evaluation(config, train_params)\n","    trainer = pl.Trainer(**train_params)\n","    trainer.test(eval_model)\n","  \n","def main(config: DictConfig) -> None:\n","    if os.path.exists(config.experiment.model_dir):\n","        raise \"model_dir is exist\"\n","\n","    set_seed(42)\n","    USE_GPU = torch.cuda.is_available()\n","    config.training.n_gpu=1 if USE_GPU else 0\n","    train_params = get_train_param(config)\n","\n","    Preprocess(config)\n","    # train(config, train_params)\n","    evaluation(config, train_params)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jEnv3YSy4ThR","colab":{"base_uri":"https://localhost:8080/","height":317},"executionInfo":{"status":"error","timestamp":1630821627760,"user_tz":-540,"elapsed":793,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"9f8bc642-3793-4cd4-88bd-26768d0c8901"},"source":["config = omegaconf.OmegaConf.load(\"config/config_test_pq.yaml\")\n","main(config)\n","wandb.finish()"],"execution_count":null,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-11-3b64b7f12d20>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0momegaconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config/config_test_pq.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-1fb92319cebf>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDictConfig\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0;34m\"model_dir is exist\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m     \u001b[0mset_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m42\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: exceptions must derive from BaseException"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":205,"referenced_widgets":["fdc6e8fc857c497fb5b725949bcdcd46","98a24c5ac37d4387b96247a64c1372f3","cdb19e97b507427aae9a4881742d9d6d","6941b7237e04438f9395b2490c90a10a","652a36bec25f49dd8d1f5cbc57402837","da3ec7022ca249329825f33a7522e457","2d8afa9fdf5d430d9fef595471061c6a","3aab20b2a78942ff97543defbb3f6293","6506f053e5054cb18a529ed49b392158","c43b6a693e0b4af8b4eff1d77ed4c231","162014b729d54de2adb141753a041e61"]},"id":"TrcTVswyrr8j","executionInfo":{"status":"ok","timestamp":1630821984289,"user_tz":-540,"elapsed":141640,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"f0049bdd-8e8b-4c72-8190-9c339b7c22fe"},"source":["config = omegaconf.OmegaConf.load(\"config/config_test_pq.yaml\")\n","train_params = get_train_param(config)\n","evaluation(config, train_params)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["config {'experiment': {'seed': 42, 'data': 'mhqg-pq', 'data_dir': 'data/mhqg-pq/', 'model_name_or_path': 't5-small', 'tokenizer_name_or_path': 't5-small', 'model_dir': 'content/test_mhqg-pq_t5-small_20210905_1310', 'wandb': {'exp_num': 5, 'project': 'test_mhqg-pq_t5-small', 'checkpoint_path': 'checkpoint/'}}, 'model': {'max_input_length': 100, 'max_target_length': 100}, 'training': {'learning_rate': 0.0003, 'weight_decay': 0.0, 'adam_epsilon': 1e-08, 'warmup_steps': 0, 'gradient_accumulation_steps': 1, 'early_stop_callback': True, 'fp_16': False, 'opt_level': 'O1', 'max_grad_norm': 1.0, 'seed': 42, 'train_batch_size': 32, 'eval_batch_size': 32, 'test_batch_size': 8, 'num_train_epochs': 10, 'precision': 16, 'n_gpu': 1}}\n","type_config <class 'omegaconf.dictconfig.DictConfig'>\n"]},{"output_type":"stream","name":"stderr","text":["GPU available: True, used: True\n","TPU available: None, using: 0 TPU cores\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"fdc6e8fc857c497fb5b725949bcdcd46","version_minor":0,"version_major":2},"text/plain":["Testing: 0it [00:00, ?it/s]"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["--------------------------------------------------------------------------------\n","DATALOADER:0 TEST RESULTS\n","{}\n","--------------------------------------------------------------------------------\n"]}]},{"cell_type":"code","metadata":{"id":"Um1KqH54jC-V"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"KVO_b7iTmoqa"},"source":["# 20210918"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"iGYu6igVo5wu","executionInfo":{"status":"ok","timestamp":1631936375117,"user_tz":-540,"elapsed":388,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"092a5a1c-3df3-44e9-c02d-43401e88f037"},"source":["cd /content/drive/MyDrive/ColabNotebooks/Research/KG2QGwithT5/TripleAttention"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/ColabNotebooks/Research/KG2QGwithT5/TripleAttention\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LWpo-1M1sCim","executionInfo":{"status":"ok","timestamp":1631931355516,"user_tz":-540,"elapsed":9054,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"c7e8628d-d8dc-48c2-e187-bf7455807e95"},"source":["!pip install -q --pre poetry==1.1.8"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 173 kB 31.6 MB/s \n","\u001b[K     |████████████████████████████████| 5.3 MB 33.9 MB/s \n","\u001b[K     |████████████████████████████████| 91 kB 9.0 MB/s \n","\u001b[K     |████████████████████████████████| 420 kB 68.3 MB/s \n","\u001b[K     |████████████████████████████████| 40 kB 5.5 MB/s \n","\u001b[K     |████████████████████████████████| 54 kB 2.4 MB/s \n","\u001b[K     |████████████████████████████████| 54 kB 2.6 MB/s \n","\u001b[K     |████████████████████████████████| 3.0 MB 66.8 MB/s \n","\u001b[K     |████████████████████████████████| 338 kB 77.2 MB/s \n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n","\u001b[?25h"]}]},{"cell_type":"code","metadata":{"id":"oRqK7zOEr91F"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JuL4J4j8tfhW"},"source":["## Install dependency\n","~~~\n","Based on the pyproject.toml in the current path, dependency will be installed in order.\n","~~~"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"5m56aXqutfhY","executionInfo":{"status":"ok","timestamp":1631931895461,"user_tz":-540,"elapsed":153811,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"0a536682-7881-43da-8329-194632e4770f"},"source":["!pip install --requirement <(poetry export --format requirements.txt)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Ignoring colorama: markers 'python_version >= \"3.6\" and python_full_version < \"3.0.0\" and platform_system == \"Windows\" or python_full_version >= \"3.5.0\" and python_version >= \"3.6\" and platform_system == \"Windows\"' don't match your environment\n","Ignoring pywin32: markers 'sys_platform == \"win32\" and python_version >= \"3.6\"' don't match your environment\n","Ignoring waitress: markers 'platform_system == \"Windows\" and python_version >= \"3.6\" and python_full_version >= \"3.6.0\"' don't match your environment\n","Collecting absl-py==0.13.0\n","  Downloading absl_py-0.13.0-py3-none-any.whl (132 kB)\n","\u001b[K     |████████████████████████████████| 132 kB 30.6 MB/s \n","\u001b[?25hCollecting aiohttp==3.7.4.post0\n","  Downloading aiohttp-3.7.4.post0-cp37-cp37m-manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[K     |████████████████████████████████| 1.3 MB 42.8 MB/s \n","\u001b[?25hCollecting alembic==1.4.1\n","  Downloading alembic-1.4.1.tar.gz (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 46.1 MB/s \n","\u001b[?25hCollecting antlr4-python3-runtime==4.8\n","  Downloading antlr4-python3-runtime-4.8.tar.gz (112 kB)\n","\u001b[K     |████████████████████████████████| 112 kB 58.4 MB/s \n","\u001b[?25hCollecting async-timeout==3.0.1\n","  Downloading async_timeout-3.0.1-py3-none-any.whl (8.2 kB)\n","Requirement already satisfied: attrs==21.2.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 49)) (21.2.0)\n","Requirement already satisfied: cachetools==4.2.2 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 52)) (4.2.2)\n","Requirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 55)) (2021.5.30)\n","Collecting chardet==4.0.0\n","  Downloading chardet-4.0.0-py2.py3-none-any.whl (178 kB)\n","\u001b[K     |████████████████████████████████| 178 kB 58.4 MB/s \n","\u001b[?25hCollecting charset-normalizer==2.0.6\n","  Downloading charset_normalizer-2.0.6-py3-none-any.whl (37 kB)\n","Collecting click==8.0.1\n","  Downloading click-8.0.1-py3-none-any.whl (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 7.5 MB/s \n","\u001b[?25hCollecting cloudpickle==2.0.0\n","  Downloading cloudpickle-2.0.0-py3-none-any.whl (25 kB)\n","Collecting configparser==5.0.2\n","  Downloading configparser-5.0.2-py3-none-any.whl (19 kB)\n","Collecting databricks-cli==0.15.0\n","  Downloading databricks-cli-0.15.0.tar.gz (56 kB)\n","\u001b[K     |████████████████████████████████| 56 kB 4.4 MB/s \n","\u001b[?25hCollecting docker-pycreds==0.4.0\n","  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n","Collecting docker==5.0.2\n","  Downloading docker-5.0.2-py2.py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 59.4 MB/s \n","\u001b[?25hRequirement already satisfied: entrypoints==0.3 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 85)) (0.3)\n","Requirement already satisfied: filelock==3.0.12 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 88)) (3.0.12)\n","Collecting flask==2.0.1\n","  Downloading Flask-2.0.1-py3-none-any.whl (94 kB)\n","\u001b[K     |████████████████████████████████| 94 kB 3.8 MB/s \n","\u001b[?25hCollecting fsspec==2021.8.1\n","  Downloading fsspec-2021.8.1-py3-none-any.whl (119 kB)\n","\u001b[K     |████████████████████████████████| 119 kB 59.5 MB/s \n","\u001b[?25hCollecting future==0.18.2\n","  Downloading future-0.18.2.tar.gz (829 kB)\n","\u001b[K     |████████████████████████████████| 829 kB 50.4 MB/s \n","\u001b[?25hCollecting gitdb==4.0.7\n","  Downloading gitdb-4.0.7-py3-none-any.whl (63 kB)\n","\u001b[K     |████████████████████████████████| 63 kB 1.7 MB/s \n","\u001b[?25hCollecting gitpython==3.1.24\n","  Downloading GitPython-3.1.24-py3-none-any.whl (180 kB)\n","\u001b[K     |████████████████████████████████| 180 kB 53.2 MB/s \n","\u001b[?25hRequirement already satisfied: google-auth-oauthlib==0.4.6 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 105)) (0.4.6)\n","Requirement already satisfied: google-auth==1.35.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 108)) (1.35.0)\n","Requirement already satisfied: greenlet==1.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 111)) (1.1.1)\n","Requirement already satisfied: grpcio==1.40.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 162)) (1.40.0)\n","Collecting gunicorn==20.1.0\n","  Downloading gunicorn-20.1.0-py3-none-any.whl (79 kB)\n","\u001b[K     |████████████████████████████████| 79 kB 8.5 MB/s \n","\u001b[?25hCollecting hydra-core==1.1.1\n","  Downloading hydra_core-1.1.1-py3-none-any.whl (145 kB)\n","\u001b[K     |████████████████████████████████| 145 kB 59.0 MB/s \n","\u001b[?25hCollecting hydra==2.5\n","  Downloading Hydra-2.5.tar.gz (82 kB)\n","\u001b[K     |████████████████████████████████| 82 kB 679 kB/s \n","\u001b[?25hCollecting idna==3.2\n","  Downloading idna-3.2-py3-none-any.whl (59 kB)\n","\u001b[K     |████████████████████████████████| 59 kB 6.7 MB/s \n","\u001b[?25hCollecting importlib-metadata==4.8.1\n","  Using cached importlib_metadata-4.8.1-py3-none-any.whl (17 kB)\n","Requirement already satisfied: importlib-resources==5.2.2 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 221)) (5.2.2)\n","Collecting itsdangerous==2.0.1\n","  Downloading itsdangerous-2.0.1-py3-none-any.whl (18 kB)\n","Collecting jinja2==3.0.1\n","  Downloading Jinja2-3.0.1-py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 62.4 MB/s \n","\u001b[?25hRequirement already satisfied: joblib==1.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 230)) (1.0.1)\n","Collecting mako==1.1.5\n","  Downloading Mako-1.1.5-py2.py3-none-any.whl (75 kB)\n","\u001b[K     |████████████████████████████████| 75 kB 4.3 MB/s \n","\u001b[?25hRequirement already satisfied: markdown==3.3.4 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 236)) (3.3.4)\n","Requirement already satisfied: markupsafe==2.0.1 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 239)) (2.0.1)\n","Collecting mlflow==1.20.2\n","  Downloading mlflow-1.20.2-py3-none-any.whl (14.6 MB)\n","\u001b[K     |████████████████████████████████| 14.6 MB 257 kB/s \n","\u001b[?25hCollecting multidict==5.1.0\n","  Downloading multidict-5.1.0-cp37-cp37m-manylinux2014_x86_64.whl (142 kB)\n","\u001b[K     |████████████████████████████████| 142 kB 65.7 MB/s \n","\u001b[?25hCollecting nltk==3.6\n","  Downloading nltk-3.6-py3-none-any.whl (1.5 MB)\n","\u001b[K     |████████████████████████████████| 1.5 MB 74.6 MB/s \n","\u001b[?25hCollecting numpy==1.21.1\n","  Downloading numpy-1.21.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n","\u001b[K     |████████████████████████████████| 15.7 MB 176 kB/s \n","\u001b[?25hRequirement already satisfied: oauthlib==3.1.1 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 367)) (3.1.1)\n","Collecting omegaconf==2.1.1\n","  Downloading omegaconf-2.1.1-py3-none-any.whl (74 kB)\n","\u001b[K     |████████████████████████████████| 74 kB 3.2 MB/s \n","\u001b[?25hCollecting packaging==21.0\n","  Downloading packaging-21.0-py3-none-any.whl (40 kB)\n","\u001b[K     |████████████████████████████████| 40 kB 5.1 MB/s \n","\u001b[?25hRequirement already satisfied: pandas==1.1.5 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 376)) (1.1.5)\n","Collecting pillow==8.3.2\n","  Downloading Pillow-8.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n","\u001b[K     |████████████████████████████████| 3.0 MB 64.3 MB/s \n","\u001b[?25hRequirement already satisfied: prometheus-client==0.11.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 455)) (0.11.0)\n","Collecting prometheus-flask-exporter==0.18.2\n","  Downloading prometheus_flask_exporter-0.18.2.tar.gz (22 kB)\n","Requirement already satisfied: promise==2.3 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 460)) (2.3)\n","Collecting protobuf==3.18.0\n","  Downloading protobuf-3.18.0-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n","\u001b[K     |████████████████████████████████| 1.1 MB 56.0 MB/s \n","\u001b[?25hCollecting psutil==5.8.0\n","  Downloading psutil-5.8.0-cp37-cp37m-manylinux2010_x86_64.whl (296 kB)\n","\u001b[K     |████████████████████████████████| 296 kB 60.1 MB/s \n","\u001b[?25hRequirement already satisfied: pyasn1-modules==0.2.8 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 513)) (0.2.8)\n","Requirement already satisfied: pyasn1==0.4.8 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 527)) (0.4.8)\n","Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 541)) (2.4.7)\n","Requirement already satisfied: python-dateutil==2.8.2 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 544)) (2.8.2)\n","Collecting python-editor==1.0.4\n","  Downloading python_editor-1.0.4-py3-none-any.whl (4.9 kB)\n","Collecting pytorch-lightning==1.2.1\n","  Downloading pytorch_lightning-1.2.1-py3-none-any.whl (814 kB)\n","\u001b[K     |████████████████████████████████| 814 kB 50.9 MB/s \n","\u001b[?25hCollecting pytz==2021.1\n","  Downloading pytz-2021.1-py2.py3-none-any.whl (510 kB)\n","\u001b[K     |████████████████████████████████| 510 kB 62.7 MB/s \n","\u001b[?25hCollecting pyyaml==5.3.1\n","  Downloading PyYAML-5.3.1.tar.gz (269 kB)\n","\u001b[K     |████████████████████████████████| 269 kB 64.4 MB/s \n","\u001b[?25hCollecting querystring-parser==1.2.4\n","  Downloading querystring_parser-1.2.4-py2.py3-none-any.whl (7.9 kB)\n","Collecting regex==2021.8.28\n","  Downloading regex-2021.8.28-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (745 kB)\n","\u001b[K     |████████████████████████████████| 745 kB 74.7 MB/s \n","\u001b[?25hRequirement already satisfied: requests-oauthlib==1.3.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 631)) (1.3.0)\n","Collecting requests==2.26.0\n","  Downloading requests-2.26.0-py2.py3-none-any.whl (62 kB)\n","\u001b[K     |████████████████████████████████| 62 kB 841 kB/s \n","\u001b[?25hRequirement already satisfied: rsa==4.7.2 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 638)) (4.7.2)\n","Collecting sacremoses==0.0.45\n","  Downloading sacremoses-0.0.45-py3-none-any.whl (895 kB)\n","\u001b[K     |████████████████████████████████| 895 kB 63.2 MB/s \n","\u001b[?25hCollecting sentencepiece==0.1.96\n","  Downloading sentencepiece-0.1.96-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[K     |████████████████████████████████| 1.2 MB 44.5 MB/s \n","\u001b[?25hCollecting sentry-sdk==1.3.1\n","  Downloading sentry_sdk-1.3.1-py2.py3-none-any.whl (133 kB)\n","\u001b[K     |████████████████████████████████| 133 kB 59.5 MB/s \n","\u001b[?25hCollecting shortuuid==1.0.1\n","  Downloading shortuuid-1.0.1-py3-none-any.whl (7.5 kB)\n","Collecting six==1.16.0\n","  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n","Collecting smmap==4.0.0\n","  Downloading smmap-4.0.0-py2.py3-none-any.whl (24 kB)\n","Requirement already satisfied: sqlalchemy==1.4.23 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 699)) (1.4.23)\n","Requirement already satisfied: sqlparse==0.4.2 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 730)) (0.4.2)\n","Collecting subprocess32==3.5.4\n","  Downloading subprocess32-3.5.4.tar.gz (97 kB)\n","\u001b[K     |████████████████████████████████| 97 kB 7.3 MB/s \n","\u001b[?25hRequirement already satisfied: tabulate==0.8.9 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 737)) (0.8.9)\n","Requirement already satisfied: tensorboard-data-server==0.6.1 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 740)) (0.6.1)\n","Requirement already satisfied: tensorboard-plugin-wit==1.8.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 744)) (1.8.0)\n","Requirement already satisfied: tensorboard==2.6.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 746)) (2.6.0)\n","Collecting tokenizers==0.10.3\n","  Downloading tokenizers-0.10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (3.3 MB)\n","\u001b[K     |████████████████████████████████| 3.3 MB 48.2 MB/s \n","\u001b[?25hCollecting torch==1.7.1\n","  Downloading torch-1.7.1-cp37-cp37m-manylinux1_x86_64.whl (776.8 MB)\n","\u001b[K     |████████████████████████████████| 776.8 MB 17 kB/s \n","\u001b[?25hCollecting torchtext==0.8.0\n","  Downloading torchtext-0.8.0-cp37-cp37m-manylinux1_x86_64.whl (6.9 MB)\n","\u001b[K     |████████████████████████████████| 6.9 MB 15.8 MB/s \n","\u001b[?25hCollecting torchvision==0.8.2\n","  Downloading torchvision-0.8.2-cp37-cp37m-manylinux1_x86_64.whl (12.8 MB)\n","\u001b[K     |████████████████████████████████| 12.8 MB 67.4 MB/s \n","\u001b[?25hRequirement already satisfied: tqdm==4.62.2 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 795)) (4.62.2)\n","Collecting transformers==4.4.2\n","  Downloading transformers-4.4.2-py3-none-any.whl (2.0 MB)\n","\u001b[K     |████████████████████████████████| 2.0 MB 27.4 MB/s \n","\u001b[?25hCollecting typing-extensions==3.10.0.2\n","  Using cached typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n","Collecting urllib3==1.26.6\n","  Downloading urllib3-1.26.6-py2.py3-none-any.whl (138 kB)\n","\u001b[K     |████████████████████████████████| 138 kB 41.3 MB/s \n","\u001b[?25hCollecting wandb==0.10.7\n","  Downloading wandb-0.10.7-py2.py3-none-any.whl (1.7 MB)\n","\u001b[K     |████████████████████████████████| 1.7 MB 50.3 MB/s \n","\u001b[?25hCollecting watchdog==2.1.5\n","  Downloading watchdog-2.1.5-py3-none-manylinux2014_x86_64.whl (75 kB)\n","\u001b[K     |████████████████████████████████| 75 kB 2.8 MB/s \n","\u001b[?25hCollecting websocket-client==1.2.1\n","  Downloading websocket_client-1.2.1-py2.py3-none-any.whl (52 kB)\n","\u001b[K     |████████████████████████████████| 52 kB 1.4 MB/s \n","\u001b[?25hCollecting werkzeug==2.0.1\n","  Downloading Werkzeug-2.0.1-py3-none-any.whl (288 kB)\n","\u001b[K     |████████████████████████████████| 288 kB 50.6 MB/s \n","\u001b[?25hCollecting yarl==1.6.3\n","  Downloading yarl-1.6.3-cp37-cp37m-manylinux2014_x86_64.whl (294 kB)\n","\u001b[K     |████████████████████████████████| 294 kB 51.6 MB/s \n","\u001b[?25hRequirement already satisfied: zipp==3.5.0 in /usr/local/lib/python3.7/dist-packages (from -r /dev/fd/63 (line 882)) (3.5.0)\n","Requirement already satisfied: setuptools>=40.3.0 in /usr/local/lib/python3.7/dist-packages (from google-auth==1.35.0->-r /dev/fd/63 (line 108)) (57.4.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard==2.6.0->-r /dev/fd/63 (line 746)) (0.37.0)\n","Building wheels for collected packages: alembic, antlr4-python3-runtime, databricks-cli, future, hydra, prometheus-flask-exporter, pyyaml, subprocess32\n","  Building wheel for alembic (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for alembic: filename=alembic-1.4.1-py2.py3-none-any.whl size=158172 sha256=e9b2b73f1a63c06da435e086c5f3634bca9898e25d1a9b618846cac0e1a88f41\n","  Stored in directory: /root/.cache/pip/wheels/be/5d/0a/9e13f53f4f5dfb67cd8d245bb7cdffe12f135846f491a283e3\n","  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.8-py3-none-any.whl size=141230 sha256=8fc08b088c65a09307bfa8ba3d141429312dc5b4ebca9cd40fc16358e4295752\n","  Stored in directory: /root/.cache/pip/wheels/ca/33/b7/336836125fc9bb4ceaa4376d8abca10ca8bc84ddc824baea6c\n","  Building wheel for databricks-cli (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for databricks-cli: filename=databricks_cli-0.15.0-py3-none-any.whl size=105260 sha256=a0f75a20597730020aeca296d85d817c79750d02ebd5f65034f42d94abdefb03\n","  Stored in directory: /root/.cache/pip/wheels/e7/ba/75/284f9a90ff7a010bb23b9798f2e9a19dd9fe619379c917bff4\n","  Building wheel for future (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=700344a549c24ca15eeb1144d46ee1f0b6e40828c95eb0ef9a556a3b74709564\n","  Stored in directory: /root/.cache/pip/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n","  Building wheel for hydra (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for hydra: filename=Hydra-2.5-cp37-cp37m-linux_x86_64.whl size=220769 sha256=fd0d751a7444658be677b1cb532826682fe1b294ffa7327e9443244b4eeed333\n","  Stored in directory: /root/.cache/pip/wheels/46/28/7d/3b38a41d900da90c4e17576f442bac9344eb1f5a4e78ee9f83\n","  Building wheel for prometheus-flask-exporter (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for prometheus-flask-exporter: filename=prometheus_flask_exporter-0.18.2-py3-none-any.whl size=17415 sha256=bc654acf9ca6d99277994bd6f051dd1b66e2a8defca24d724538ac5041905db2\n","  Stored in directory: /root/.cache/pip/wheels/6a/1e/1c/c765920cb92b2f0343d2dd8b481a407cee2823f9b4bbd2e52a\n","  Building wheel for pyyaml (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyyaml: filename=PyYAML-5.3.1-cp37-cp37m-linux_x86_64.whl size=44635 sha256=d61e61d738e0f3f3912ebb8c86f682b3d11d0007047859e7aa8dc8634bfc8c7e\n","  Stored in directory: /root/.cache/pip/wheels/5e/03/1e/e1e954795d6f35dfc7b637fe2277bff021303bd9570ecea653\n","  Building wheel for subprocess32 (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for subprocess32: filename=subprocess32-3.5.4-py3-none-any.whl size=6502 sha256=82db1bafddb51c895a0844db04d06cb697c76939ede77d9bdb1483ebd2bd5dc5\n","  Stored in directory: /root/.cache/pip/wheels/50/ca/fa/8fca8d246e64f19488d07567547ddec8eb084e8c0d7a59226a\n","Successfully built alembic antlr4-python3-runtime databricks-cli future hydra prometheus-flask-exporter pyyaml subprocess32\n","Installing collected packages: urllib3, typing-extensions, idna, charset-normalizer, six, requests, multidict, importlib-metadata, yarl, werkzeug, smmap, jinja2, itsdangerous, click, chardet, async-timeout, websocket-client, regex, pyyaml, pytz, python-editor, protobuf, numpy, mako, gitdb, fsspec, flask, antlr4-python3-runtime, aiohttp, absl-py, watchdog, torch, tokenizers, subprocess32, shortuuid, sentry-sdk, sacremoses, querystring-parser, psutil, prometheus-flask-exporter, pillow, packaging, omegaconf, gunicorn, gitpython, future, docker-pycreds, docker, databricks-cli, configparser, cloudpickle, alembic, wandb, transformers, torchvision, torchtext, sentencepiece, pytorch-lightning, nltk, mlflow, hydra-core, hydra\n","  Attempting uninstall: urllib3\n","    Found existing installation: urllib3 1.24.3\n","    Uninstalling urllib3-1.24.3:\n","      Successfully uninstalled urllib3-1.24.3\n","  Attempting uninstall: typing-extensions\n","    Found existing installation: typing-extensions 3.7.4.3\n","    Uninstalling typing-extensions-3.7.4.3:\n","      Successfully uninstalled typing-extensions-3.7.4.3\n","  Attempting uninstall: idna\n","    Found existing installation: idna 2.10\n","    Uninstalling idna-2.10:\n","      Successfully uninstalled idna-2.10\n","  Attempting uninstall: charset-normalizer\n","    Found existing installation: charset-normalizer 2.0.5\n","    Uninstalling charset-normalizer-2.0.5:\n","      Successfully uninstalled charset-normalizer-2.0.5\n","  Attempting uninstall: six\n","    Found existing installation: six 1.15.0\n","    Uninstalling six-1.15.0:\n","      Successfully uninstalled six-1.15.0\n","  Attempting uninstall: requests\n","    Found existing installation: requests 2.23.0\n","    Uninstalling requests-2.23.0:\n","      Successfully uninstalled requests-2.23.0\n","  Attempting uninstall: importlib-metadata\n","    Found existing installation: importlib-metadata 1.7.0\n","    Uninstalling importlib-metadata-1.7.0:\n","      Successfully uninstalled importlib-metadata-1.7.0\n","  Attempting uninstall: werkzeug\n","    Found existing installation: Werkzeug 1.0.1\n","    Uninstalling Werkzeug-1.0.1:\n","      Successfully uninstalled Werkzeug-1.0.1\n","  Attempting uninstall: jinja2\n","    Found existing installation: Jinja2 2.11.3\n","    Uninstalling Jinja2-2.11.3:\n","      Successfully uninstalled Jinja2-2.11.3\n","  Attempting uninstall: itsdangerous\n","    Found existing installation: itsdangerous 1.1.0\n","    Uninstalling itsdangerous-1.1.0:\n","      Successfully uninstalled itsdangerous-1.1.0\n","  Attempting uninstall: click\n","    Found existing installation: click 7.1.2\n","    Uninstalling click-7.1.2:\n","      Successfully uninstalled click-7.1.2\n","  Attempting uninstall: chardet\n","    Found existing installation: chardet 3.0.4\n","    Uninstalling chardet-3.0.4:\n","      Successfully uninstalled chardet-3.0.4\n","  Attempting uninstall: regex\n","    Found existing installation: regex 2019.12.20\n","    Uninstalling regex-2019.12.20:\n","      Successfully uninstalled regex-2019.12.20\n","  Attempting uninstall: pyyaml\n","    Found existing installation: PyYAML 3.13\n","    Uninstalling PyYAML-3.13:\n","      Successfully uninstalled PyYAML-3.13\n","  Attempting uninstall: pytz\n","    Found existing installation: pytz 2018.9\n","    Uninstalling pytz-2018.9:\n","      Successfully uninstalled pytz-2018.9\n","  Attempting uninstall: protobuf\n","    Found existing installation: protobuf 3.17.3\n","    Uninstalling protobuf-3.17.3:\n","      Successfully uninstalled protobuf-3.17.3\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.19.5\n","    Uninstalling numpy-1.19.5:\n","      Successfully uninstalled numpy-1.19.5\n","  Attempting uninstall: flask\n","    Found existing installation: Flask 1.1.4\n","    Uninstalling Flask-1.1.4:\n","      Successfully uninstalled Flask-1.1.4\n","  Attempting uninstall: absl-py\n","    Found existing installation: absl-py 0.12.0\n","    Uninstalling absl-py-0.12.0:\n","      Successfully uninstalled absl-py-0.12.0\n","  Attempting uninstall: torch\n","    Found existing installation: torch 1.9.0+cu102\n","    Uninstalling torch-1.9.0+cu102:\n","      Successfully uninstalled torch-1.9.0+cu102\n","  Attempting uninstall: psutil\n","    Found existing installation: psutil 5.4.8\n","    Uninstalling psutil-5.4.8:\n","      Successfully uninstalled psutil-5.4.8\n","  Attempting uninstall: pillow\n","    Found existing installation: Pillow 7.1.2\n","    Uninstalling Pillow-7.1.2:\n","      Successfully uninstalled Pillow-7.1.2\n","  Attempting uninstall: packaging\n","    Found existing installation: packaging 20.9\n","    Uninstalling packaging-20.9:\n","      Successfully uninstalled packaging-20.9\n","  Attempting uninstall: future\n","    Found existing installation: future 0.16.0\n","    Uninstalling future-0.16.0:\n","      Successfully uninstalled future-0.16.0\n","  Attempting uninstall: cloudpickle\n","    Found existing installation: cloudpickle 1.3.0\n","    Uninstalling cloudpickle-1.3.0:\n","      Successfully uninstalled cloudpickle-1.3.0\n","  Attempting uninstall: torchvision\n","    Found existing installation: torchvision 0.10.0+cu102\n","    Uninstalling torchvision-0.10.0+cu102:\n","      Successfully uninstalled torchvision-0.10.0+cu102\n","  Attempting uninstall: torchtext\n","    Found existing installation: torchtext 0.10.0\n","    Uninstalling torchtext-0.10.0:\n","      Successfully uninstalled torchtext-0.10.0\n","  Attempting uninstall: nltk\n","    Found existing installation: nltk 3.2.5\n","    Uninstalling nltk-3.2.5:\n","      Successfully uninstalled nltk-3.2.5\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","tensorflow 2.6.0 requires numpy~=1.19.2, but you have numpy 1.21.1 which is incompatible.\n","tensorflow 2.6.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n","tensorflow 2.6.0 requires typing-extensions~=3.7.4, but you have typing-extensions 3.10.0.2 which is incompatible.\n","tensorflow-metadata 1.2.0 requires absl-py<0.13,>=0.9, but you have absl-py 0.13.0 which is incompatible.\n","poetry 1.1.8 requires importlib-metadata<2.0.0,>=1.6.0; python_version < \"3.8\", but you have importlib-metadata 4.8.1 which is incompatible.\n","poetry 1.1.8 requires packaging<21.0,>=20.4, but you have packaging 21.0 which is incompatible.\n","poetry-core 1.0.4 requires importlib-metadata<2.0.0,>=1.7.0; python_version >= \"2.7\" and python_version < \"2.8\" or python_version >= \"3.5\" and python_version < \"3.8\", but you have importlib-metadata 4.8.1 which is incompatible.\n","gym 0.17.3 requires cloudpickle<1.7.0,>=1.2.0, but you have cloudpickle 2.0.0 which is incompatible.\n","google-colab 1.0.0 requires requests~=2.23.0, but you have requests 2.26.0 which is incompatible.\n","google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed absl-py-0.13.0 aiohttp-3.7.4.post0 alembic-1.4.1 antlr4-python3-runtime-4.8 async-timeout-3.0.1 chardet-4.0.0 charset-normalizer-2.0.6 click-8.0.1 cloudpickle-2.0.0 configparser-5.0.2 databricks-cli-0.15.0 docker-5.0.2 docker-pycreds-0.4.0 flask-2.0.1 fsspec-2021.8.1 future-0.18.2 gitdb-4.0.7 gitpython-3.1.24 gunicorn-20.1.0 hydra-2.5 hydra-core-1.1.1 idna-3.2 importlib-metadata-4.8.1 itsdangerous-2.0.1 jinja2-3.0.1 mako-1.1.5 mlflow-1.20.2 multidict-5.1.0 nltk-3.6 numpy-1.21.1 omegaconf-2.1.1 packaging-21.0 pillow-8.3.2 prometheus-flask-exporter-0.18.2 protobuf-3.18.0 psutil-5.8.0 python-editor-1.0.4 pytorch-lightning-1.2.1 pytz-2021.1 pyyaml-5.3.1 querystring-parser-1.2.4 regex-2021.8.28 requests-2.26.0 sacremoses-0.0.45 sentencepiece-0.1.96 sentry-sdk-1.3.1 shortuuid-1.0.1 six-1.16.0 smmap-4.0.0 subprocess32-3.5.4 tokenizers-0.10.3 torch-1.7.1 torchtext-0.8.0 torchvision-0.8.2 transformers-4.4.2 typing-extensions-3.10.0.2 urllib3-1.26.6 wandb-0.10.7 watchdog-2.1.5 websocket-client-1.2.1 werkzeug-2.0.1 yarl-1.6.3\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["PIL","google","numpy","psutil","pydevd_plugins","pytz","six"]}}},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"vSmn4LI5tpy2"},"source":["## Import Library"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-1A842t1tpy2","executionInfo":{"status":"ok","timestamp":1631936389652,"user_tz":-540,"elapsed":10384,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"21671848-980e-40e7-8888-5a6e54c2fbae"},"source":["import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","import re\n","from itertools import chain\n","from string import punctuation\n"," \n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","import pytorch_lightning as pl\n","\n","import io\n","import string\n","from collections import Counter, defaultdict, OrderedDict\n","from nltk.tokenize import wordpunct_tokenize, word_tokenize\n"," \n","import gc\n","\n","from mytransformers.src.transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n",")\n","\n","import os.path\n","\n","import wandb\n","wandb.login()"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stderr","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmacho000\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"nld_TxiEttUD"},"source":["## Preprocess"]},{"cell_type":"code","metadata":{"id":"uPUYzplVttUD","executionInfo":{"status":"ok","timestamp":1631936535708,"user_tz":-540,"elapsed":2213,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":["def preprocess_wq(config):\n","    if not os.path.isfile(\"processed/mhqg-wq/SOURCE_ID_memmap.npy\"):\n","        SOURCE_ID_PATH = \"processed/mhqg-wq/SOURCE_ID_memmap.npy\"\n","        SOURCE_MASK_PATH = \"processed/mhqg-wq/SOURCE_MASK_memmap.npy\"\n","        TARGET_ID_PATH = \"processed/mhqg-wq/TARGET_ID_memmap.npy\"\n","        TARGET_MASK_PATH = \"processed/mhqg-wq/TARGET_MASK_memmap.npy\"\n","        # input_length = 18624\n","        input_length = 18989\n","        SOURCE_ID_memmap = np.memmap(\n","        filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,512) \n","        )\n","\n","        SOURCE_MASK_memmap = np.memmap(\n","        filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512,512)\n","        )\n","\n","        TARGET_ID_memmap = np.memmap(\n","        filename=TARGET_ID_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,100)\n","        )\n","\n","        TARGET_MASK_memmap = np.memmap(\n","        filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,100)\n","        )\n","\n","        del SOURCE_ID_memmap\n","        del SOURCE_MASK_memmap\n","        del TARGET_ID_memmap\n","        del TARGET_MASK_memmap\n","        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        def _make_record(answer, input, target):\n","            # 質問生成タスク用の入出力形式に変換する。\n","            input = f\"{answer}{input}\"\n","            target = f\"{target}\"\n","            return input, target\n","\n","        with open(\"data/mhqg-wq/train.json\", 'r') as f:\n","            list_index = 0\n","            for index, line in enumerate(f):\n","                line = line.strip()\n","                jo = json.loads(line, object_pairs_hook=OrderedDict)\n","                assert len(jo['inGraph']['g_adj']) > 0\n","\n","                answers = jo['answers']\n","                normalized_answers = \"\"\n","                for x in answers:\n","                    normalized_answers += \" <answer> \"\n","                    normalized_answers += x\n","                target = jo['outSeq']\n","\n","                graph = {'node_name_id2word':{}}\n","                for idx, nid in enumerate(jo['inGraph']['g_node_names']):\n","                    graph['node_name_id2word'][nid] = jo['inGraph']['g_node_names'][nid]\n","                input_knowledge_graph = \"\"\n","                for nid, val in jo['inGraph']['g_adj'].items():\n","                    for nid2, edge in val.items():\n","                        edge = edge.split('/')[-1]\n","                        input_knowledge_graph += \" <sep> \" + \" <subject> \"+ str(graph[\"node_name_id2word\"][nid])+\" <relation> \"+str(edge)+\" <object> \"+str(graph[\"node_name_id2word\"][nid2])\n","                \n","                input, target = _make_record(normalized_answers,input_knowledge_graph, target)\n"," \n","                tokenized_inputs = tokenizer.batch_encode_plus(\n","                    [input], max_length=512, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n"," \n","                tokenized_targets = tokenizer.batch_encode_plus(\n","                    [target], max_length=100, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                SOURCE_ID_memmap = np.memmap(\n","                    filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,512) \n","                )\n","\n","                SOURCE_MASK_memmap = np.memmap(\n","                    filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512,512)\n","                )\n","\n","\n","                TARGET_ID_memmap = np.memmap(\n","                    filename=TARGET_ID_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,100)\n","                )\n","\n","                TARGET_MASK_memmap = np.memmap(\n","                    filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,100)\n","                )\n","\n","                SOURCE_ID_memmap[list_index] = tokenized_inputs[\"input_ids\"].squeeze().numpy()\n","                SOURCE_MASK_memmap[list_index] = tokenized_inputs[\"attention_mask\"].squeeze().numpy()\n","\n","                TARGET_ID_memmap[list_index] = tokenized_targets[\"input_ids\"].squeeze().numpy()\n","                TARGET_MASK_memmap[list_index] = tokenized_targets[\"attention_mask\"].squeeze().numpy()\n","\n","                del SOURCE_ID_memmap\n","                del SOURCE_MASK_memmap\n","\n","                del TARGET_ID_memmap\n","                del TARGET_MASK_memmap\n","\n","                if index%500==0:\n","                    print(f\"{index} was finished\", index)\n","\n","                list_index += 1\n","\n","    if not os.path.isfile(\"processed/mhqg-wq/VAL_SOURCE_ID_memmap.npy\"):\n","        SOURCE_ID_PATH = \"processed/mhqg-wq/VAL_SOURCE_ID_memmap.npy\"\n","        SOURCE_MASK_PATH = \"processed/mhqg-wq/VAL_SOURCE_MASK_memmap.npy\"\n","        TARGET_ID_PATH = \"processed/mhqg-wq/VAL_TARGET_ID_memmap.npy\"\n","        TARGET_MASK_PATH = \"processed/mhqg-wq/VAL_TARGET_MASK_memmap.npy\"\n","        # input_length = 18624\n","        input_length = 2000\n","        SOURCE_ID_memmap = np.memmap(\n","        filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,512) \n","        )\n","\n","        SOURCE_MASK_memmap = np.memmap(\n","        filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512,512)\n","        )\n","\n","        TARGET_ID_memmap = np.memmap(\n","        filename=TARGET_ID_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,100)\n","        )\n","\n","        TARGET_MASK_memmap = np.memmap(\n","        filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,100)\n","        )\n","\n","        del SOURCE_ID_memmap\n","        del SOURCE_MASK_memmap\n","        del TARGET_ID_memmap\n","        del TARGET_MASK_memmap\n","        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        def _make_record(answer, input, target):\n","            # 質問生成タスク用の入出力形式に変換する。\n","            input = f\"{answer}{input}\"\n","            target = f\"{target}\"\n","            return input, target\n","\n","        with open(\"data/mhqg-wq/dev.json\", 'r') as f:\n","            list_index = 0\n","            for index, line in enumerate(f):\n","                line = line.strip()\n","                jo = json.loads(line, object_pairs_hook=OrderedDict)\n","                assert len(jo['inGraph']['g_adj']) > 0\n","\n","                answers = jo['answers']\n","                normalized_answers = \"\"\n","                for x in answers:\n","                    normalized_answers += \" <answer> \"\n","                    normalized_answers += x\n","                target = jo['outSeq']\n","\n","                graph = {'node_name_id2word':{}}\n","                for idx, nid in enumerate(jo['inGraph']['g_node_names']):\n","                    graph['node_name_id2word'][nid] = jo['inGraph']['g_node_names'][nid]\n","                input_knowledge_graph = \"\"\n","                for nid, val in jo['inGraph']['g_adj'].items():\n","                    for nid2, edge in val.items():\n","                        edge = edge.split('/')[-1]\n","                        input_knowledge_graph += \" <sep> \" + \" <subject> \"+ str(graph[\"node_name_id2word\"][nid])+\" <relation> \"+str(edge)+\" <object> \"+str(graph[\"node_name_id2word\"][nid2])\n","                \n","                input, target = _make_record(normalized_answers,input_knowledge_graph, target)\n"," \n","                tokenized_inputs = tokenizer.batch_encode_plus(\n","                    [input], max_length=512, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n"," \n","                tokenized_targets = tokenizer.batch_encode_plus(\n","                    [target], max_length=100, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                SOURCE_ID_memmap = np.memmap(\n","                    filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,512) \n","                )\n","\n","                SOURCE_MASK_memmap = np.memmap(\n","                    filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512,512)\n","                )\n","\n","\n","                TARGET_ID_memmap = np.memmap(\n","                    filename=TARGET_ID_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,100)\n","                )\n","\n","                TARGET_MASK_memmap = np.memmap(\n","                    filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,100)\n","                )\n","\n","                SOURCE_ID_memmap[list_index] = tokenized_inputs[\"input_ids\"].squeeze().numpy()\n","                SOURCE_MASK_memmap[list_index] = tokenized_inputs[\"attention_mask\"].squeeze().numpy()\n","\n","                TARGET_ID_memmap[list_index] = tokenized_targets[\"input_ids\"].squeeze().numpy()\n","                TARGET_MASK_memmap[list_index] = tokenized_targets[\"attention_mask\"].squeeze().numpy()\n","\n","                del SOURCE_ID_memmap\n","                del SOURCE_MASK_memmap\n","\n","                del TARGET_ID_memmap\n","                del TARGET_MASK_memmap\n","\n","                if index%500==0:\n","                    print(f\"{index} was finished\", index)\n","\n","                list_index += 1\n","\n","    if not os.path.isfile(\"processed/mhqg-wq/TEST_SOURCE_ID_memmap.npy\"):\n","        SOURCE_ID_PATH = \"processed/mhqg-wq/TEST_SOURCE_ID_memmap.npy\"\n","        SOURCE_MASK_PATH = \"processed/mhqg-wq/TEST_SOURCE_MASK_memmap.npy\"\n","        SOURCE_CROSS_MASK_PATH = \"processed/mhqg-wq/TEST_SOURCE_CROSS_MASK_memmap.npy\"\n","        TARGET_ID_PATH = \"processed/mhqg-wq/TEST_TARGET_ID_memmap.npy\"\n","        TARGET_MASK_PATH = \"processed/mhqg-wq/TEST_TARGET_MASK_memmap.npy\"\n","        # input_length = 18624\n","        input_length = 2000\n","        SOURCE_ID_memmap = np.memmap(\n","        filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,512) \n","        )\n","\n","        SOURCE_MASK_memmap = np.memmap(\n","        filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512,512)\n","        )\n","\n","        TARGET_ID_memmap = np.memmap(\n","        filename=TARGET_ID_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,100)\n","        )\n","\n","        TARGET_MASK_memmap = np.memmap(\n","        filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,100)\n","        )\n","\n","        del SOURCE_ID_memmap\n","        del SOURCE_MASK_memmap\n","        del TARGET_ID_memmap\n","        del TARGET_MASK_memmap\n","        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        def _make_record(answer, input, target):\n","            # 質問生成タスク用の入出力形式に変換する。\n","            input = f\"{answer}{input}\"\n","            target = f\"{target}\"\n","            return input, target\n","\n","        with open(\"data/mhqg-wq/test.json\", 'r') as f:\n","            list_index = 0\n","            for index, line in enumerate(f):\n","                line = line.strip()\n","                jo = json.loads(line, object_pairs_hook=OrderedDict)\n","                assert len(jo['inGraph']['g_adj']) > 0\n","\n","                answers = jo['answers']\n","                normalized_answers = \"\"\n","                for x in answers:\n","                    normalized_answers += \" <answer> \"\n","                    normalized_answers += x\n","                target = jo['outSeq']\n","                \n","                graph = {'node_name_id2word':{}}\n","                for idx, nid in enumerate(jo['inGraph']['g_node_names']):\n","                    graph['node_name_id2word'][nid] = jo['inGraph']['g_node_names'][nid]\n","                input_knowledge_graph = \"\"\n","                for nid, val in jo['inGraph']['g_adj'].items():\n","                    for nid2, edge in val.items():\n","                        edge = edge.split('/')[-1]\n","                        input_knowledge_graph += \" <sep> \" + \" <subject> \"+ str(graph[\"node_name_id2word\"][nid])+\" <relation> \"+str(edge)+\" <object> \"+str(graph[\"node_name_id2word\"][nid2])\n","                \n","                input, target = _make_record(normalized_answers,input_knowledge_graph, target)\n"," \n","                tokenized_inputs = tokenizer.batch_encode_plus(\n","                    [input], max_length=512, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n"," \n","                tokenized_targets = tokenizer.batch_encode_plus(\n","                    [target], max_length=100, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                SOURCE_ID_memmap = np.memmap(\n","                    filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,512) \n","                )\n","\n","                SOURCE_MASK_memmap = np.memmap(\n","                    filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512,512)\n","                )\n","\n","\n","                TARGET_ID_memmap = np.memmap(\n","                    filename=TARGET_ID_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,100)\n","                )\n","\n","                TARGET_MASK_memmap = np.memmap(\n","                    filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,100)\n","                )\n","\n","                SOURCE_ID_memmap[list_index] = tokenized_inputs[\"input_ids\"].squeeze().numpy()\n","                SOURCE_MASK_memmap[list_index] = tokenized_inputs[\"attention_mask\"].squeeze().numpy()\n","\n","                TARGET_ID_memmap[list_index] = tokenized_targets[\"input_ids\"].squeeze().numpy()\n","                TARGET_MASK_memmap[list_index] = tokenized_targets[\"attention_mask\"].squeeze().numpy()\n","\n","                del SOURCE_ID_memmap\n","                del SOURCE_MASK_memmap\n","\n","                del TARGET_ID_memmap\n","                del TARGET_MASK_memmap\n","\n","                if index%500==0:\n","                    print(f\"{index} was finished\", index)\n","\n","                list_index += 1"],"execution_count":15,"outputs":[]},{"cell_type":"code","metadata":{"id":"QbXyz1IxttUG","executionInfo":{"status":"ok","timestamp":1631936394530,"user_tz":-540,"elapsed":803,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":["def preprocess_pq(config):\n","    # preprocess input data\n","    if not os.path.isfile(\"processed/mhqg-pq/SOURCE_ID_memmap.npy\"):\n","        SOURCE_ID_PATH = \"processed/mhqg-pq/SOURCE_ID_memmap.npy\"\n","        SOURCE_MASK_PATH = \"processed/mhqg-pq/SOURCE_MASK_memmap.npy\"\n","        TARGET_ID_PATH = \"processed/mhqg-pq/TARGET_ID_memmap.npy\"\n","        TARGET_MASK_PATH = \"processed/mhqg-pq/TARGET_MASK_memmap.npy\"\n","        input_length = 9793\n","        SOURCE_ID_memmap = np.memmap(\n","        filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,512) \n","        )\n","\n","        SOURCE_MASK_memmap = np.memmap(\n","        filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512,512)\n","        )\n","\n","        TARGET_ID_memmap = np.memmap(\n","        filename=TARGET_ID_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,100)\n","        )\n","\n","        TARGET_MASK_memmap = np.memmap(\n","        filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,100)\n","        )\n","\n","        del SOURCE_ID_memmap\n","        del SOURCE_MASK_memmap\n","        del TARGET_ID_memmap\n","        del TARGET_MASK_memmap\n","        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        def _make_record(answer, input, target):\n","            #   質問生成タスク用の入出力形式に変換する。\n","            input = f\"{answer}{input}\"\n","            target = f\"{target}\"\n","            return input, target\n","\n","        with open(\"data/mhqg-pq/train.json\", 'r') as f:\n","            list_index = 0\n","            for index, line in enumerate(f):\n","                line = line.strip()\n","                jo = json.loads(line, object_pairs_hook=OrderedDict)\n","                assert len(jo['inGraph']['g_adj']) > 0\n","\n","                answers = jo['answers']\n","                normalized_answers = \"\"\n","                for x in answers:\n","                    normalized_answers += \" <answer> \"\n","                    normalized_answers += x\n","                target = jo['outSeq']\n","\n","                kglist = []\n","                for node in jo[\"inGraph\"][\"g_node_names\"]:\n","                  try:\n","                    kglist.append((node,jo[\"inGraph\"][\"g_adj\"][node]))\n","                  except KeyError:\n","                    continue\n","\n","                input_knowledge_graph = \"\"\n","                for subject, dict_of_subject in kglist:\n","                  for node in jo[\"inGraph\"][\"g_node_names\"]:\n","                    try:\n","                      input_knowledge_graph += \" <sep> \" + \" <subject> \"+ str(jo[\"inGraph\"][\"g_node_names\"][subject])+\" <relation> \"+str(\" \".join(dict_of_subject[node][0].split('/')[-1].split('_')))+\" <object> \"+str(jo[\"inGraph\"][\"g_node_names\"][node])\n","                    except KeyError:\n","                      continue\n","                \n","                input, target = _make_record(normalized_answers,input_knowledge_graph, target)\n","\n","                tokenized_inputs = tokenizer.batch_encode_plus(\n","                    [input], max_length=512, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                tokenized_targets = tokenizer.batch_encode_plus(\n","                    [target], max_length=100, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                tokenized_row_input = \"\"\n","                tokenized_row_input = tokenizer.tokenize(input)\n","                if len(tokenized_row_input) > 512:\n","                  continue\n","\n","                SOURCE_ID_memmap = np.memmap(\n","                    filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,512) \n","                )\n","\n","                SOURCE_MASK_memmap = np.memmap(\n","                    filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512,512)\n","                )\n","\n","                TARGET_ID_memmap = np.memmap(\n","                    filename=TARGET_ID_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,100)\n","                )\n","\n","                TARGET_MASK_memmap = np.memmap(\n","                    filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,100)\n","                )\n","\n","                SOURCE_ID_memmap[list_index] = tokenized_inputs[\"input_ids\"].squeeze().numpy()\n","                SOURCE_MASK_memmap[list_index] = tokenized_inputs[\"attention_mask\"].squeeze().numpy()\n","\n","                TARGET_ID_memmap[list_index] = tokenized_targets[\"input_ids\"].squeeze().numpy()\n","                TARGET_MASK_memmap[list_index] = tokenized_targets[\"attention_mask\"].squeeze().numpy()\n","\n","                del SOURCE_ID_memmap\n","                del SOURCE_MASK_memmap\n","                del TARGET_ID_memmap\n","                del TARGET_MASK_memmap\n","\n","                if index%500==0:\n","                    print(f\"{index} was finished\", index)\n","\n","                list_index += 1\n","\n","    # preprocess validation.json \n","    if not os.path.isfile(\"processed/mhqg-pq/VAL_SOURCE_ID_memmap.npy\"):\n","        SOURCE_ID_PATH = \"processed/mhqg-pq/VAL_SOURCE_ID_memmap.npy\"\n","        SOURCE_MASK_PATH = \"processed/mhqg-pq/VAL_SOURCE_MASK_memmap.npy\"\n","        TARGET_ID_PATH = \"processed/mhqg-pq/VAL_TARGET_ID_memmap.npy\"\n","        TARGET_MASK_PATH = \"processed/mhqg-pq/VAL_TARGET_MASK_memmap.npy\"\n","        input_length = 1000\n","        SOURCE_ID_memmap = np.memmap(\n","        filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,512) \n","        )\n","\n","        SOURCE_MASK_memmap = np.memmap(\n","        filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512,512)\n","        )\n","\n","        TARGET_ID_memmap = np.memmap(\n","        filename=TARGET_ID_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,100)\n","        )\n","\n","        TARGET_MASK_memmap = np.memmap(\n","        filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,100)\n","        )\n","\n","        del SOURCE_ID_memmap\n","        del SOURCE_MASK_memmap\n","        del TARGET_ID_memmap\n","        del TARGET_MASK_memmap\n","        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        def _make_record(answer, input, target):\n","            # 質問生成タスク用の入出力形式に変換する。\n","            input = f\"{answer}{input}\"\n","            target = f\"{target}\"\n","            return input, target\n","\n","        with open(\"data/mhqg-pq/dev.json\", 'r') as f:\n","            list_index = 0\n","            for index, line in enumerate(f):\n","                line = line.strip()\n","                jo = json.loads(line, object_pairs_hook=OrderedDict)\n","                assert len(jo['inGraph']['g_adj']) > 0\n","\n","                answers = jo['answers']\n","                normalized_answers = \"\"\n","                for x in answers:\n","                    normalized_answers += \" <answer> \"\n","                    normalized_answers += x\n","                target = jo['outSeq']\n","\n","                kglist = []\n","                for node in jo[\"inGraph\"][\"g_node_names\"]:\n","                  try:\n","                    kglist.append((node,jo[\"inGraph\"][\"g_adj\"][node]))\n","                  except KeyError:\n","                    continue\n","\n","                input_knowledge_graph = \"\"\n","                for subject, dict_of_subject in kglist:\n","                  for node in jo[\"inGraph\"][\"g_node_names\"]:\n","                    try:\n","                      input_knowledge_graph += \" <sep> \" + \" <subject> \"+ str(jo[\"inGraph\"][\"g_node_names\"][subject])+\" <relation> \"+str(\" \".join(dict_of_subject[node][0].split('/')[-1].split('_')))+\" <object> \"+str(jo[\"inGraph\"][\"g_node_names\"][node])\n","                    except KeyError:\n","                      continue\n","                \n","                input, target = _make_record(normalized_answers,input_knowledge_graph, target)\n","\n","                tokenized_inputs = tokenizer.batch_encode_plus(\n","                    [input], max_length=512, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                tokenized_targets = tokenizer.batch_encode_plus(\n","                    [target], max_length=100, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                tokenized_row_input = \"\"\n","                tokenized_row_input = tokenizer.tokenize(input)\n","                if len(tokenized_row_input) > 512:\n","                  continue\n","\n","                SOURCE_ID_memmap = np.memmap(\n","                    filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,512) \n","                )\n","\n","                SOURCE_MASK_memmap = np.memmap(\n","                    filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512,512)\n","                )\n","\n","                TARGET_ID_memmap = np.memmap(\n","                    filename=TARGET_ID_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,100)\n","                )\n","\n","                TARGET_MASK_memmap = np.memmap(\n","                    filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,100)\n","                )\n","\n","                SOURCE_ID_memmap[list_index] = tokenized_inputs[\"input_ids\"].squeeze().numpy()\n","                SOURCE_MASK_memmap[list_index] = tokenized_inputs[\"attention_mask\"].squeeze().numpy()\n","\n","                TARGET_ID_memmap[list_index] = tokenized_targets[\"input_ids\"].squeeze().numpy()\n","                TARGET_MASK_memmap[list_index] = tokenized_targets[\"attention_mask\"].squeeze().numpy()\n","\n","                del SOURCE_ID_memmap\n","                del SOURCE_MASK_memmap\n","                del TARGET_ID_memmap\n","                del TARGET_MASK_memmap\n","\n","                if index%500==0:\n","                    print(f\"{index} was finished\", index)\n","\n","                list_index += 1\n","\n","    # preprocess test.json\n","    if not os.path.isfile(\"processed/mhqg-pq/TEST_SOURCE_ID_memmap.npy\"):\n","        SOURCE_ID_PATH = \"processed/mhqg-pq/TEST_SOURCE_ID_memmap.npy\"\n","        SOURCE_MASK_PATH = \"processed/mhqg-pq/TEST_SOURCE_MASK_memmap.npy\"\n","        TARGET_ID_PATH = \"processed/mhqg-pq/TEST_TARGET_ID_memmap.npy\"\n","        TARGET_MASK_PATH = \"processed/mhqg-pq/TEST_TARGET_MASK_memmap.npy\"\n","        # input_length = 18624\n","        input_length = 1000\n","        SOURCE_ID_memmap = np.memmap(\n","        filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,512) \n","        )\n","\n","        SOURCE_MASK_memmap = np.memmap(\n","        filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,512,512)\n","        )\n","\n","        TARGET_ID_memmap = np.memmap(\n","        filename=TARGET_ID_PATH, dtype=np.int64, mode=\"w+\", shape=(input_length,100)\n","        )\n","\n","        TARGET_MASK_memmap = np.memmap(\n","        filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"w+\",shape=(input_length,100)\n","        )\n","\n","        del SOURCE_ID_memmap\n","        del SOURCE_MASK_memmap\n","        del TARGET_ID_memmap\n","        del TARGET_MASK_memmap\n","        tokenizer = T5Tokenizer.from_pretrained(\"t5-small\", is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        def _make_record(answer, input, target):\n","            # 質問生成タスク用の入出力形式に変換する。\n","            input = f\"{answer}{input}\"\n","            target = f\"{target}\"\n","            return input, target\n","\n","        with open(\"data/mhqg-pq/test.json\", 'r') as f:\n","            list_index = 0\n","            for index, line in enumerate(f):\n","                line = line.strip()\n","                jo = json.loads(line, object_pairs_hook=OrderedDict)\n","                assert len(jo['inGraph']['g_adj']) > 0\n","\n","                answers = jo['answers']\n","                normalized_answers = \"\"\n","                for x in answers:\n","                    normalized_answers += \" <answer> \"\n","                    normalized_answers += x\n","                target = jo['outSeq']\n","\n","                kglist = []\n","                for node in jo[\"inGraph\"][\"g_node_names\"]:\n","                  try:\n","                    kglist.append((node,jo[\"inGraph\"][\"g_adj\"][node]))\n","                  except KeyError:\n","                    continue\n","\n","                input_knowledge_graph = \"\"\n","                for subject, dict_of_subject in kglist:\n","                  for node in jo[\"inGraph\"][\"g_node_names\"]:\n","                    try:\n","                      input_knowledge_graph += \" <sep> \" + \" <subject> \"+ str(jo[\"inGraph\"][\"g_node_names\"][subject])+\" <relation> \"+str(\" \".join(dict_of_subject[node][0].split('/')[-1].split('_')))+\" <object> \"+str(jo[\"inGraph\"][\"g_node_names\"][node])\n","                    except KeyError:\n","                      continue\n","                \n","                input, target = _make_record(normalized_answers,input_knowledge_graph, target)\n","\n","                tokenized_inputs = tokenizer.batch_encode_plus(\n","                    [input], max_length=512, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                tokenized_targets = tokenizer.batch_encode_plus(\n","                    [target], max_length=100, truncation=True, \n","                    padding=\"max_length\", return_tensors=\"pt\"\n","                )\n","\n","                tokenized_row_input = \"\"\n","                tokenized_row_input = tokenizer.tokenize(input)\n","                if len(tokenized_row_input) > 512:\n","                  continue\n","\n","                SOURCE_ID_memmap = np.memmap(\n","                    filename=SOURCE_ID_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,512) \n","                )\n","\n","                SOURCE_MASK_memmap = np.memmap(\n","                    filename=SOURCE_MASK_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,512,512)\n","                )\n","\n","                TARGET_ID_memmap = np.memmap(\n","                    filename=TARGET_ID_PATH, dtype=np.int64, mode=\"r+\", shape=(input_length,100)\n","                )\n","\n","                TARGET_MASK_memmap = np.memmap(\n","                    filename=TARGET_MASK_PATH, dtype=np.int64, mode=\"r+\",shape=(input_length,100)\n","                )\n","\n","                SOURCE_ID_memmap[list_index] = tokenized_inputs[\"input_ids\"].squeeze().numpy()\n","                SOURCE_MASK_memmap[list_index] = tokenized_inputs[\"attention_mask\"].squeeze().numpy()\n","\n","                TARGET_ID_memmap[list_index] = tokenized_targets[\"input_ids\"].squeeze().numpy()\n","                TARGET_MASK_memmap[list_index] = tokenized_targets[\"attention_mask\"].squeeze().numpy()\n","\n","                del SOURCE_ID_memmap\n","                del SOURCE_MASK_memmap\n","                del TARGET_ID_memmap\n","                del TARGET_MASK_memmap\n","\n","                if index%500==0:\n","                    print(f\"{index} was finished\", index)\n","\n","                list_index += 1"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"unAD_UO8ttUK","executionInfo":{"status":"ok","timestamp":1631936394531,"user_tz":-540,"elapsed":7,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":["class Preprocess():\n","    def __init__(self, config):\n","        if config.experiment.data==\"mhqg-wq\":\n","            preprocess_wq(config)\n","        elif config.experiment.data==\"mhqg-pq\":\n","            preprocess_pq(config)\n","        else:\n","            raise"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"RRtJVAJ8zgH3","executionInfo":{"status":"ok","timestamp":1631936395881,"user_tz":-540,"elapsed":4,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":[""],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"10SyPISVzgQ0"},"source":["## Dataset"]},{"cell_type":"code","metadata":{"id":"e-1-k99KzgQ1","executionInfo":{"status":"ok","timestamp":1631936756875,"user_tz":-540,"elapsed":387,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":["from torch.utils.data import Dataset\n","import numpy as np\n","import json\n","import torch\n","from collections import Counter, defaultdict, OrderedDict\n","import gc\n","import os\n","\n","class JsonDatasetWQ(Dataset):\n","    def __init__(self, tokenizer, data_dir, type_path, input_max_len=512, target_max_len=512, mode=None):\n","        assert mode==\"Train\" or mode==\"Val\" or mode==\"Test\"\n","        self.file_path = os.path.join(data_dir, type_path)\n","        \n","        self.input_max_len = input_max_len\n","        self.target_max_len = target_max_len\n","        self.tokenizer = tokenizer\n","        self.inputs = []\n","        self.targets = []\n"," \n","        self.mode = mode\n"," \n","        if mode==\"Train\":\n","          self.SOURCE_ID_PATH = \"processed/mhqg-wq/SOURCE_ID_memmap.npy\"\n","          self.SOURCE_MASK_PATH = \"processed/mhqg-wq/SOURCE_MASK_memmap.npy\"\n","          self.TARGET_ID_PATH = \"processed/mhqg-wq/TARGET_ID_memmap.npy\"\n","          self.TARGET_MASK_PATH = \"processed/mhqg-wq/TARGET_MASK_memmap.npy\"\n","        #   self.input_length = 18624\n","          self.input_length = 18989\n","          self.list_index = 18989\n","        elif mode==\"Val\":\n","          self.SOURCE_ID_PATH = \"processed/mhqg-wq/VAL_SOURCE_ID_memmap.npy\"\n","          self.SOURCE_MASK_PATH = \"processed/mhqg-wq/VAL_SOURCE_MASK_memmap.npy\"\n","          self.TARGET_ID_PATH = \"processed/mhqg-wq/VAL_TARGET_ID_memmap.npy\"\n","          self.TARGET_MASK_PATH = \"processed/mhqg-wq/VAL_TARGET_MASK_memmap.npy\"\n","          self.input_length = 2000\n","          self.list_index = 2000\n","        #   self.input_length = 1985\n","        elif mode==\"Test\":\n","          self.SOURCE_ID_PATH = \"processed/mhqg-wq/TEST_SOURCE_ID_memmap.npy\"\n","          self.SOURCE_MASK_PATH = \"processed/mhqg-wq/TEST_SOURCE_MASK_memmap.npy\"\n","          self.TARGET_ID_PATH = \"processed/mhqg-wq/TEST_TARGET_ID_memmap.npy\"\n","          self.TARGET_MASK_PATH = \"processed/mhqg-wq/TEST_TARGET_MASK_memmap.npy\"\n","          self.input_length = 2000\n","          self.list_index = 2000\n","          \n","        #   self.input_length = 1985\n"," \n","        self.SOURCE_ID_memmap = np.memmap(\n","          filename=self.SOURCE_ID_PATH, dtype=np.int64, mode=\"r\",shape=(self.input_length,512) \n","        )\n"," \n","        self.SOURCE_MASK_memmap = np.memmap(\n","          filename=self.SOURCE_MASK_PATH, dtype=np.int64, mode=\"r\", shape=(self.input_length,512,512)\n","        )\n","\n","        self.TARGET_ID_memmap = np.memmap(\n","          filename=self.TARGET_ID_PATH, dtype=np.int64, mode=\"r\", shape=(self.input_length,100)\n","        )\n"," \n","        self.TARGET_MASK_memmap = np.memmap(\n","          filename=self.TARGET_MASK_PATH, dtype=np.int64, mode=\"r\",shape=(self.input_length,100)\n","        )\n","\n","  \n","    def __len__(self):\n","        return self.list_index\n","  \n","    def __getitem__(self, index):\n","        return {\"source_ids\": torch.from_numpy(np.array(self.SOURCE_ID_memmap[index])).squeeze(), \"source_mask\": torch.from_numpy(np.array(self.SOURCE_MASK_memmap[index])).squeeze(),\n","                \"target_ids\": torch.from_numpy(np.array(self.TARGET_ID_memmap[index])).squeeze(), \"target_mask\": torch.from_numpy(np.array(self.TARGET_MASK_memmap[index])).squeeze()}"],"execution_count":18,"outputs":[]},{"cell_type":"code","metadata":{"id":"2kMlZjqZzgQ1","executionInfo":{"status":"ok","timestamp":1631936398820,"user_tz":-540,"elapsed":451,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":["from torch.utils.data import Dataset\n","import numpy as np\n","import json\n","import torch\n","from collections import Counter, defaultdict, OrderedDict\n","import gc\n","import os\n","\n","class JsonDatasetPQ(Dataset):\n","    def __init__(self, tokenizer, data_dir, type_path, input_max_len=512, target_max_len=512, mode=None):\n","        assert mode==\"Train\" or mode==\"Val\" or mode==\"Test\"\n","        self.file_path = os.path.join(data_dir, type_path)\n","        \n","        self.input_max_len = input_max_len\n","        self.target_max_len = target_max_len\n","        self.tokenizer = tokenizer\n","        self.inputs = []\n","        self.targets = []\n"," \n","        self.mode = mode\n"," \n","        if mode==\"Train\":\n","          self.SOURCE_ID_PATH = \"processed/mhqg-pq/SOURCE_ID_memmap.npy\"\n","          self.SOURCE_MASK_PATH = \"processed/mhqg-pq/SOURCE_MASK_memmap.npy\"\n","          self.TARGET_ID_PATH = \"processed/mhqg-pq/TARGET_ID_memmap.npy\"\n","          self.TARGET_MASK_PATH = \"processed/mhqg-pq/TARGET_MASK_memmap.npy\"\n","          self.input_length = 9793\n","          self.list_index = 9793\n","        elif mode==\"Val\":\n","          self.SOURCE_ID_PATH = \"processed/mhqg-pq/VAL_SOURCE_ID_memmap.npy\"\n","          self.SOURCE_MASK_PATH = \"processed/mhqg-pq/VAL_SOURCE_MASK_memmap.npy\"\n","          self.TARGET_ID_PATH = \"processed/mhqg-pq/VAL_TARGET_ID_memmap.npy\"\n","          self.TARGET_MASK_PATH = \"processed/mhqg-pq/VAL_TARGET_MASK_memmap.npy\"\n","          self.input_length = 1000\n","          self.list_index = 1000\n","        elif mode==\"Test\":\n","          self.SOURCE_ID_PATH = \"processed/mhqg-pq/TEST_SOURCE_ID_memmap.npy\"\n","          self.SOURCE_MASK_PATH = \"processed/mhqg-pq/TEST_SOURCE_MASK_memmap.npy\"\n","          self.TARGET_ID_PATH = \"processed/mhqg-pq/TEST_TARGET_ID_memmap.npy\"\n","          self.TARGET_MASK_PATH = \"processed/mhqg-pq/TEST_TARGET_MASK_memmap.npy\"\n","          self.input_length = 1000\n","          self.list_index = 1000\n","          \n"," \n","        self.SOURCE_ID_memmap = np.memmap(\n","          filename=self.SOURCE_ID_PATH, dtype=np.int64, mode=\"r\",shape=(self.input_length,512) \n","        )\n"," \n","        self.SOURCE_MASK_memmap = np.memmap(\n","          filename=self.SOURCE_MASK_PATH, dtype=np.int64, mode=\"r\", shape=(self.input_length,512,512)\n","        )\n","\n"," \n","        self.TARGET_ID_memmap = np.memmap(\n","          filename=self.TARGET_ID_PATH, dtype=np.int64, mode=\"r\", shape=(self.input_length,100)\n","        )\n"," \n","        self.TARGET_MASK_memmap = np.memmap(\n","          filename=self.TARGET_MASK_PATH, dtype=np.int64, mode=\"r\",shape=(self.input_length,100)\n","        )\n","\n","  \n","    def __len__(self):\n","        return self.list_index\n","  \n","    def __getitem__(self, index):\n","        return {\"source_ids\": torch.from_numpy(np.array(self.SOURCE_ID_memmap[index])).squeeze(), \"source_mask\": torch.from_numpy(np.array(self.SOURCE_MASK_memmap[index])).squeeze(), \n","                \"target_ids\": torch.from_numpy(np.array(self.TARGET_ID_memmap[index])).squeeze(), \"target_mask\": torch.from_numpy(np.array(self.TARGET_MASK_memmap[index])).squeeze()}"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"p6Bz6Tf7zjMY","executionInfo":{"status":"ok","timestamp":1631936400181,"user_tz":-540,"elapsed":5,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":[""],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"U7g4AjwgzjV5"},"source":["## Training"]},{"cell_type":"code","metadata":{"id":"Z6511wEizjV6","executionInfo":{"status":"ok","timestamp":1631936401700,"user_tz":-540,"elapsed":7,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":["import pytorch_lightning as pl\n","from mytransformers.src.transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n",")\n","from torch.utils.data import DataLoader\n","\n","from tqdm.auto import tqdm\n","\n","import pandas as pd\n","from core.evaluation.eval import QGEvalCap\n","\n","from omegaconf import DictConfig\n","\n","def saveOutputs(hparams: DictConfig, inputs: list, outputs: list, targets: list) -> None:\n","  data = pd.DataFrame(list(zip(inputs, outputs, targets)), columns =['inputs', 'outputs', 'targets'])\n","  data.to_csv(os.path.join(\"out\",hparams.experiment.model_dir.split(\"/\")[-1]+\".csv\"),index=False, header=True)\n","\n","def run_eval(target_src, decoded_text) -> dict:\n","  assert len(target_src) == len(decoded_text)\n","  eval_targets = {}\n","  eval_predictions = {}\n","  for idx in range(len(target_src)):\n","      eval_targets[idx] = [target_src[idx]]\n","      eval_predictions[idx] = [decoded_text[idx]]\n","\n","  QGEval = QGEvalCap(eval_targets, eval_predictions)\n","  scores = QGEval.evaluate()\n","  return scores\n","\n","class T5FineTuner(pl.LightningModule):\n","    def __init__(self, config):\n","        super().__init__()\n","        self.config = config\n","\n","        # 事前学習済みモデルの読み込み\n","        self.model = T5ForConditionalGeneration.from_pretrained(config.experiment.model_name_or_path)\n","\n","        # トークナイザーの読み込み\n","        self.tokenizer = T5Tokenizer.from_pretrained(config.experiment.tokenizer_name_or_path, is_fast=True)\n","\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        self.save_hyperparameters()\n","\n","    def forward(self, input_ids, attention_mask=None, decoder_input_ids=None, \n","                decoder_attention_mask=None, labels=None, mode=None):\n","        \"\"\"順伝搬\"\"\"\n","        return self.model(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            decoder_input_ids=decoder_input_ids,\n","            decoder_attention_mask=decoder_attention_mask,\n","            labels=labels,\n","            three_dim_attention_mask=True\n","        )\n","\n","    def _step(self, batch, mode=None):\n","        \"\"\"ロス計算\"\"\"\n","        labels = batch[\"target_ids\"].detach().clone()\n","\n","        # All labels set to -100 are ignored (masked), \n","        # the loss is only computed for labels in [0, ..., config.vocab_size]\n","        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","        outputs = self(\n","            input_ids=batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            decoder_attention_mask=batch['target_mask'],\n","            labels=labels,\n","            mode=mode\n","        )\n","\n","        return outputs\n","\n","    def training_step(self, batch, batch_idx):\n","        \"\"\"訓練ステップ処理\"\"\"\n","        outputs = self._step(batch)\n","        loss = outputs[0]\n","        logit = outputs[1]\n","        # logging metrics we calculated by hand\n","        self.log('train/loss', loss, on_epoch=True)\n","        return {\"loss\": loss}\n","\n","    def validation_step(self, batch, batch_idx):\n","        \"\"\"バリデーションステップ処理\"\"\"\n","        outputs = self._step(batch)\n","        loss = outputs[0]\n","        logit = outputs[1]\n","        self.log(\"valid/loss_epoch\", loss)  # default on val/test is on_epoch only\n","        # return {\"val_loss\": loss}\n","\n","    def test_epoch_end(self, training_step_outputs):\n","        saveOutputs(self.config, self.inputs, self.outputs, self.targets)\n","        scores = run_eval(self.targets, self.outputs)\n","        wandb.log(scores)\n","\n","    def configure_optimizers(self):\n","        \"\"\"オプティマイザーとスケジューラーを作成する\"\"\"\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() \n","                            if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.config.training.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() \n","                            if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, \n","                          lr=self.config.training.learning_rate, \n","                          eps=self.config.training.adam_epsilon)\n","        self.optimizer = optimizer\n","\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer, num_warmup_steps=self.config.training.warmup_steps, \n","            num_training_steps=self.t_total\n","        )\n","        self.scheduler = scheduler\n","\n","        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}]\n","\n","    def get_dataset(self, tokenizer, type_path, args, mode=None):\n","        \"\"\"データセットを作成する\"\"\"\n","        if args.experiment.data==\"mhqg-wq\":\n","          return JsonDatasetWQ(\n","            tokenizer=tokenizer, \n","            data_dir=args.experiment.data_dir, \n","            type_path=type_path, \n","            input_max_len=args.model.max_input_length,\n","            target_max_len=args.model.max_target_length,\n","            mode=mode)\n","        else:\n","          return JsonDatasetPQ(\n","            tokenizer=tokenizer, \n","            data_dir=args.experiment.data_dir, \n","            type_path=type_path, \n","            input_max_len=args.model.max_input_length,\n","            target_max_len=args.model.max_target_length,\n","            mode=mode)\n","    \n","    def setup(self, stage=None):\n","        \"\"\"初期設定（データセットの読み込み）\"\"\"\n","        if stage == 'fit' or stage is None:\n","            train_dataset = self.get_dataset(tokenizer=self.tokenizer, \n","                                             type_path=\"train.json\", args=self.config, mode=\"Train\")\n","            self.train_dataset = train_dataset\n","\n","            val_dataset = self.get_dataset(tokenizer=self.tokenizer, \n","                                           type_path=\"dev.json\", args=self.config, mode=\"Val\")\n","            self.val_dataset = val_dataset\n","\n","            self.t_total = (\n","                (len(train_dataset) // (self.config.training.train_batch_size * max(1, self.config.training.n_gpu)))\n","                // self.config.training.gradient_accumulation_steps\n","                * float(self.config.training.num_train_epochs)\n","            )\n","        elif stage == 'test':\n","            val_dataset = self.get_dataset(tokenizer=self.tokenizer, \n","                                           type_path=\"test.json\", args=self.config, mode=\"Test\")\n","            self.test_dataset = test_dataset\n","\n","    def train_dataloader(self):\n","        \"\"\"訓練データローダーを作成する\"\"\"\n","        return DataLoader(self.train_dataset, \n","                          batch_size=self.config.training.train_batch_size, \n","                          drop_last=True, shuffle=True, num_workers=4)\n","\n","    def val_dataloader(self):\n","        \"\"\"バリデーションデータローダーを作成する\"\"\"\n","        return DataLoader(self.val_dataset, \n","                          batch_size=self.config.training.eval_batch_size, \n","                          num_workers=4)\n","        \n","    def test_dataloader(self):\n","        \"\"\"バリデーションデータローダーを作成する\"\"\"\n","        return DataLoader(self.val_dataset, \n","                          batch_size=self.config.training.test_batch_size, \n","                          num_workers=4)"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"WyQsWDHWzljA","executionInfo":{"status":"ok","timestamp":1631936403761,"user_tz":-540,"elapsed":728,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":[""],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"sQFWzaOnzlsf"},"source":["## Evaluation"]},{"cell_type":"code","metadata":{"id":"BWZfboQ1zlsf","executionInfo":{"status":"ok","timestamp":1631936406275,"user_tz":-540,"elapsed":1971,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":["from mytransformers.src.transformers import (\n","    AdamW,\n","    T5ForConditionalGeneration,\n","    T5Tokenizer,\n","    get_linear_schedule_with_warmup\n",")\n","from torch.utils.data import DataLoader\n","\n","from tqdm.auto import tqdm\n","\n","import pandas as pd\n","from core.evaluation.eval import QGEvalCap\n","\n","from omegaconf import DictConfig\n","\n","\n","# OmegaConf.register_new_resolver(\"now\", lambda pattern: strftime(pattern, localtime()))\n","\n","def saveOutputs(hparams: DictConfig, inputs: list, outputs: list, targets: list) -> None:\n","  data = pd.DataFrame(list(zip(inputs, outputs, targets)), columns =['inputs', 'outputs', 'targets'])\n","  data.to_csv(os.path.join(\"out\",hparams.experiment.model_dir.split(\"/\")[-1]+\".csv\"),index=False, header=True)\n","\n","def run_eval(target_src, decoded_text) -> dict:\n","  assert len(target_src) == len(decoded_text)\n","  eval_targets = {}\n","  eval_predictions = {}\n","  for idx in range(len(target_src)):\n","      eval_targets[idx] = [target_src[idx]]\n","      eval_predictions[idx] = [decoded_text[idx]]\n","\n","  QGEval = QGEvalCap(eval_targets, eval_predictions)\n","  scores = QGEval.evaluate()\n","  return scores\n","\n","class T5Evaluation(pl.LightningModule):\n","    def __init__(self, config: DictConfig, train_params: DictConfig):\n","        super().__init__()\n","        self.config = config\n","        self.train_params = train_params\n","        # 事前学習済みモデルの読み込み\n","        self.model = T5ForConditionalGeneration.from_pretrained(self.config.experiment.model_dir)\n","\n","        # トークナイザーの読み込み\n","        self.tokenizer = T5Tokenizer.from_pretrained(self.config.experiment.model_dir, is_fast=True)\n","        special_tokens_dict = {'sep_token': '<sep>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<answer>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<SEP>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<subject>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<relation>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        special_tokens_dict = {'sep_token': '<object>'}\n","        self.tokenizer.add_special_tokens(special_tokens_dict)\n","\n","        self.save_hyperparameters()\n","\n","        self.inputs = []\n","        self.outputs = []\n","        self.targets = []\n","\n","\n","    def forward(self, input_ids, attention_mask=None, cross_attention_mask=None, decoder_input_ids=None, \n","                decoder_attention_mask=None, labels=None):\n","        \"\"\"順伝搬\"\"\"\n","        return self.model.generate(input_ids=input_ids, \n","            attention_mask=attention_mask, \n","            cross_attention_mask=cross_attention_mask,\n","            max_length=self.config.model.max_target_length,\n","            temperature=1.0,          # 生成にランダム性を入れる温度パラメータ\n","            repetition_penalty=1.5,   # 同じ文の繰り返し（モード崩壊）へのペナルティ\n","            three_dim_attention_mask=True\n","            )\n","\n","    def _step(self, batch):\n","        \"\"\"ロス計算\"\"\"\n","        labels = batch[\"target_ids\"]\n","\n","        # All labels set to -100 are ignored (masked), \n","        # the loss is only computed for labels in [0, ..., config.vocab_size]\n","        labels[labels[:, :] == self.tokenizer.pad_token_id] = -100\n","\n","        outputs = self(\n","            input_ids=batch[\"source_ids\"],\n","            attention_mask=batch[\"source_mask\"],\n","            cross_attention_mask=batch[\"cross_attention_mask\"],\n","            decoder_attention_mask=batch['target_mask'],\n","            labels=labels\n","        )\n","\n","        return outputs\n","\n","    def test_step(self, batch, batch_idx):\n","        \"\"\"テストステップ処理\"\"\"\n","        output = self._step(batch)\n","        labels = batch[\"target_ids\"]\n","        labels[labels[:, :] == -100] = 0\n","        output_text = [self.tokenizer.decode(ids, skip_special_tokens=True, \n","                                clean_up_tokenization_spaces=False) \n","                    for ids in output]\n","        target_text = [self.tokenizer.decode(ids, skip_special_tokens=True, \n","                                clean_up_tokenization_spaces=False) \n","                    for ids in labels]\n","        input_text = [self.tokenizer.decode(ids, skip_special_tokens=False, \n","                                clean_up_tokenization_spaces=False) \n","                    for ids in batch[\"source_ids\"]]\n","\n","        self.inputs.extend(input_text)\n","        self.outputs.extend(output_text)\n","        self.targets.extend(target_text)\n","\n","        return {\"batch_idx\":batch_idx}\n","        \n","    \n","    def test_epoch_end(self, training_step_outputs):\n","        saveOutputs(self.config, self.inputs, self.outputs, self.targets)\n","        scores = run_eval(self.targets, self.outputs)\n","        wandb.log(scores)\n","\n","    def configure_optimizers(self):\n","        \"\"\"オプティマイザーとスケジューラーを作成する\"\"\"\n","        model = self.model\n","        no_decay = [\"bias\", \"LayerNorm.weight\"]\n","        optimizer_grouped_parameters = [\n","            {\n","                \"params\": [p for n, p in model.named_parameters() \n","                            if not any(nd in n for nd in no_decay)],\n","                \"weight_decay\": self.config.training.weight_decay,\n","            },\n","            {\n","                \"params\": [p for n, p in model.named_parameters() \n","                            if any(nd in n for nd in no_decay)],\n","                \"weight_decay\": 0.0,\n","            },\n","        ]\n","        optimizer = AdamW(optimizer_grouped_parameters, \n","                          lr=self.config.training.learning_rate, \n","                          eps=self.config.training.adam_epsilon)\n","        self.optimizer = optimizer\n","\n","        scheduler = get_linear_schedule_with_warmup(\n","            optimizer, num_warmup_steps=self.config.training.warmup_steps, \n","            num_training_steps=self.t_total\n","        )\n","        self.scheduler = scheduler\n","\n","        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\", \"frequency\": 1}]\n","\n","    def get_dataset(self, tokenizer, type_path, mode=None):\n","        \"\"\"データセットを作成する\"\"\"\n","        if self.config.get('experiment').data==\"mhqg-wq\":\n","          return JsonDatasetWQ(\n","            tokenizer=tokenizer, \n","            data_dir=self.config.experiment.data_dir, \n","            type_path=type_path, \n","            input_max_len=self.config.model.max_input_length,\n","            target_max_len=self.config.model.max_target_length,\n","            mode=mode)\n","        else:\n","          return JsonDatasetPQ(\n","            tokenizer=tokenizer, \n","            data_dir=self.config.experiment.data_dir, \n","            type_path=type_path, \n","            input_max_len=self.config.model.max_input_length,\n","            target_max_len=self.config.model.max_target_length,\n","            mode=mode)\n","    \n","    def setup(self, stage=None):\n","        \"\"\"初期設定（データセットの読み込み）\"\"\"\n","        if stage == 'test':\n","            test_dataset = self.get_dataset(tokenizer=self.tokenizer, \n","                                           type_path=\"test.json\", mode=\"Test\")\n","            self.test_dataset = test_dataset\n","        \n","    def test_dataloader(self):\n","        \"\"\"バリデーションデータローダーを作成する\"\"\"\n","        return DataLoader(self.test_dataset, \n","                          batch_size=self.config.training.test_batch_size, \n","                          num_workers=4)\n","\n","class Evaluation():\n","  def __init__(self, config, train_params):\n","    self.config = config\n","    self.train_params = train_params\n","\n","  def run(self):\n","    # Tokenizer\n","    tokenizer = T5Tokenizer.from_pretrained(self.config.experiment.model_dir, is_fast=True)\n","    trained_model = T5ForConditionalGeneration.from_pretrained(self.config.experiment.model_dir)\n","\n","    if self.config.experiment.data==\"mhqg-wq\":\n","        # import test data\n","        test_dataset = JsonDatasetWQ(tokenizer, self.config.experiment.data_dir, \"test.json\", \n","                                input_max_len=self.config.model.max_input_length, \n","                                target_max_len=self.config.model.max_target_length,\n","                                mode=\"Test\")\n","\n","    test_loader = DataLoader(test_dataset, batch_size=8, num_workers=4)\n","\n","    trained_model.eval()\n","\n","    inputs = []\n","    outputs = []\n","    targets = []\n","\n","    for index, batch in enumerate(tqdm(test_loader)):\n","        input_ids = batch['source_ids']\n","        input_mask = batch['source_mask']\n","        input_cross_attention_mask = batch[\"cross_attention_mask\"]\n","        if self.config.training.n_gpu:\n","            input_ids = input_ids.cuda()\n","            input_mask = input_mask.cuda()\n","\n","        output = trained_model.generate(input_ids=input_ids, \n","            attention_mask=input_mask, \n","            cross_attention_mask=input_cross_attention_mask,\n","            max_length=self.config.model.max_target_length,\n","            temperature=1.0,          # 生成にランダム性を入れる温度パラメータ\n","            repetition_penalty=1.5,   # 同じ文の繰り返し（モード崩壊）へのペナルティ\n","            three_dim_attention_mask=True\n","            )\n","\n","        output_text = [tokenizer.decode(ids, skip_special_tokens=True, \n","                                clean_up_tokenization_spaces=False) \n","                    for ids in output]\n","        target_text = [tokenizer.decode(ids, skip_special_tokens=True, \n","                                clean_up_tokenization_spaces=False) \n","                    for ids in batch[\"target_ids\"]]\n","        input_text = [tokenizer.decode(ids, skip_special_tokens=False, \n","                                clean_up_tokenization_spaces=False) \n","                    for ids in input_ids]\n","\n","        inputs.extend(input_text)\n","        outputs.extend(output_text)\n","        targets.extend(target_text)\n","\n","        saveOutputs(self.config, inputs, outputs, targets)\n","        scores = run_eval(targets, outputs)\n","        self.train_params[\"logger\"].log_metrics(scores)"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jmz3gJtEzqF0","executionInfo":{"status":"ok","timestamp":1631936407054,"user_tz":-540,"elapsed":3,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":[""],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"unEZQgqGzqPY"},"source":["## Main"]},{"cell_type":"code","metadata":{"id":"VMfhJxzozqPZ","executionInfo":{"status":"ok","timestamp":1631936407362,"user_tz":-540,"elapsed":6,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":["import argparse\n","import glob\n","import os\n","import json\n","import time\n","import logging\n","import random\n","from itertools import chain\n","from string import punctuation\n"," \n","import numpy as np\n","import torch\n","from torch.utils.data import Dataset, DataLoader\n","\n","from collections import Counter, defaultdict, OrderedDict\n","from nltk.tokenize import wordpunct_tokenize, word_tokenize\n"," \n","import gc\n","\n","from omegaconf import DictConfig\n","import omegaconf\n","\n","import hydra\n","import pytorch_lightning as pl\n","\n","\n","\n","import textwrap\n","from tqdm.auto import tqdm\n","from sklearn import metrics\n","\n","from pytorch_lightning.loggers import WandbLogger\n","from pytorch_lightning.callbacks import EarlyStopping\n","\n","\n","def set_seed(seed):\n","    random.seed(seed)\n","    np.random.seed(seed)\n","    torch.manual_seed(seed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(seed)\n","\n","def get_train_param(config: DictConfig) -> dict:\n","    wandb_logger = WandbLogger(\n","        name=(\"exp_\" + str(config.experiment.wandb.exp_num)),\n","        project=config.experiment.wandb.project,\n","        log_model=True,\n","    )\n","    checkpoint_path = os.path.join(\n","        config.experiment.model_dir, config.experiment.wandb.checkpoint_path\n","    )\n","    wandb_logger.log_hyperparams(config)\n","    early_stopping = EarlyStopping('valid/loss_epoch')\n","    train_params = dict(\n","        callbacks=[early_stopping],\n","        logger = wandb_logger,\n","        accumulate_grad_batches=config.training.gradient_accumulation_steps,\n","        gpus=config.training.n_gpu,\n","        max_epochs=config.training.num_train_epochs,\n","        precision= 16 if config.training.fp_16 else 32,\n","        amp_level=config.training.opt_level,\n","        gradient_clip_val=config.training.max_grad_norm,\n","    )\n","    return train_params\n","\n","# @hydra.main(config_path=\"config.yaml\")\n","def train(config: DictConfig, train_params: dict) -> None:\n","    \n","    # conduct transfer learning\n","    model = T5FineTuner(config)\n","    wandb.watch(model, log_freq=100)\n","    trainer = pl.Trainer(**train_params)\n","    trainer.fit(model)\n","\n","    model.tokenizer.save_pretrained(config.experiment.model_dir)\n","    model.model.save_pretrained(config.experiment.model_dir)\n","\n","    # trainer.test(model)\n","\n","    # torch.save(\n","    #             {\"model\": model.state_dict(), \"preds\": preds}, OUTPUT_DIR + f\"bert-base-uncased_fold{fold}_best.pth\"\n","    #         )\n","\n","# @hydra.main(config_path=\"adjacencyattentionwithoutselfloop\", config_name=\"config\")\n","def evaluation(config: DictConfig, train_params: dict) -> None:\n","    print(\"config\",config)\n","    print(\"type_config\",type(config))\n","    set_seed(42)\n","    USE_GPU = torch.cuda.is_available()\n","    config.training.n_gpu=1 if USE_GPU else 0\n","    eval_model = T5Evaluation(config, train_params)\n","    trainer = pl.Trainer(**train_params)\n","    trainer.test(eval_model)\n","  \n","def main(config: DictConfig) -> None:\n","    if os.path.exists(config.experiment.model_dir):\n","        raise \"model_dir is exist\"\n","\n","    set_seed(42)\n","    USE_GPU = torch.cuda.is_available()\n","    config.training.n_gpu=1 if USE_GPU else 0\n","    train_params = get_train_param(config)\n","\n","    Preprocess(config)\n","    train(config, train_params)\n","    evaluation(config, train_params)\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"vGA1hL9565ow","executionInfo":{"status":"ok","timestamp":1631936412105,"user_tz":-540,"elapsed":225,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}}},"source":["!mkdir processed/mhqg-wq"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"KyuMD2dpzqPZ","colab":{"base_uri":"https://localhost:8080/","height":268,"referenced_widgets":["702e6bb503ca4a4885e3920c05a7cabf","d1099ccaea9641b4b465b550fddcf4db","99773eb59e0e45cd8b211e10f1f59820","5319edc0707e40eda070b6e285ee6906","813fb8ea9a01480e9f5ca3d13d5dd44d","ee1e2fccdeaf4568b5eadd134e8b896a","0794694188694a82b247070ceb595d94","7e8560edaf2240078f7b5bb0dd65af1e","181463f2563748eaa89d60c42f8951b0","363f5049239b42268afc08478e8c1b63","125b60ae92bf4ce096524a6ae64e99c8","e38915d2b83044c2a66cdf4a1edf3d14","0a4fa8640fd54e459e4ec5c8d7298d1e","91f253f3a1204785a2f7c9033b024640","aeffe848a6044e6fa625c15ff0d48686","4b0f98402fde4fbd830d931893bbe529","dae97cfc37fd419893e5fccd21f747c7","6d746117a5044cc0b506b06f0aa0af79","9f8bf4310d4247779fc103cfe36972d6","cc1543901dcf416eb4d202d15fb4b778","a62ce01af61d46abb113e5f5ca275cad","64e1c33027804be6938a151499cb2f36"]},"outputId":"ebdf003c-4e47-4a15-cf5c-7af0a53ef9a4"},"source":["config = omegaconf.OmegaConf.load(\"config/config_test_pq.yaml\")\n","main(config)\n","wandb.finish()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["GPU available: False, used: False\n","TPU available: None, using: 0 TPU cores\n","\n","  | Name  | Type                       | Params\n","-----------------------------------------------------\n","0 | model | T5ForConditionalGeneration | 60.5 M\n","-----------------------------------------------------\n","60.5 M    Trainable params\n","0         Non-trainable params\n","60.5 M    Total params\n","242.026   Total estimated model params size (MB)\n"]},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"702e6bb503ca4a4885e3920c05a7cabf","version_minor":0,"version_major":2},"text/plain":["Validation sanity check: 0it [00:00, ?it/s]"]},"metadata":{}},{"output_type":"display_data","data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e38915d2b83044c2a66cdf4a1edf3d14","version_minor":0,"version_major":2},"text/plain":["Training: 0it [00:00, ?it/s]"]},"metadata":{}}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"aAxIgrx9_Imb","executionInfo":{"status":"error","timestamp":1631936687065,"user_tz":-540,"elapsed":134137,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"4ac0c597-10dc-413b-d3a1-92b094fdbea6"},"source":["config = omegaconf.OmegaConf.load(\"config/config_test_wq.yaml\")\n","main(config)\n","wandb.finish()"],"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["0 was finished 0\n","500 was finished 500\n","1000 was finished 1000\n","1500 was finished 1500\n"]},{"output_type":"stream","name":"stderr","text":["GPU available: False, used: False\n","TPU available: None, using: 0 TPU cores\n"]},{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-16-4a9220e9900c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0momegaconf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOmegaConf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"config/config_test_wq.yaml\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-127d8989261f>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m     \u001b[0mPreprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m     \u001b[0mevaluation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-10-127d8989261f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(config, train_params)\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mwandb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrainer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mtrain_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m     \u001b[0mtrainer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, model, train_dataloader, val_dataloaders, datamodule)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[0;31m# SET UP TRAINING\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    470\u001b[0m         \u001b[0;31m# ----------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 471\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_setup_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    472\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"on_before_accelerator_backend_setup\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maccelerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py\u001b[0m in \u001b[0;36mcall_setup_hook\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m   1070\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatamodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1071\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1072\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstage_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1073\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_reset_result_and_set_hook_fx_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-651105ffff62>\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m    166\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'fit'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mstage\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m             train_dataset = self.get_dataset(tokenizer=self.tokenizer, \n\u001b[0;32m--> 168\u001b[0;31m                                              type_path=\"train.json\", args=self.config, mode=\"Train\")\n\u001b[0m\u001b[1;32m    169\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-8-651105ffff62>\u001b[0m in \u001b[0;36mget_dataset\u001b[0;34m(self, tokenizer, type_path, args, mode)\u001b[0m\n\u001b[1;32m    152\u001b[0m             \u001b[0minput_max_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_input_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m             \u001b[0mtarget_max_len\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_target_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             mode=mode)\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m           return JsonDatasetPQ(\n","\u001b[0;32m<ipython-input-6-0fec6b92e4a6>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, tokenizer, data_dir, type_path, input_max_len, target_max_len, mode)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         self.VAL_SOURCE_CROSS_MASK_memmap = np.memmap(\n\u001b[0;32m---> 60\u001b[0;31m           \u001b[0mfilename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSOURCE_CROSS_MASK_PATH\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"r\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_length\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m512\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         )\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/numpy/core/memmap.py\u001b[0m in \u001b[0;36m__new__\u001b[0;34m(subtype, filename, dtype, mode, offset, shape, order)\u001b[0m\n\u001b[1;32m    226\u001b[0m             \u001b[0mf_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnullcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m             \u001b[0mf_ctx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mos_fspath\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'r'\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mf_ctx\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfid\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'processed/mhqg-wq/SOURCE_CROSS_MASK_memmap.npy'"]}]},{"cell_type":"code","metadata":{"id":"xnhgk0rQzqPa"},"source":["config = omegaconf.OmegaConf.load(\"config/config_test_pq.yaml\")\n","train_params = get_train_param(config)\n","evaluation(config, train_params)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":38},"id":"Z8rlgwBozqPa","executionInfo":{"status":"ok","timestamp":1631934184653,"user_tz":-540,"elapsed":473,"user":{"displayName":"Kosuke Aigo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj8bGNs_XsxiFA4-kq5-koM9uVsQo4bz2KIDtRQrw=s64","userId":"01303562836135023230"}},"outputId":"da5be14b-32d9-47eb-b2f0-5046f591caf6"},"source":["pwd"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"},"text/plain":["'/content/drive/My Drive/ColabNotebooks/Research/KG2QGwithT5/TripleAttention'"]},"metadata":{},"execution_count":18}]},{"cell_type":"code","metadata":{"id":"CqJDAI5m23b2"},"source":[""],"execution_count":null,"outputs":[]}]}