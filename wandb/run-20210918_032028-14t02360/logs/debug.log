2021-09-18 03:20:28,204 INFO    MainThread:1770 [wandb_init.py:_log_setup():292] Logging user logs to wandb/run-20210918_032028-14t02360/logs/debug.log
2021-09-18 03:20:28,205 INFO    MainThread:1770 [wandb_init.py:_log_setup():293] Logging internal logs to wandb/run-20210918_032028-14t02360/logs/debug-internal.log
2021-09-18 03:20:28,205 INFO    MainThread:1770 [wandb_setup.py:_flush():69] setting env: {}
2021-09-18 03:20:28,205 INFO    MainThread:1770 [wandb_setup.py:_flush():69] setting user settings: {'email': 'saburotyan758@gmail.com'}
2021-09-18 03:20:28,205 INFO    MainThread:1770 [wandb_setup.py:_flush():69] multiprocessing start_methods=fork,spawn,forkserver
2021-09-18 03:20:28,206 INFO    MainThread:1770 [wandb_setup.py:_flush():69] configuring jupyter hooks <wandb.sdk.wandb_init._WandbInit object at 0x7f5670029990>
2021-09-18 03:20:30,418 INFO    MainThread:1770 [wandb_run.py:_console_start():1276] atexit reg
2021-09-18 03:20:30,427 INFO    MainThread:1770 [wandb_run.py:_redirect():1146] redirect: SettingsConsole.WRAP
2021-09-18 03:20:30,428 INFO    MainThread:1770 [wandb_run.py:_redirect():1177] Wrapping output streams.
2021-09-18 03:20:30,428 INFO    MainThread:1770 [wandb_run.py:_redirect():1193] Redirects installed.
2021-09-18 03:20:30,430 INFO    MainThread:1770 [wandb_run.py:_config_callback():586] config_cb None None {'_wandb': {'cli_version': '0.10.7', 'python_version': '3.7.12', 'is_jupyter_run': True, 'is_kaggle_kernel': False, 'framework': 'torch'}, 'experiment/seed': 42, 'experiment/data': 'mhqg-pq', 'experiment/data_dir': 'data/mhqg-pq/', 'experiment/model_name_or_path': 't5-small', 'experiment/tokenizer_name_or_path': 't5-small', 'experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'experiment/wandb/exp_num': 1, 'experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'experiment/wandb/checkpoint_path': 'checkpoint/', 'model/max_input_length': 100, 'model/max_target_length': 100, 'training/learning_rate': 0.0003, 'training/weight_decay': 0.0, 'training/adam_epsilon': 1e-08, 'training/warmup_steps': 0, 'training/gradient_accumulation_steps': 1, 'training/early_stop_callback': True, 'training/fp_16': False, 'training/opt_level': 'O1', 'training/max_grad_norm': 1.0, 'training/seed': 42, 'training/train_batch_size': 32, 'training/eval_batch_size': 32, 'training/test_batch_size': 8, 'training/num_train_epochs': 10, 'training/precision': 16, 'training/n_gpu': 0}
2021-09-18 03:20:30,890 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:20:53,250 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:20:53,386 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:20:54,651 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:20:54,677 INFO    MainThread:1770 [wandb_run.py:_config_callback():586] config_cb None None {'_wandb': {'cli_version': '0.10.7', 'python_version': '3.7.12', 'is_jupyter_run': True, 'is_kaggle_kernel': False, 'framework': 'torch'}, 'experiment/seed': 42, 'experiment/data': 'mhqg-pq', 'experiment/data_dir': 'data/mhqg-pq/', 'experiment/model_name_or_path': 't5-small', 'experiment/tokenizer_name_or_path': 't5-small', 'experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'experiment/wandb/exp_num': 1, 'experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'experiment/wandb/checkpoint_path': 'checkpoint/', 'model/max_input_length': 100, 'model/max_target_length': 100, 'training/learning_rate': 0.0003, 'training/weight_decay': 0.0, 'training/adam_epsilon': 1e-08, 'training/warmup_steps': 0, 'training/gradient_accumulation_steps': 1, 'training/early_stop_callback': True, 'training/fp_16': False, 'training/opt_level': 'O1', 'training/max_grad_norm': 1.0, 'training/seed': 42, 'training/train_batch_size': 32, 'training/eval_batch_size': 32, 'training/test_batch_size': 8, 'training/num_train_epochs': 10, 'training/precision': 16, 'training/n_gpu': 0}
2021-09-18 03:20:54,738 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:21:02,321 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:21:02,456 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:21:04,530 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:21:04,554 INFO    MainThread:1770 [wandb_run.py:_config_callback():586] config_cb None None {'_wandb': {'cli_version': '0.10.7', 'python_version': '3.7.12', 'is_jupyter_run': True, 'is_kaggle_kernel': False, 'framework': 'torch'}, 'experiment/seed': 42, 'experiment/data': 'mhqg-pq', 'experiment/data_dir': 'data/mhqg-pq/', 'experiment/model_name_or_path': 't5-small', 'experiment/tokenizer_name_or_path': 't5-small', 'experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'experiment/wandb/exp_num': 1, 'experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'experiment/wandb/checkpoint_path': 'checkpoint/', 'model/max_input_length': 100, 'model/max_target_length': 100, 'training/learning_rate': 0.0003, 'training/weight_decay': 0.0, 'training/adam_epsilon': 1e-08, 'training/warmup_steps': 0, 'training/gradient_accumulation_steps': 1, 'training/early_stop_callback': True, 'training/fp_16': False, 'training/opt_level': 'O1', 'training/max_grad_norm': 1.0, 'training/seed': 42, 'training/train_batch_size': 32, 'training/eval_batch_size': 32, 'training/test_batch_size': 8, 'training/num_train_epochs': 10, 'training/precision': 16, 'training/n_gpu': 0}
2021-09-18 03:21:05,153 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:23:30,433 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:23:32,269 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:23:45,162 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:23:45,196 INFO    MainThread:1770 [wandb_run.py:_config_callback():586] config_cb None None {'_wandb': {'cli_version': '0.10.7', 'python_version': '3.7.12', 'is_jupyter_run': True, 'is_kaggle_kernel': False, 'framework': 'torch'}, 'experiment/seed': 42, 'experiment/data': 'mhqg-pq', 'experiment/data_dir': 'data/mhqg-pq/', 'experiment/model_name_or_path': 't5-small', 'experiment/tokenizer_name_or_path': 't5-small', 'experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'experiment/wandb/exp_num': 1, 'experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'experiment/wandb/checkpoint_path': 'checkpoint/', 'model/max_input_length': 100, 'model/max_target_length': 100, 'training/learning_rate': 0.0003, 'training/weight_decay': 0.0, 'training/adam_epsilon': 1e-08, 'training/warmup_steps': 0, 'training/gradient_accumulation_steps': 1, 'training/early_stop_callback': True, 'training/fp_16': False, 'training/opt_level': 'O1', 'training/max_grad_norm': 1.0, 'training/seed': 42, 'training/train_batch_size': 32, 'training/eval_batch_size': 32, 'training/test_batch_size': 8, 'training/num_train_epochs': 10, 'training/precision': 16, 'training/n_gpu': 0}
2021-09-18 03:23:45,664 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:27:38,314 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:27:39,383 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:27:54,844 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:27:54,867 INFO    MainThread:1770 [wandb_run.py:_config_callback():586] config_cb None None {'_wandb': {'cli_version': '0.10.7', 'python_version': '3.7.12', 'is_jupyter_run': True, 'is_kaggle_kernel': False, 'framework': 'torch'}, 'experiment/seed': 42, 'experiment/data': 'mhqg-pq', 'experiment/data_dir': 'data/mhqg-pq/', 'experiment/model_name_or_path': 't5-small', 'experiment/tokenizer_name_or_path': 't5-small', 'experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'experiment/wandb/exp_num': 1, 'experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'experiment/wandb/checkpoint_path': 'checkpoint/', 'model/max_input_length': 100, 'model/max_target_length': 100, 'training/learning_rate': 0.0003, 'training/weight_decay': 0.0, 'training/adam_epsilon': 1e-08, 'training/warmup_steps': 0, 'training/gradient_accumulation_steps': 1, 'training/early_stop_callback': True, 'training/fp_16': False, 'training/opt_level': 'O1', 'training/max_grad_norm': 1.0, 'training/seed': 42, 'training/train_batch_size': 32, 'training/eval_batch_size': 32, 'training/test_batch_size': 8, 'training/num_train_epochs': 10, 'training/precision': 16, 'training/n_gpu': 0}
2021-09-18 03:27:55,193 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:28:15,904 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:28:17,434 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:28:29,947 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:28:29,981 INFO    MainThread:1770 [wandb_run.py:_config_callback():586] config_cb None None {'_wandb': {'cli_version': '0.10.7', 'python_version': '3.7.12', 'is_jupyter_run': True, 'is_kaggle_kernel': False, 'framework': 'torch'}, 'experiment/seed': 42, 'experiment/data': 'mhqg-pq', 'experiment/data_dir': 'data/mhqg-pq/', 'experiment/model_name_or_path': 't5-small', 'experiment/tokenizer_name_or_path': 't5-small', 'experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'experiment/wandb/exp_num': 1, 'experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'experiment/wandb/checkpoint_path': 'checkpoint/', 'model/max_input_length': 100, 'model/max_target_length': 100, 'training/learning_rate': 0.0003, 'training/weight_decay': 0.0, 'training/adam_epsilon': 1e-08, 'training/warmup_steps': 0, 'training/gradient_accumulation_steps': 1, 'training/early_stop_callback': True, 'training/fp_16': False, 'training/opt_level': 'O1', 'training/max_grad_norm': 1.0, 'training/seed': 42, 'training/train_batch_size': 32, 'training/eval_batch_size': 32, 'training/test_batch_size': 8, 'training/num_train_epochs': 10, 'training/precision': 16, 'training/n_gpu': 0}
2021-09-18 03:28:39,446 INFO    MainThread:1770 [wandb_watch.py:watch():32] Watching
2021-09-18 03:28:39,539 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:29:39,280 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:29:39,329 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:29:58,808 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:29:58,860 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:30:02,000 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:30:02,054 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:30:10,975 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:30:11,000 INFO    MainThread:1770 [wandb_run.py:_config_callback():586] config_cb None None {'_wandb': {'cli_version': '0.10.7', 'python_version': '3.7.12', 'is_jupyter_run': True, 'is_kaggle_kernel': False, 'framework': 'torch'}, 'experiment/seed': 42, 'experiment/data': 'mhqg-pq', 'experiment/data_dir': 'data/mhqg-pq/', 'experiment/model_name_or_path': 't5-small', 'experiment/tokenizer_name_or_path': 't5-small', 'experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'experiment/wandb/exp_num': 1, 'experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'experiment/wandb/checkpoint_path': 'checkpoint/', 'model/max_input_length': 100, 'model/max_target_length': 100, 'training/learning_rate': 0.0003, 'training/weight_decay': 0.0, 'training/adam_epsilon': 1e-08, 'training/warmup_steps': 0, 'training/gradient_accumulation_steps': 1, 'training/early_stop_callback': True, 'training/fp_16': False, 'training/opt_level': 'O1', 'training/max_grad_norm': 1.0, 'training/seed': 42, 'training/train_batch_size': 32, 'training/eval_batch_size': 32, 'training/test_batch_size': 8, 'training/num_train_epochs': 10, 'training/precision': 16, 'training/n_gpu': 0}
2021-09-18 03:30:13,552 INFO    MainThread:1770 [wandb_watch.py:watch():32] Watching
2021-09-18 03:30:13,685 INFO    MainThread:1770 [wandb_run.py:_config_callback():586] config_cb None None {'_wandb': {'cli_version': '0.10.7', 'python_version': '3.7.12', 'is_jupyter_run': True, 'is_kaggle_kernel': False, 'framework': 'torch'}, 'experiment/seed': 42, 'experiment/data': 'mhqg-pq', 'experiment/data_dir': 'data/mhqg-pq/', 'experiment/model_name_or_path': 't5-small', 'experiment/tokenizer_name_or_path': 't5-small', 'experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'experiment/wandb/exp_num': 1, 'experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'experiment/wandb/checkpoint_path': 'checkpoint/', 'model/max_input_length': 100, 'model/max_target_length': 100, 'training/learning_rate': 0.0003, 'training/weight_decay': 0.0, 'training/adam_epsilon': 1e-08, 'training/warmup_steps': 0, 'training/gradient_accumulation_steps': 1, 'training/early_stop_callback': True, 'training/fp_16': False, 'training/opt_level': 'O1', 'training/max_grad_norm': 1.0, 'training/seed': 42, 'training/train_batch_size': 32, 'training/eval_batch_size': 32, 'training/test_batch_size': 8, 'training/num_train_epochs': 10, 'training/precision': 16, 'training/n_gpu': 0, 'config/experiment/seed': 42, 'config/experiment/data': 'mhqg-pq', 'config/experiment/data_dir': 'data/mhqg-pq/', 'config/experiment/model_name_or_path': 't5-small', 'config/experiment/tokenizer_name_or_path': 't5-small', 'config/experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'config/experiment/wandb/exp_num': 1, 'config/experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'config/experiment/wandb/checkpoint_path': 'checkpoint/', 'config/model/max_input_length': 100, 'config/model/max_target_length': 100, 'config/training/learning_rate': 0.0003, 'config/training/weight_decay': 0.0, 'config/training/adam_epsilon': 1e-08, 'config/training/warmup_steps': 0, 'config/training/gradient_accumulation_steps': 1, 'config/training/early_stop_callback': True, 'config/training/fp_16': False, 'config/training/opt_level': 'O1', 'config/training/max_grad_norm': 1.0, 'config/training/seed': 42, 'config/training/train_batch_size': 32, 'config/training/eval_batch_size': 32, 'config/training/test_batch_size': 8, 'config/training/num_train_epochs': 10, 'config/training/precision': 16, 'config/training/n_gpu': 0}
2021-09-18 03:30:20,649 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:30:41,201 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:30:41,489 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:33:06,620 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:33:06,643 INFO    MainThread:1770 [wandb_run.py:_config_callback():586] config_cb None None {'_wandb': {'cli_version': '0.10.7', 'python_version': '3.7.12', 'is_jupyter_run': True, 'is_kaggle_kernel': False, 'framework': 'torch'}, 'experiment/seed': 42, 'experiment/data': 'mhqg-pq', 'experiment/data_dir': 'data/mhqg-pq/', 'experiment/model_name_or_path': 't5-small', 'experiment/tokenizer_name_or_path': 't5-small', 'experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'experiment/wandb/exp_num': 1, 'experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'experiment/wandb/checkpoint_path': 'checkpoint/', 'model/max_input_length': 100, 'model/max_target_length': 100, 'training/learning_rate': 0.0003, 'training/weight_decay': 0.0, 'training/adam_epsilon': 1e-08, 'training/warmup_steps': 0, 'training/gradient_accumulation_steps': 1, 'training/early_stop_callback': True, 'training/fp_16': False, 'training/opt_level': 'O1', 'training/max_grad_norm': 1.0, 'training/seed': 42, 'training/train_batch_size': 32, 'training/eval_batch_size': 32, 'training/test_batch_size': 8, 'training/num_train_epochs': 10, 'training/precision': 16, 'training/n_gpu': 0, 'config/experiment/seed': 42, 'config/experiment/data': 'mhqg-pq', 'config/experiment/data_dir': 'data/mhqg-pq/', 'config/experiment/model_name_or_path': 't5-small', 'config/experiment/tokenizer_name_or_path': 't5-small', 'config/experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'config/experiment/wandb/exp_num': 1, 'config/experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'config/experiment/wandb/checkpoint_path': 'checkpoint/', 'config/model/max_input_length': 100, 'config/model/max_target_length': 100, 'config/training/learning_rate': 0.0003, 'config/training/weight_decay': 0.0, 'config/training/adam_epsilon': 1e-08, 'config/training/warmup_steps': 0, 'config/training/gradient_accumulation_steps': 1, 'config/training/early_stop_callback': True, 'config/training/fp_16': False, 'config/training/opt_level': 'O1', 'config/training/max_grad_norm': 1.0, 'config/training/seed': 42, 'config/training/train_batch_size': 32, 'config/training/eval_batch_size': 32, 'config/training/test_batch_size': 8, 'config/training/num_train_epochs': 10, 'config/training/precision': 16, 'config/training/n_gpu': 0}
2021-09-18 03:33:09,249 INFO    MainThread:1770 [wandb_watch.py:watch():32] Watching
2021-09-18 03:33:09,438 INFO    MainThread:1770 [wandb_run.py:_config_callback():586] config_cb None None {'_wandb': {'cli_version': '0.10.7', 'python_version': '3.7.12', 'is_jupyter_run': True, 'is_kaggle_kernel': False, 'framework': 'torch'}, 'experiment/seed': 42, 'experiment/data': 'mhqg-pq', 'experiment/data_dir': 'data/mhqg-pq/', 'experiment/model_name_or_path': 't5-small', 'experiment/tokenizer_name_or_path': 't5-small', 'experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'experiment/wandb/exp_num': 1, 'experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'experiment/wandb/checkpoint_path': 'checkpoint/', 'model/max_input_length': 100, 'model/max_target_length': 100, 'training/learning_rate': 0.0003, 'training/weight_decay': 0.0, 'training/adam_epsilon': 1e-08, 'training/warmup_steps': 0, 'training/gradient_accumulation_steps': 1, 'training/early_stop_callback': True, 'training/fp_16': False, 'training/opt_level': 'O1', 'training/max_grad_norm': 1.0, 'training/seed': 42, 'training/train_batch_size': 32, 'training/eval_batch_size': 32, 'training/test_batch_size': 8, 'training/num_train_epochs': 10, 'training/precision': 16, 'training/n_gpu': 0, 'config/experiment/seed': 42, 'config/experiment/data': 'mhqg-pq', 'config/experiment/data_dir': 'data/mhqg-pq/', 'config/experiment/model_name_or_path': 't5-small', 'config/experiment/tokenizer_name_or_path': 't5-small', 'config/experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'config/experiment/wandb/exp_num': 1, 'config/experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'config/experiment/wandb/checkpoint_path': 'checkpoint/', 'config/model/max_input_length': 100, 'config/model/max_target_length': 100, 'config/training/learning_rate': 0.0003, 'config/training/weight_decay': 0.0, 'config/training/adam_epsilon': 1e-08, 'config/training/warmup_steps': 0, 'config/training/gradient_accumulation_steps': 1, 'config/training/early_stop_callback': True, 'config/training/fp_16': False, 'config/training/opt_level': 'O1', 'config/training/max_grad_norm': 1.0, 'config/training/seed': 42, 'config/training/train_batch_size': 32, 'config/training/eval_batch_size': 32, 'config/training/test_batch_size': 8, 'config/training/num_train_epochs': 10, 'config/training/precision': 16, 'config/training/n_gpu': 0}
2021-09-18 03:33:10,996 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:35:11,616 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:35:11,902 INFO    MainThread:1770 [wandb_init.py:_pause_backend():214] pausing backend
2021-09-18 03:35:49,918 INFO    MainThread:1770 [wandb_init.py:_resume_backend():219] resuming backend
2021-09-18 03:35:49,940 INFO    MainThread:1770 [wandb_run.py:_config_callback():586] config_cb None None {'_wandb': {'cli_version': '0.10.7', 'python_version': '3.7.12', 'is_jupyter_run': True, 'is_kaggle_kernel': False, 'framework': 'torch'}, 'experiment/seed': 42, 'experiment/data': 'mhqg-pq', 'experiment/data_dir': 'data/mhqg-pq/', 'experiment/model_name_or_path': 't5-small', 'experiment/tokenizer_name_or_path': 't5-small', 'experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'experiment/wandb/exp_num': 1, 'experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'experiment/wandb/checkpoint_path': 'checkpoint/', 'model/max_input_length': 100, 'model/max_target_length': 100, 'training/learning_rate': 0.0003, 'training/weight_decay': 0.0, 'training/adam_epsilon': 1e-08, 'training/warmup_steps': 0, 'training/gradient_accumulation_steps': 1, 'training/early_stop_callback': True, 'training/fp_16': False, 'training/opt_level': 'O1', 'training/max_grad_norm': 1.0, 'training/seed': 42, 'training/train_batch_size': 32, 'training/eval_batch_size': 32, 'training/test_batch_size': 8, 'training/num_train_epochs': 10, 'training/precision': 16, 'training/n_gpu': 0, 'config/experiment/seed': 42, 'config/experiment/data': 'mhqg-pq', 'config/experiment/data_dir': 'data/mhqg-pq/', 'config/experiment/model_name_or_path': 't5-small', 'config/experiment/tokenizer_name_or_path': 't5-small', 'config/experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'config/experiment/wandb/exp_num': 1, 'config/experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'config/experiment/wandb/checkpoint_path': 'checkpoint/', 'config/model/max_input_length': 100, 'config/model/max_target_length': 100, 'config/training/learning_rate': 0.0003, 'config/training/weight_decay': 0.0, 'config/training/adam_epsilon': 1e-08, 'config/training/warmup_steps': 0, 'config/training/gradient_accumulation_steps': 1, 'config/training/early_stop_callback': True, 'config/training/fp_16': False, 'config/training/opt_level': 'O1', 'config/training/max_grad_norm': 1.0, 'config/training/seed': 42, 'config/training/train_batch_size': 32, 'config/training/eval_batch_size': 32, 'config/training/test_batch_size': 8, 'config/training/num_train_epochs': 10, 'config/training/precision': 16, 'config/training/n_gpu': 0}
2021-09-18 03:35:52,514 INFO    MainThread:1770 [wandb_watch.py:watch():32] Watching
2021-09-18 03:35:52,690 INFO    MainThread:1770 [wandb_run.py:_config_callback():586] config_cb None None {'_wandb': {'cli_version': '0.10.7', 'python_version': '3.7.12', 'is_jupyter_run': True, 'is_kaggle_kernel': False, 'framework': 'torch'}, 'experiment/seed': 42, 'experiment/data': 'mhqg-pq', 'experiment/data_dir': 'data/mhqg-pq/', 'experiment/model_name_or_path': 't5-small', 'experiment/tokenizer_name_or_path': 't5-small', 'experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'experiment/wandb/exp_num': 1, 'experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'experiment/wandb/checkpoint_path': 'checkpoint/', 'model/max_input_length': 100, 'model/max_target_length': 100, 'training/learning_rate': 0.0003, 'training/weight_decay': 0.0, 'training/adam_epsilon': 1e-08, 'training/warmup_steps': 0, 'training/gradient_accumulation_steps': 1, 'training/early_stop_callback': True, 'training/fp_16': False, 'training/opt_level': 'O1', 'training/max_grad_norm': 1.0, 'training/seed': 42, 'training/train_batch_size': 32, 'training/eval_batch_size': 32, 'training/test_batch_size': 8, 'training/num_train_epochs': 10, 'training/precision': 16, 'training/n_gpu': 0, 'config/experiment/seed': 42, 'config/experiment/data': 'mhqg-pq', 'config/experiment/data_dir': 'data/mhqg-pq/', 'config/experiment/model_name_or_path': 't5-small', 'config/experiment/tokenizer_name_or_path': 't5-small', 'config/experiment/model_dir': 'content/test_mhqg-pq_t5-small_with_TripleAttention_20210918_1218', 'config/experiment/wandb/exp_num': 1, 'config/experiment/wandb/project': 'test_mhqg-pq_t5-small_with_TripleAttention', 'config/experiment/wandb/checkpoint_path': 'checkpoint/', 'config/model/max_input_length': 100, 'config/model/max_target_length': 100, 'config/training/learning_rate': 0.0003, 'config/training/weight_decay': 0.0, 'config/training/adam_epsilon': 1e-08, 'config/training/warmup_steps': 0, 'config/training/gradient_accumulation_steps': 1, 'config/training/early_stop_callback': True, 'config/training/fp_16': False, 'config/training/opt_level': 'O1', 'config/training/max_grad_norm': 1.0, 'config/training/seed': 42, 'config/training/train_batch_size': 32, 'config/training/eval_batch_size': 32, 'config/training/test_batch_size': 8, 'config/training/num_train_epochs': 10, 'config/training/precision': 16, 'config/training/n_gpu': 0}
